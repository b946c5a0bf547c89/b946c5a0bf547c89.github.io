<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Public12345678910111213141516171819options lnet networks&#x3D;tcp(bond0)options ptlrpc ldlm_num_threads&#x3D;16options ptlrpc at_max&#x3D;300options ptlrpc at_min&#x3D;50options ptlrpc ldlm_enqueue_min&#x3D;250# ldlm_enqueue_">
<meta property="og:type" content="article">
<meta property="og:title" content="lfs command">
<meta property="og:url" content="http://yoursite.com/2019/12/29/lfs_command/index.html">
<meta property="og:site_name" content="b946c5a0bf547c89">
<meta property="og:description" content="Public12345678910111213141516171819options lnet networks&#x3D;tcp(bond0)options ptlrpc ldlm_num_threads&#x3D;16options ptlrpc at_max&#x3D;300options ptlrpc at_min&#x3D;50options ptlrpc ldlm_enqueue_min&#x3D;250# ldlm_enqueue_">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-29T07:41:16.000Z">
<meta property="article:modified_time" content="2020-01-29T15:30:10.000Z">
<meta property="article:author" content="Ginger">
<meta property="article:tag" content="lfs">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2019/12/29/lfs_command/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>lfs command | b946c5a0bf547c89</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">b946c5a0bf547c89</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">My Silent Hill</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/29/lfs_command/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/logo-4-blog.png">
      <meta itemprop="name" content="Ginger">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="b946c5a0bf547c89">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          lfs command
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-12-29 15:41:16" itemprop="dateCreated datePublished" datetime="2019-12-29T15:41:16+08:00">2019-12-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-29 23:30:10" itemprop="dateModified" datetime="2020-01-29T23:30:10+08:00">2020-01-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/filesystem/" itemprop="url" rel="index">
                    <span itemprop="name">filesystem</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2019/12/29/lfs_command/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/29/lfs_command/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>32k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>29 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="Public"><a href="#Public" class="headerlink" title="Public"></a>Public</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span></span><br><span class="line"><span class="comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lustre servers is also 40 seconds</span></span><br><span class="line"><span class="comment">#Adaptive Timeouts: In a Lustre file system servers keep track of the time it takes for RPCs to be completed</span></span><br><span class="line"><span class="comment">#The quiescent_time in this formula is to account for the time it takes all Lustre clients to reestablish connections with all Lustre targets following an HSN quiesce. We've experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lustre clients, the number of Lustre targets, the number of Lustre file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># readonly mount</span></span><br><span class="line">$ mount.lfs <span class="variable">$zpool</span> /ost0 -o rdonly_dev</span><br></pre></td></tr></table></figure>
<p><a href="http://wiki.lfs.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency" target="_blank" rel="noopener">Understanding Lustre Message Loss and Tuning for Resiliency</a></p>
<a id="more"></a>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">#reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span><br><span class="line">options ptlrpc ldlm_num_threads&#x3D;16</span><br><span class="line"></span><br><span class="line"># Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size&#x3D;clear</span><br><span class="line"></span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 &#x2F;mnt&#x2F;striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  obdfilter.*OST0000*.stats</span><br><span class="line"></span><br><span class="line"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span><br><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;lib64&#x2F;openmpi&#x2F;bin&#x2F;:$PATH</span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear &#x2F;tmp&#x2F;tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? &#96;too stupid&#96; bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ .&#x2F;configure --enable-client --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;kernels&#x2F;$(uname -r);make rpms&#x2F;deps</span><br><span class="line">## install in ubuntu 18.04</span><br><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;&#x2F;lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ .&#x2F;configure --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  &#x2F;lib&#x2F;modules&#x2F;4.15.0-64-generic&#x2F;kernel&#x2F;drivers&#x2F;staging&#x2F;lfs&#x2F;</span><br><span class="line">$ depmod -a</span><br><span class="line"></span><br><span class="line">### set lfs client for a lot metadata ops</span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size&#x3D;10000 ldlm.namespaces.*.lru_max_age&#x3D;600000</span><br><span class="line"></span><br><span class="line"># check object server status</span><br><span class="line">client $ lfs osts</span><br><span class="line">client $ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;lov&#x2F;$fsname-clilov-fffff882037467800&#x2F;target_obd</span><br><span class="line">mds    $ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;lov&#x2F;$fsname-MDT0000-mdtlov&#x2F;target_obd</span><br><span class="line">client $ lctl get_param osc.*-OST*.active</span><br><span class="line"></span><br><span class="line"># check object server status in my production env</span><br><span class="line">$ (lfs osts | awk -F &#39;[ :_]+&#39; &#39;$0~&#x2F;OST&#x2F;&#123;print $2&#125;&#39; | while read line; do grep &quot;FULL&quot; &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;osc&#x2F;$&#123;line&#125;-*&#x2F;state &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 || echo -e &quot;Got these bad OSTs:&quot; $&#123;RED&#125;$line$&#123;NC&#125; ; done) &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line"># show ost ip  addr</span><br><span class="line">$ lctl dl -t</span><br><span class="line"></span><br><span class="line"># list nids</span><br><span class="line">$ lctl lst_nids</span><br><span class="line">$ lctl which_nid $your_ipaddr@tcp</span><br><span class="line">$ lctl ping $your_ipaddr@tcp </span><br><span class="line"></span><br><span class="line"># mount namespace &quot;D&quot; and clear mds info from client</span><br><span class="line">client $ echo 0 &gt; cat &#x2F;sys&#x2F;fs&#x2F;lfs&#x2F;mdc&#x2F;$FNAME-*&#x2F;active</span><br><span class="line">client $ lctl set_param mdc.$FNAME-*.active&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="metadata-server"><a href="#metadata-server" class="headerlink" title="metadata server"></a>metadata server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"># Get all client info from mds</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;nodemap&#x2F;default&#x2F;exports</span><br><span class="line"></span><br><span class="line"># No root squash</span><br><span class="line">mds $ lctl set_param $fsname.mdt.root_squash&#x3D;108:108</span><br><span class="line">error: set_param: param_path &#39;$fsname&#x2F;mdt&#x2F;root_squash&#39;: No such file or directory</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.root_squash&#x3D;108:108</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.nosquash_nids&#x3D;&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line">mds $ cat &#x2F;proc&#x2F;fs&#x2F;lf&#x2F;mdt&#x2F;$FNAME-MDT0000&#x2F;nosquash_nids</span><br><span class="line">ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp</span><br><span class="line"></span><br><span class="line">client $ lctl set_param llite.$FNAME-*.nosquash_nids&#x3D;&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line"></span><br><span class="line"># MDS to totally avoid new object creation on that OST</span><br><span class="line">$ lctl set_param osp.$FNAME-OST00XX.max_create_count&#x3D;0</span><br><span class="line"></span><br><span class="line">#degrade will only prefer to skip the OST</span><br><span class="line">$ lctl set_param obdfilter.$FNAME-OST00XX.degraded&#x3D;1</span><br><span class="line"></span><br><span class="line">$ lctl set_param -P timeout&#x3D;300</span><br><span class="line">$ lctl set_param timeout&#x3D;300 </span><br><span class="line"># if you want it work ,you have set it twice...</span><br><span class="line"></span><br><span class="line"># Monitor status</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;mdc&#x2F;$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800&#x2F;state </span><br><span class="line">$ lctl get_param mdc.$fname-MDT*.state</span><br><span class="line">$ lctl get_param mdc.$fname-MDT0000-mdc-*.rpc_stats</span><br><span class="line"></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             1556726087.189561170 secs.nsecs</span><br><span class="line">open                      3412130101 samples [reqs]</span><br><span class="line">close                     2926922120 samples [reqs]</span><br><span class="line">mknod                     293730475 samples [reqs]</span><br><span class="line">link                      20713305 samples [reqs]</span><br><span class="line">unlink                    316042257 samples [reqs]</span><br><span class="line">mkdir                     3275032 samples [reqs]</span><br><span class="line">rmdir                     2731821 samples [reqs]</span><br><span class="line">rename                    7687699 samples [reqs]</span><br><span class="line">getattr                   2060900881 samples [reqs]</span><br><span class="line">setattr                   320658776 samples [reqs]</span><br><span class="line">getxattr                  1080139037 samples [reqs]</span><br><span class="line">setxattr                  222105 samples [reqs]</span><br><span class="line">statfs                    11587278 samples [reqs]</span><br><span class="line">sync                      20670980 samples [reqs]</span><br><span class="line">samedir_rename            7199107 samples [reqs]</span><br><span class="line">crossdir_rename           488592 samples [reqs]</span><br></pre></td></tr></table></figure>

<h3 id="object-server"><a href="#object-server" class="headerlink" title="object server"></a>object server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size&#x3D;1048576</span><br><span class="line"></span><br><span class="line">options ost oss_num_threads&#x3D;0</span><br><span class="line"># you can limit the server performance by num_threads </span><br><span class="line"></span><br><span class="line">#limit ost io threads</span><br><span class="line">$ lctl set_param ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line">$ lctl set_param -P ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line"></span><br><span class="line"># rpc info</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;osc&#x2F;lfs-OST0000-osc-ffff88103a993000&#x2F;import</span><br><span class="line"></span><br><span class="line"># Get status</span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* timeout</span><br><span class="line">$ lctl get_param llite.*$FNAME*.stats</span><br><span class="line">$ lctl get_param obdfilter.*OST005e*.brw_stats</span><br><span class="line"></span><br><span class="line">#Read and print the last_rcvd file from a device</span><br><span class="line">#display client information</span><br><span class="line">$ lr_reader -c &#x2F;dev&#x2F;sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r &#x2F;dev&#x2F;sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line"># disable ost cache</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line"></span><br><span class="line">#make sure ost mount parameters and flag</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;ldiskfs&#x2F;dm-xx&#x2F;options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid&#x3D;0</span><br><span class="line">resgid&#x3D;0</span><br><span class="line">errors&#x3D;remount-ro</span><br><span class="line">commit&#x3D;5</span><br><span class="line">min_batch_time&#x3D;0</span><br><span class="line">max_batch_time&#x3D;15000</span><br><span class="line">stripe&#x3D;0</span><br><span class="line">data&#x3D;ordered</span><br><span class="line">inode_readahead_blks&#x3D;32</span><br><span class="line">init_itable&#x3D;10</span><br><span class="line">max_dir_size_kb&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="net"><a href="#net" class="headerlink" title="net"></a>net</h3><p>I think in some the bad network quality env, you must improve them</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Peer Credits</span></span><br><span class="line"><span class="comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers </span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue </span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available <span class="keyword">for</span> this peer</span><br><span class="line">min is the smallest number of peer credits seen </span><br><span class="line">Negative credit count indicates the number of messages awaiting a credit</span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Interface Credits</span></span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min </span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 </span><br><span class="line">max is total available (i.e. value of ko2iblnd credits) </span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line"><span class="comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span></span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="comment"># the number of concurrent sends (to all peers), defaults:64</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">peer_buffer_credits=256</span><br><span class="line">concurrent_sends=256 - send work-queue sizing</span><br><span class="line"><span class="comment"># not test</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">the number of concurrent sends to a single peer, <span class="comment">#default:8</span></span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line"></span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line"><span class="comment">## Acceptor's listen backlog, the number of the connections the server instance can buffer in the wait queue.</span></span><br><span class="line">options lnet accept_timeout=128</span><br><span class="line"><span class="comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span></span><br><span class="line">k</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="comment"># 1024 = 1024*4KB =4MB per RPC</span></span><br><span class="line"><span class="comment">#Max RPCS in flight between OSC and OST</span></span><br><span class="line">$ lctl set_param -P <span class="variable">$FNAME</span>.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"><span class="comment"># Max number of 4K pages per RPC</span></span><br><span class="line"><span class="comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span></span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"><span class="comment"># Maximum MBs of dirty data that can be written and queued on a client</span></span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ modinfo mdt | grep max_mod_rpcs_per_client</span><br><span class="line">parm:           max_mod_rpcs_per_client:maximum number of modify RPCs <span class="keyword">in</span> flight allowed per client (uint)</span><br><span class="line"></span><br><span class="line">mds $ <span class="built_in">echo</span> 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show --verbose</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane</span></span><br><span class="line">options lnet networks=<span class="string">"tcp1(eth1),tcp2(eth2),o2ib0(ib0)"</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">"tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]"</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span></span><br><span class="line"><span class="comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span></span><br><span class="line"><span class="comment"># fs.pipe-max-size=104857600</span></span><br><span class="line"><span class="comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it 's pipe buffer size in the ulimit, not pipe size</span></span><br><span class="line"><span class="comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here 2 tools for pipe</span></span><br><span class="line"><span class="comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span></span><br><span class="line"><span class="comment"># process1 | pv -pterbTCB 1G | process2</span></span><br><span class="line"></span><br><span class="line">$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat</span><br><span class="line">      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]</span><br><span class="line">  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            </span><br><span class="line">1538323+1 records <span class="keyword">in</span></span><br><span class="line">1538323+1 records out</span><br><span class="line">787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ mkfifo -m 777 /tmp/lfs.log</span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it 'll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:user@my.server.example.org">user@my.server.example.org</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">* MARK – Internal record keeping</span><br><span class="line">* CREAT – Regular file creation</span><br><span class="line">* MKDIR – Directory creation</span><br><span class="line">* HLINK – Hard link</span><br><span class="line">* SLINK – Soft link</span><br><span class="line">* OPEN – open file</span><br><span class="line">* CLOSE – close file</span><br><span class="line">* MKNOD – Other file creation</span><br><span class="line">* UNLNK – Regular file removal</span><br><span class="line">* RMDIR – Directory removal</span><br><span class="line">* RNMFM – Rename, original</span><br><span class="line">* RNMTO – Rename, final</span><br><span class="line">* IOCTL – ioctl on file or directory</span><br><span class="line">* TRUNC – Regular file truncated</span><br><span class="line">* SATTR – Attribute change</span><br><span class="line">* XATTR – Extended Attribute change</span><br><span class="line">* HSM – HSM action</span><br><span class="line">* UNKNW – Unkown operation</span><br></pre></td></tr></table></figure>

<h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm.max_requests&#x3D;8</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT0000.changelog_users mdd.$FNAME-MDT0000.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask&#x3D;MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<h4 id="Disable"><a href="#Disable" class="headerlink" title="Disable"></a>Disable</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#Notify a device that user cl1 no longer needs records (up toand including 3)</span><br><span class="line">$ lfs changelog_clear $FNAME-MDT0000 cl1 3</span><br><span class="line"></span><br><span class="line">#To stop changelogs, changelog_mask should be set to MARK only</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line">mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line"></span><br><span class="line">#or youcan set it -all</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;-all</span><br></pre></td></tr></table></figure>

<h3 id="FSCK"><a href="#FSCK" class="headerlink" title="FSCK"></a>FSCK</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 <span class="keyword">in</span> dir <span class="comment">#145170469</span></span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem <span class="built_in">read</span>-only</span><br></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy &#x2F;dev&#x2F;sdx</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>
<p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p>
<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## you can 't use lctl set_param, it 's not work</span></span><br><span class="line"></span><br><span class="line">mds $ lctl conf_param <span class="variable">$FNAME</span>.quota.mdt=ug</span><br><span class="line">mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">mds $ lctl get_param osd-*.*.quota_slave.info</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">mds $ lctl conf_param <span class="variable">$FNAME</span>.quota.ost=ug</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### lctl set_param -P must reboot, no -P not work, if you don' t reboot ,you have too use conf_param</span></span><br><span class="line"></span><br><span class="line">client $ lfs setquota –u user1 –b 307200 –B 309200 –i 10000 –I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota –g group1 –b 5120000 –B 5150000 –i 100000 –I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota –u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 deactivate</span><br><span class="line">mds $ lctl --device 8 activate</span><br></pre></td></tr></table></figure>

<h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or </span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont 't use lfs migrate, it 's too dangerous, it will cause data loss</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Job status</span></span><br><span class="line">```bash</span><br><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn't any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats for job "dd.0" on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]# lfs fid2path &#x2F;mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid &#x2F;mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>

<h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line"></span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lustre to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>

<h3 id="Lustre-issues"><a href="#Lustre-issues" class="headerlink" title="Lustre issues"></a>Lustre issues</h3><h4 id="Don-‘t-use-openzfs-with-lfs-file-system"><a href="#Don-‘t-use-openzfs-with-lfs-file-system" class="headerlink" title="Don ‘t use openzfs with lfs file system"></a>Don ‘t use openzfs with lfs file system</h4><ul>
<li>No bility to support openzfs</li>
<li>If your zpool brain split, they will not show you any help</li>
<li>If you enable openzfs MMP, if single HDD will broken, your zpool have to suspend once<ul>
<li><a href="https://github.com/zfsonlinux/zfs/issues/7045" target="_blank" rel="noopener">https://github.com/zfsonlinux/zfs/issues/7045</a><ul>
<li><a href="https://github.com/zfsonlinux/zfs/issues/7118" target="_blank" rel="noopener">https://github.com/zfsonlinux/zfs/issues/7118</a></li>
<li>this issue impact from zfs 0.7 to 0.7.9, I ‘m not sure 0.8 has the same issue, the best way is disable MMP and use single link for your SAS devices</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Multipath-failed-cause"><a href="#Multipath-failed-cause" class="headerlink" title="Multipath failed cause"></a>Multipath failed cause</h4><ul>
<li>Found error log; <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: attempting task abort! scmd(ffff97ac51784a80)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi 14:0:5:31: [sg64] tag<span class="comment">#76 CDB: Read(6) 08 00 02 61 01 00</span></span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)</span><br><span class="line">Sep 22 03:16:00 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:01 lfs-node21 systemd: Started Session 361698 of user root.</span><br><span class="line">Sep 22 03:16:01 lfs-node21 systemd: Starting Session 361698 of user root.</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: attempting task abort! scmd(ffff97b2ae507b80)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: sd 14:0:5:6: [sdp] tag<span class="comment">#8 CDB: Write(16) 8a 00 00 00 00 07 c6 2a 89 e8 00 00 08 00 00 00</span></span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c66c014), phy(4)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosurelogical id(0x51866da0a8a62200), slot(4)</span><br><span class="line">Sep 22 03:16:09 lfs-node21 kernel: scsi target14:0:5: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:30 lfs-node21 kernel: mpt3sas_cm3: Command Timeout</span><br><span class="line">Sep 22 03:16:30 lfs-node21 kernel: mf:<span class="comment">#012#0110100000a 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 0000004d</span></span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: attempting task abort! scmd(ffff97bc6aad1c00)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi 13:0:0:31: [sg45] tag<span class="comment">#197 CDB: Read(6) 08 00 02 00 01 00</span></span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c6c3014), phy(0)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosurelogical id(0x51866da0a8a62100), slot(0)</span><br><span class="line">Sep 22 03:16:32 lfs-node21 kernel: scsi target13:0:0: enclosure level(0x0000), connector name( 0   )</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: attempting task abort! scmd(ffff97b2ae506680)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: sd 13:0:1:6: [sdap] tag<span class="comment">#4 CDB: Write(16) 8a 00 00 00 00 07 c5 fd 27 10 00 00 08 00 00 00</span></span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c5d9014), phy(4)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosurelogical id(0x51866da0a8a62100), slot(4)</span><br><span class="line">Sep 22 03:16:39 lfs-node21 kernel: scsi target13:0:1: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:16:40 lfs-node21 kernel: mpt3sas_cm3: sending diag reset !!</span><br><span class="line">Sep 22 03:16:41 lfs-node21 kernel: mpt3sas_cm3: diag reset: SUCCESS</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: IOC Number : 0</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: FW Package Version(16.17.00.03)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Dell 12Gbps SAS HBA</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Sep 22 03:16:42 lfs-node21 kernel: mpt3sas_cm3: sending port <span class="built_in">enable</span> !!</span><br><span class="line"></span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is down: inquiry failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: checker failed path 67:80 <span class="keyword">in</span> map 3600a098000b4c3060000024359244375</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 8:240: reinstated</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: sdbg: mark as failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: sdbi: mark as failed</span><br><span class="line">Sep 22 03:32:04 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 1</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdu, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdv, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdw, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdx, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdy, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdar, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdas, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdat, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdau, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdav, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaw, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdax, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sday, open() failed: No such device</span><br><span class="line">Sep 22 03:32:06 lfs-node21 smartd[5112]: Device: /dev/sdaz, open() failed: No such device</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is upa</span><br><span class="line"></span><br><span class="line"><span class="comment">### All devices single link offline at the same time, I have 4 x 9300-8E, impossible offline at the same time</span></span><br><span class="line"><span class="comment">### Filter all error devices and make sure just single HBA status not correct, and you can found mpt3sas_cm0 in the system message</span></span><br><span class="line"></span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: sdbb - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:80.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:160.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:80: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c3060000024359244375: remaining active paths: 2</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: sdbg - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:160: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c0000020d59243745: remaining active paths: 2</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: sdbi - rdac checker reports path is up</span><br><span class="line">Sep 22 03:32:14 lfs-node21 kernel: device-mapper: multipath: Reinstating path 67:192.</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 67:192: reinstated</span><br><span class="line">Sep 22 03:32:14 lfs-node21 multipathd: 3600a098000b4c66c00000213592437d2: remaining active paths: 2</span><br><span class="line"></span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: attempting task abort! scmd(ffff97b265c7b2c0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi 1:0:0:31: [sg9] tag<span class="comment">#17 CDB: Read(6) 08 00 02 00 01 00</span></span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:34:16 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )</span><br><span class="line">Sep 22 03:34:46 lfs-node21 kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">Sep 22 03:34:46 lfs-node21 kernel: mf:<span class="comment">#012#01101000009 00000100 00000000 00001f00 00000000 00000000 00000000 00000000 #012#01100000000 00000000 00000000 00000000 00000012</span></span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: attempting task abort! scmd(ffff97a35d8c9500)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: sd 1:0:1:24: [sdk] tag<span class="comment">#42 CDB: Read(16) 88 00 00 00 00 09 1f 1d 30 00 00 00 00 08 00 00</span></span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: _scsih_tm_display_info: handle(0x000a), sas_address(0x500a098b4c810014), phy(4)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosurelogical id(0x51866da09fad6700), slot(4)</span><br><span class="line">Sep 22 03:34:55 lfs-node21 kernel: scsi target1:0:1: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Sep 22 03:34:56 lfs-node21 kernel: mpt3sas_cm0: sending diag reset !!</span><br><span class="line">Sep 22 03:34:57 lfs-node21 kernel: mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: IOC Number : 0</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: FW Package Version(16.17.00.03)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Dell 12Gbps SAS HBA</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Sep 22 03:34:58 lfs-node21 kernel: mpt3sas_cm0: sending port <span class="built_in">enable</span> !!</span><br><span class="line"></span><br><span class="line"><span class="comment"># del the others</span></span><br><span class="line"></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: handle(0x0009), sas_address(0x500a098b4c834014), port: 255</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure logical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: handle(0x000a), sas_address(0x500a098b4c810014), port: 255</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:1: enclosure logical id(0x51866da09fad6700), slot(4)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: <span class="comment">#011break from _scsih_search_responding_sas_devices: ioc_status(0x0022), loginfo(0x310f0400)</span></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> PCIe end-devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> expanders: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: search <span class="keyword">for</span> expanders: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi 1:0:0:31: task abort: SUCCESS scmd(ffff97b265c7b2c0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: sas end-devices</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: pcie end-devices</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: expanders</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: removing unresponding devices: complete</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: mpt3sas_cm0: scan devices: start</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: attempting task abort! scmd(ffff97b37ad6a4c0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: sd 1:0:0:13: [sdg] tag<span class="comment">#89 CDB: Inquiry 12 01 c9 00 30 00</span></span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: _scsih_tm_display_info: handle(0x0009), sas_address(0x500a098b4c834014), phy(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosurelogical id(0x51866da09fad6700), slot(0)</span><br><span class="line">Sep 22 03:35:05 lfs-node21 kernel: scsi target1:0:0: enclosure level(0x0000), connector name( 0   )</span><br></pre></td></tr></table></figure>
Single link offline, after reset the link attch back, and try to recovery the connnection</li>
</ul>
<p>But lfs has a <a href="https://jira.whamcloud.com/browse/LU-10510" target="_blank" rel="noopener">bug, LU-10510</a>, lfs will modify the value when it mount, but the designer not consider link/HBA/IO expander offline or some reason cause the value back to default value. what a pity, the operate (modify to 16383) only trigger on mount.lfs operation, the link back to recovery the connection ? no way.</p>
<p>It ‘s a simple logic. Why the desiger not consider the link offline ? I can ‘t understand. it must be an intern.<br>When the link recovery, the max_sectors_kb recovery to default value (512)<br>You have to use echo 16383 to improve it, why 16383, sorry , I don ‘t know.<br>Does the lfs system rigorous enough ? hahahahaahaha……………….<br>Does is it for Enterprise file system ? hahahahahahaha……………….</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ grep . /sys/block/*/queue/max_sectors_kb</span><br><span class="line">/sys/block/sdak/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdal/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdal/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sda/queue/max_hw_sectors_kb 256</span><br><span class="line">/sys/block/sda/queue/max_sectors_kb 256</span><br><span class="line">/sys/block/sdba/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdba/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbb/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbb/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbc/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbc/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sdbd/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbd/queue/max_sectors_kb 16383</span><br><span class="line">/sys/block/sdbe/queue/max_hw_sectors_kb 16383</span><br><span class="line">/sys/block/sdbe/queue/max_sectors_kb 512</span><br><span class="line">/sys/block/sdbf/queue/max_hw_sectors_kb 16383</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.00    0.00    0.02   47.92    0.00   52.06</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00    67.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdc               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdi               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdf               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdd               0.00     0.00    0.00    0.00     0.00     0.00     0.00   104.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdh               0.00     0.00    0.00    0.00     0.00     0.00     0.00    31.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdj               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdk               0.00     0.00    0.00    0.00     0.00     0.00     0.00    63.00    0.00    0.00    0.00   0.00 100.00</span><br><span class="line">sdl               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdm               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdn               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">sdo               0.00     0.00    0.00    0.00     0.00     0.00     0.00    64.00    0.00    0.00    0.00   0.00 100.00</span><br></pre></td></tr></table></figure>

<h4 id="Too-bad-desiger"><a href="#Too-bad-desiger" class="headerlink" title="Too bad desiger"></a>Too bad desiger</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">......................&#x2F;´¯&#x2F;)</span><br><span class="line">....................,&#x2F;¯..&#x2F;</span><br><span class="line">...................&#x2F;....&#x2F;</span><br><span class="line">.............&#x2F;´¯&#x2F;&#39;...&#39;&#x2F;´¯¯&#96;·¸</span><br><span class="line">..........&#x2F;&#39;&#x2F;...&#x2F;....&#x2F;.......&#x2F;¨¯\</span><br><span class="line">........(&#39;(...´...´.... ¯~&#x2F;&#39;...&#39;)</span><br><span class="line">.........\.................&#39;.....&#x2F;</span><br><span class="line">..........&#39;&#39;...\.......... _.·´</span><br><span class="line">............\..............(</span><br><span class="line">..............\.............\...</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/lfs/" rel="tag"><i class="fa fa-tag"></i> lfs</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/11/26/xfs/" rel="prev" title="xfs">
      <i class="fa fa-chevron-left"></i> xfs
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/01/04/hello-world/" rel="next" title="Hello World">
      Hello World <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Public"><span class="nav-number">1.</span> <span class="nav-text">Public</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Client"><span class="nav-number">2.</span> <span class="nav-text">Client</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#metadata-server"><span class="nav-number">3.</span> <span class="nav-text">metadata server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#object-server"><span class="nav-number">4.</span> <span class="nav-text">object server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#net"><span class="nav-number">5.</span> <span class="nav-text">net</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trace-log"><span class="nav-number">6.</span> <span class="nav-text">Trace log</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#lfs-log"><span class="nav-number">6.1.</span> <span class="nav-text">lfs log</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kdump"><span class="nav-number">6.2.</span> <span class="nav-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="nav-number"></span> <span class="nav-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="nav-number"></span> <span class="nav-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#changelog"><span class="nav-number">1.</span> <span class="nav-text">changelog</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Enable"><span class="nav-number">1.1.</span> <span class="nav-text">Enable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Disable"><span class="nav-number">1.2.</span> <span class="nav-text">Disable</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#FSCK"><span class="nav-number">2.</span> <span class="nav-text">FSCK</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Flush-the-journal"><span class="nav-number">2.1.</span> <span class="nav-text">Flush the journal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#re-writeconf"><span class="nav-number">2.2.</span> <span class="nav-text">re-writeconf</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#User-group-quota"><span class="nav-number">3.</span> <span class="nav-text">User group quota</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Disable-the-ost"><span class="nav-number">4.</span> <span class="nav-text">Disable the ost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skip-recovery"><span class="nav-number">5.</span> <span class="nav-text">Skip recovery</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lfs-migarate"><span class="nav-number">6.</span> <span class="nav-text">lfs migarate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lfs-fid-and-path"><span class="nav-number">7.</span> <span class="nav-text">lfs fid and path</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#increase-openzfs-sync-performance-in-test-env"><span class="nav-number">8.</span> <span class="nav-text">increase openzfs sync performance in test env</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lustre-issues"><span class="nav-number">9.</span> <span class="nav-text">Lustre issues</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Don-‘t-use-openzfs-with-lfs-file-system"><span class="nav-number">9.1.</span> <span class="nav-text">Don ‘t use openzfs with lfs file system</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multipath-failed-cause"><span class="nav-number">9.2.</span> <span class="nav-text">Multipath failed cause</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Too-bad-desiger"><span class="nav-number">9.3.</span> <span class="nav-text">Too bad desiger</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ginger"
      src="/img/logo-4-blog.png">
  <p class="site-author-name" itemprop="name">Ginger</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ginger</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">803k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">12:10</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://b946c5a0bf547c89.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://yoursite.com/2019/12/29/lfs_command/",
            identifier: "2019/12/29/lfs_command/",
            title: "lfs command"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://b946c5a0bf547c89.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
