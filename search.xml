<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>lfs command</title>
    <url>/2019/12/29/lfs_command/</url>
    <content><![CDATA[<h3 id="Public"><a href="#Public" class="headerlink" title="Public"></a>Public</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=260</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span></span><br><span class="line"><span class="comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lustre servers is also 40 seconds</span></span><br><span class="line"><span class="comment">#Adaptive Timeouts: In a Lustre file system servers keep track of the time it takes for RPCs to be completed</span></span><br><span class="line"><span class="comment">#The quiescent_time in this formula is to account for the time it takes all Lustre clients to reestablish connections with all Lustre targets following an HSN quiesce. We've experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lustre clients, the number of Lustre targets, the number of Lustre file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># Readonly mount</span></span><br><span class="line">$ mount.lfs <span class="variable">$zpool</span> /ost0 -o rdonly_dev</span><br></pre></td></tr></table></figure>
<p><a href="http://wiki.lfs.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency" target="_blank" rel="noopener">Understanding Lustre Message Loss and Tuning for Resiliency</a></p>
<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span><br><span class="line">options ptlrpc ldlm_num_threads&#x3D;16</span><br><span class="line"></span><br><span class="line"># Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size&#x3D;clear</span><br><span class="line"></span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 &#x2F;mnt&#x2F;striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  obdfilter.*OST0000*.stats</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;nodemap&#x2F;default&#x2F;exports</span><br><span class="line"></span><br><span class="line"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span><br><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;lib64&#x2F;openmpi&#x2F;bin&#x2F;:$PATH</span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear &#x2F;tmp&#x2F;tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? &#96;too stupid&#96; bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ .&#x2F;configure --enable-client --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;kernels&#x2F;$(uname -r);make rpms&#x2F;deps</span><br><span class="line">## install in ubuntu 18.04</span><br><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;&#x2F;lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ .&#x2F;configure --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  &#x2F;lib&#x2F;modules&#x2F;4.15.0-64-generic&#x2F;kernel&#x2F;drivers&#x2F;staging&#x2F;lfs&#x2F;</span><br><span class="line">$ depmod -a</span><br><span class="line"></span><br><span class="line">### set lfs client for a lot metadata ops</span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size&#x3D;10000 ldlm.namespaces.*.lru_max_age&#x3D;600000</span><br></pre></td></tr></table></figure>

<h3 id="metadata-server"><a href="#metadata-server" class="headerlink" title="metadata server"></a>metadata server</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lctl set_param -P timeout&#x3D;300</span><br><span class="line">$ lctl set_param timeout&#x3D;300 </span><br><span class="line"># if you want it work ,you have set it twice...</span><br><span class="line"></span><br><span class="line"># Monitor status</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lus...&#x2F;mdc&#x2F;$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800&#x2F;state </span><br><span class="line">$ lctl get_param mdc.$fname-MDT*.state</span><br><span class="line">$ lctl get_param mdc.$fname-MDT0000-mdc-*.rpc_stats</span><br><span class="line"></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             1556726087.189561170 secs.nsecs</span><br><span class="line">open                      3412130101 samples [reqs]</span><br><span class="line">close                     2926922120 samples [reqs]</span><br><span class="line">mknod                     293730475 samples [reqs]</span><br><span class="line">link                      20713305 samples [reqs]</span><br><span class="line">unlink                    316042257 samples [reqs]</span><br><span class="line">mkdir                     3275032 samples [reqs]</span><br><span class="line">rmdir                     2731821 samples [reqs]</span><br><span class="line">rename                    7687699 samples [reqs]</span><br><span class="line">getattr                   2060900881 samples [reqs]</span><br><span class="line">setattr                   320658776 samples [reqs]</span><br><span class="line">getxattr                  1080139037 samples [reqs]</span><br><span class="line">setxattr                  222105 samples [reqs]</span><br><span class="line">statfs                    11587278 samples [reqs]</span><br><span class="line">sync                      20670980 samples [reqs]</span><br><span class="line">samedir_rename            7199107 samples [reqs]</span><br><span class="line">crossdir_rename           488592 samples [reqs]</span><br><span class="line"></span><br><span class="line"># not make sure</span><br><span class="line">$ echo 0 &gt; &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;mdc&#x2F;zfsz4-*&#x2F;active</span><br><span class="line">$ lctl set_param mdc.zfsz4-*.active&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="object-server"><a href="#object-server" class="headerlink" title="object server"></a>object server</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size&#x3D;1048576</span><br><span class="line"></span><br><span class="line">options ost oss_num_threads&#x3D;0</span><br><span class="line"># you can limit the server performance by num_threads </span><br><span class="line"></span><br><span class="line">#limit ost io threads</span><br><span class="line">$ lctl set_param ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line">$ lctl set_param -P ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line"></span><br><span class="line"># rpc info</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lus...&#x2F;osc&#x2F;lus...-OST0000-osc-ffff88103a993000&#x2F;import</span><br><span class="line"></span><br><span class="line"># Get status</span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* timeout</span><br><span class="line">$ lctl get_param llite.*$FNAME*.stats</span><br><span class="line">$ lctl get_param obdfilter.*OST005e*.brw_stats</span><br><span class="line"></span><br><span class="line">#Read and print the last_rcvd file from a device</span><br><span class="line">#display client information</span><br><span class="line">$ lr_reader -c &#x2F;dev&#x2F;sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r &#x2F;dev&#x2F;sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line"># disable ost cache</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line"></span><br><span class="line">#make sure ost mount parameters</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;ldiskfs&#x2F;dm-xx&#x2F;options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid&#x3D;0</span><br><span class="line">resgid&#x3D;0</span><br><span class="line">errors&#x3D;remount-ro</span><br><span class="line">commit&#x3D;5</span><br><span class="line">min_batch_time&#x3D;0</span><br><span class="line">max_batch_time&#x3D;15000</span><br><span class="line">stripe&#x3D;0</span><br><span class="line">data&#x3D;ordered</span><br><span class="line">inode_readahead_blks&#x3D;32</span><br><span class="line">init_itable&#x3D;10</span><br><span class="line">max_dir_size_kb&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="net"><a href="#net" class="headerlink" title="net"></a>net</h3><p>I think in some the bad network quality env, you must improve them</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Peer Credits</span></span><br><span class="line"><span class="comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers </span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue </span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available <span class="keyword">for</span> this peer</span><br><span class="line">min is the smallest number of peer credits seen </span><br><span class="line">Negative credit count indicates the number of messages awaiting a credit</span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Interface Credits</span></span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min </span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 </span><br><span class="line">max is total available (i.e. value of ko2iblnd credits) </span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line"><span class="comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span></span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="comment"># the number of concurrent sends (to all peers), defaults:64</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">peer_buffer_credits=256</span><br><span class="line">concurrent_sends=256 - send work-queue sizing</span><br><span class="line"><span class="comment"># not make sure </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">the number of concurrent sends to a single peer, <span class="comment">#default:8</span></span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line"></span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line"><span class="comment">## Acceptor's listen backlog, the number of the connections the server instance can buffer in the wait queue.</span></span><br><span class="line">options lnet accept_timeout=128</span><br><span class="line"><span class="comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span></span><br><span class="line">k</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="comment"># 1024 = 1024*4KB =4MB per RPC</span></span><br><span class="line"><span class="comment">#Max RPCS in flight between OSC and OST</span></span><br><span class="line">$ lctl set_param -P <span class="variable">$FNAME</span>.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"><span class="comment"># Max number of 4K pages per RPC</span></span><br><span class="line"><span class="comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span></span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"><span class="comment"># Maximum MBs of dirty data that can be written and queued on a client</span></span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line"><span class="comment">## not make sure</span></span><br><span class="line">mds $ <span class="built_in">echo</span> 12 &gt; /proc/fs/lus.../mdc/<span class="variable">$FNAME</span>-MDT0000-mdc-ffff88091eff0800/max_mod_rpcs_in_flight</span><br><span class="line">mds $ lctl set_param mdc.<span class="variable">$FNAME</span>-MDT*-mdc-*.max_mod_rpcs_in_flight=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show --verbose</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane</span></span><br><span class="line">options lnet networks=<span class="string">"tcp1(eth1),tcp2(eth2),o2ib0(ib0)"</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">"tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]"</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span></span><br><span class="line"><span class="comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span></span><br><span class="line"><span class="comment"># fs.pipe-max-size=104857600</span></span><br><span class="line"><span class="comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it 's pipe buffer size in the ulimit, not pipe size</span></span><br><span class="line"><span class="comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here 2 tools for pipe</span></span><br><span class="line"><span class="comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span></span><br><span class="line"><span class="comment"># process1 | pv -pterbTCB 1G | process2</span></span><br><span class="line"></span><br><span class="line">$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat</span><br><span class="line">      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]</span><br><span class="line">  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            </span><br><span class="line">1538323+1 records <span class="keyword">in</span></span><br><span class="line">1538323+1 records out</span><br><span class="line">787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ mkfifo -m 777 /tmp/lfs.log</span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it 'll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:user@my.server.example.org">user@my.server.example.org</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">* MARK – Internal record keeping</span><br><span class="line">* CREAT – Regular file creation</span><br><span class="line">* MKDIR – Directory creation</span><br><span class="line">* HLINK – Hard link</span><br><span class="line">* SLINK – Soft link</span><br><span class="line">* OPEN – open file</span><br><span class="line">* CLOSE – close file</span><br><span class="line">* MKNOD – Other file creation</span><br><span class="line">* UNLNK – Regular file removal</span><br><span class="line">* RMDIR – Directory removal</span><br><span class="line">* RNMFM – Rename, original</span><br><span class="line">* RNMTO – Rename, final</span><br><span class="line">* IOCTL – ioctl on file or directory</span><br><span class="line">* TRUNC – Regular file truncated</span><br><span class="line">* SATTR – Attribute change</span><br><span class="line">* XATTR – Extended Attribute change</span><br><span class="line">* HSM – HSM action</span><br><span class="line">* UNKNW – Unkown operation</span><br></pre></td></tr></table></figure>

<h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm.max_requests&#x3D;8</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT0000.changelog_users mdd.$FNAME-MDT0000.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask&#x3D;MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<h4 id="Disable"><a href="#Disable" class="headerlink" title="Disable"></a>Disable</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Notify a device that user cl1 no longer needs records (up toand including 3)</span><br><span class="line">$ lfs changelog_clear $FNAME-MDT0000 cl1 3</span><br><span class="line"></span><br><span class="line">#To stop changelogs, changelog_mask should be set to MARK only</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line">mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line"></span><br><span class="line">#or youcan set it -all</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;-all</span><br></pre></td></tr></table></figure>

<h3 id="FSCK"><a href="#FSCK" class="headerlink" title="FSCK"></a>FSCK</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 <span class="keyword">in</span> dir <span class="comment">#145170469</span></span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem <span class="built_in">read</span>-only</span><br></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy &#x2F;dev&#x2F;sdx</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>
<p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p>
<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.mdt=ug</span><br><span class="line">mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.ost=ug</span><br><span class="line"></span><br><span class="line">client $ lfs setquota –u user1 –b 307200 –B 309200 –i 10000 –I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota –g group1 –b 5120000 –B 5150000 –i 100000 –I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota –u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 deactivate</span><br><span class="line">mds $ lctl --device 8 activate</span><br></pre></td></tr></table></figure>

<h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or </span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont 't use lfs migrate, it 's too dangerous</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Job status</span></span><br><span class="line">```bash</span><br><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn't any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats for job "dd.0" on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[client]# lfs fid2path &#x2F;mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid &#x2F;mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>

<h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line"></span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lustre to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>lfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/12/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Replace megaraid by strocli</title>
    <url>/2019/05/24/megaraid/</url>
    <content><![CDATA[<h3 id="Set-time"><a href="#Set-time" class="headerlink" title="Set time"></a>Set time</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call show time</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> time=yyyymmdd hh:mm:ss|systemtime</span><br><span class="line">$ timedatectl <span class="built_in">set</span>-timezone Asia/XXXXXX</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> time=systemtime</span><br></pre></td></tr></table></figure>

<h3 id="Set-cache-policy"><a href="#Set-cache-policy" class="headerlink" title="Set cache policy"></a>Set cache policy</h3><p>Disable write back cache</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ storcli64 &#x2F;c0&#x2F;v0 set wrcache&#x3D;wb&#x2F;wt&#x2F;awb rdcache&#x3D;ra iopolicy&#x3D;cached pdcache&#x3D;on</span><br></pre></td></tr></table></figure>

<h3 id="Create-raid10"><a href="#Create-raid10" class="headerlink" title="Create raid10"></a>Create raid10</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 add vd <span class="built_in">type</span>=raid10 drives=252:0-7 pdperarray=2 WT strip=64</span><br><span class="line">$ storcli64 /c0 add vd <span class="built_in">type</span>=raid5 drives=252:0,2,4,6 WT strip=64</span><br><span class="line">$ storcli64 /c0/vXX del force</span><br></pre></td></tr></table></figure>

<p><a href="https://support.huawei.com/enterprise/en/doc/EDOC1000111853?section=j005" target="_blank" rel="noopener">reference</a><br><img src="/img/lsi-3108-example-20191010.png" alt=""><br>Read Policy</p>
<ul>
<li>No Read Ahead: disables the Read Ahead function.</li>
<li>Read Ahead: enables the Read Ahead function. The controller pre-reads sequential data or the data predicted to be used and saves it in the cache.</li>
</ul>
<p>Write Policy</p>
<ul>
<li>Write Back: After the controller cache receives all data, the controller sends the host a message indicating that data transmission is complete.</li>
<li>Write Through: After the drive subsystem receives all data, the controller sends the host a message indicating that data transmission is complete.</li>
<li>Always Write Back: Virtual drive remains in Write Back mode if the capacitor fails or no capacitor exists.</li>
</ul>
<p>I/O Policy</p>
<ul>
<li>Direct: Data is read directly and is not cached. This is the recommended mode for common configuration.</li>
<li>Cached: All data is read from cache. This is the recommended mode for CacheCade.</li>
</ul>
<p>Access Policy</p>
<ul>
<li>Read/Write: Read and write operations are allowed.</li>
<li>Read Only: The virtual drive is read-only.</li>
<li>Blocked: The virtual drive is locked from access.</li>
</ul>
<p>Drive Cache</p>
<ul>
<li>Unchanged: uses the current cache policy.</li>
<li>Enable: writes data to the cache before writing data to the hard drive. This option improves data write performance. However, data may be lost if there is no protection mechanism against power failures.</li>
<li>Disable: writes data to a hard drive without caching the data. Data is not lost if power failures occur.</li>
</ul>
<p>Emulation Type<br>Set the sector size reported to the OS.<br>50</p>
<ul>
<li><p>If the member drive is 512B/512B:</p>
<ul>
<li>Default: The logical drive sector is 512B/512B.</li>
<li>None: The logical drive sector is 512B/512B.</li>
<li>Force: The logical drive sector is 512B/4KB.</li>
</ul>
</li>
<li><p>If the member drive is 512B/4KB:</p>
<ul>
<li>Default: The logical drive sector is 512B/4KB.</li>
<li>None: The logical drive sector is 512B/512B.</li>
<li>Force: The logical drive sector is 512B/4KB.</li>
</ul>
</li>
</ul>
<h3 id="Megaraid-FastPATH-for-SAS-SATA-SSD"><a href="#Megaraid-FastPATH-for-SAS-SATA-SSD" class="headerlink" title="Megaraid FastPATH for SAS/SATA SSD"></a>Megaraid FastPATH for SAS/SATA SSD</h3><h4 id="case-1"><a href="#case-1" class="headerlink" title="case 1"></a><a href="https://lenovopress.com/lp0592.pdf" target="_blank" rel="noopener">case 1</a></h4><p>Suggest RAID5<br>To benefit from FastPath, an array must be defined using these specific parameters:</p>
<ul>
<li>Write Through</li>
<li>Direct IO</li>
<li>No Read Ahead</li>
<li>64KB stripe size (for most workloads)</li>
<li>RAID 5(fastpath support all raid level)</li>
<li>Disk Cache Policy: This controls the write cache policy for the drives in the arrays (as opposed to the write cache policy of the RAID adapter). Enabling disk write caching can put data at risk.<ul>
<li>full initialization</li>
</ul>
</li>
</ul>
<p>With FastPath enabled, and when certain conditions are met, the MegaRAID software stack running on the RAID controller can bypass portions of the software stack, resulting in a highly efficient, streamlined code path that the RAID controller can execute very quickly. This means that the RAID controller can handle more trips though its code in a given amount of time, which equates to higher possible IOPS.</p>
<p>FastPath is not designed to speed up slow storage. It will not have an effect on storage performance unless the RAID controller is the bottleneck. If the controller itself is limiting storage performance, then enabling FastPath will allow the controller to run faster, raising performance. FastPath is not meant for use with HDD storage nor is it meant for sequential workloads.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show aso</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Premium Feature Key :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Adv S/W Opt                 Time Remaining  Mode</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">MegaRAID FastPath           Unlimited       Secured</span><br><span class="line">MegaRAID CacheCade Pro 2.0  Unlimited       Secured</span><br><span class="line">MegaRAID RAID6              Unlimited       Factory Installed</span><br><span class="line">MegaRAID RAID5              Unlimited       Factory Installed</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Re-host Information :</span><br><span class="line">===================</span><br><span class="line">Needs Re-hosting = NO</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">#### [Case 2: Aerospike Certification Tool](https://www.aerospike.com/docs/operations/plan/ssd/lsi_megacli.html)</span></span><br><span class="line">RAID Level	0</span><br><span class="line">Write Policy	Write Through</span><br><span class="line">Read Policy	No Read Ahead</span><br><span class="line">IO Policy	Direct IO</span><br><span class="line">Other	        No write to cache <span class="keyword">if</span> bad BBU</span><br><span class="line"></span><br><span class="line">Dell R720xd</span><br><span class="line">PERC H710p RAID controller</span><br><span class="line">8 x 200 GB Intel s3700 SSDs</span><br><span class="line"></span><br><span class="line">Latency measures during torture tests (96,000 reads/sec and 48,000 concurrent writes/second).</span><br><span class="line">RAID setting	% &gt;1 ms	% &gt;2 ms	% &gt;4 ms	% &gt;8 ms	% &gt;16 ms</span><br><span class="line">Default	        22.20	13.96	4.29	0.22	0.00</span><br><span class="line">FastPath™	4.31	0.85	0.17	0.00	0.00</span><br><span class="line"></span><br><span class="line">The latencies <span class="keyword">for</span> the FastPath™ are much better. In general better latency results mean much higher threshold <span class="keyword">for</span> throughput.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Storcli replace Megacli</span></span><br><span class="line"><span class="comment">#### BBU info</span></span><br><span class="line">```bash</span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -AdpBbuCmd -getBbuProperties -aALL</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU Properties <span class="keyword">for</span> Adapter: 0</span><br><span class="line"></span><br><span class="line">  Auto Learn Period: 30 Days</span><br><span class="line">  Next Learn time: Mon Jun  3 12:02:38 2019</span><br><span class="line"></span><br><span class="line">  Learn Delay Interval:0 Hours</span><br><span class="line">  Auto-Learn Mode: Enabled</span><br><span class="line">  BBU Mode = 4</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show properties</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Properties :</span><br><span class="line">==============</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Property             Value</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Auto Learn Period    30d (2592000 seconds)</span><br><span class="line">Next Learn time      2019/06/03  12:02:38 (612878558 seconds)</span><br><span class="line">Learn Delay Interval 0 hour(s)</span><br><span class="line">Auto-Learn Mode      Enabled</span><br><span class="line">BBU Mode             4</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show status</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Info :</span><br><span class="line">========</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">Property      Value</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">BatteryType   iBBU08</span><br><span class="line">Voltage       4056 mV</span><br><span class="line">Current       0 mA</span><br><span class="line">Temperature   31 C</span><br><span class="line">Battery State Optimal</span><br><span class="line">Design Mode   1: 12+ Hrs retention with a transparent learn cycle</span><br><span class="line">                 and best service life.</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Firmware_Status :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Property                                   Value</span><br><span class="line">-------------------------------------------------</span><br><span class="line">Charging Status                            None</span><br><span class="line">Voltage                                    OK</span><br><span class="line">Temperature                                OK</span><br><span class="line">Learn Cycle Requested                      No</span><br><span class="line">Learn Cycle Active                         No</span><br><span class="line">Learn Cycle Status                         OK</span><br><span class="line">Learn Cycle Timeout                        No</span><br><span class="line">I2C Errors Detected                        No</span><br><span class="line">Battery Pack Missing                       No</span><br><span class="line">Battery Replacement required               No</span><br><span class="line">Remaining Capacity Low                     No</span><br><span class="line">Periodic Learn Required                    No</span><br><span class="line">Transparent Learn                          No</span><br><span class="line">No space to cache offload                  No</span><br><span class="line">Pack is about to fail &amp; should be replaced No</span><br><span class="line">Cache Offload premium feature required     No</span><br><span class="line">Module microcode update required           No</span><br><span class="line">-------------------------------------------------</span><br><span class="line"></span><br><span class="line">GasGaugeStatus :</span><br><span class="line">==============</span><br><span class="line"></span><br><span class="line">--------------------------------------</span><br><span class="line">Property                   Value</span><br><span class="line">--------------------------------------</span><br><span class="line">Fully Discharged           No</span><br><span class="line">Fully Charged              No</span><br><span class="line">Discharging                No</span><br><span class="line">Initialized                Yes</span><br><span class="line">Remaining Time Alarm       No</span><br><span class="line">Terminate Discharge Alarm  No</span><br><span class="line">Over Temperature           No</span><br><span class="line">Charging Terminated        No</span><br><span class="line">Over Charged               No</span><br><span class="line">Relative State of Charge   97%</span><br><span class="line">Charger System State       1</span><br><span class="line">Charger System Ctrl        0</span><br><span class="line">Charging current           0 mA</span><br><span class="line">Absolute state of charge   78%</span><br><span class="line">Max Error                  0%</span><br><span class="line">Battery backup charge time 48 hours +</span><br><span class="line">--------------------------------------</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show status | grep -i <span class="built_in">type</span></span><br><span class="line">BatteryType   iBBU08</span><br><span class="line"></span><br><span class="line"><span class="comment">#important value</span></span><br><span class="line">Remaining Capacity Low                  : No</span><br><span class="line"></span><br><span class="line"><span class="comment">#design capacity: 1500, max</span></span><br><span class="line">Relative State of Charge ＝ 1055 ／ 1198 ～ 89%</span><br><span class="line">Absolute State of charge ＝ 1055 ／ 1500 ～ 70%</span><br></pre></td></tr></table></figure>

<h4 id="Setting-BBU"><a href="#Setting-BBU" class="headerlink" title="Setting BBU"></a>Setting BBU</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /cx/bbu <span class="built_in">set</span> learnDelayInterval=168 <span class="comment">#hours</span></span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show modes</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = Get BBU Modes Succeeded.</span><br><span class="line"></span><br><span class="line">Available BBU Modes :</span><br><span class="line">===================</span><br><span class="line">Mode-1 = 12+ Hrs retention with a transparent learn cycle and best service life.</span><br><span class="line">Mode-3 = 24+ Hrs retention with a transparent learn cycle and balanced service life.</span><br><span class="line">Mode-4 = 48+ Hrs retention with a non-transparent learn cycle and balanced service life</span><br><span class="line"></span><br><span class="line">$ storcli /cx/bbu <span class="built_in">set</span> bbuMode=&lt;value&gt;</span><br><span class="line">0 48 hours of retentiona at 60 °C, 1-year Service Life.</span><br><span class="line">a. Indicates how long the battery can hold data <span class="keyword">in</span> the controller<span class="string">'s memory in case of accidental system shutdown.</span></span><br><span class="line"><span class="string">1 12 hours of retention at 45 °C, 5-year Service Life, transparent learn.b</span></span><br><span class="line"><span class="string">b. The controller'</span>s performance is not affected during the battery<span class="string">'s learn cycle.</span></span><br><span class="line"><span class="string">2 12 hours of retention at 55 °C, 3-year Service Life, transparent learn.</span></span><br><span class="line"><span class="string">3 24 hours of retention at 45 °C, 3-year Service Life, transparent learn.</span></span><br><span class="line"><span class="string">4 48 hours of retention at 45 °C, 3-year Service Life.</span></span><br><span class="line"><span class="string">5 48 hours of retention at 55 °C, 1-year Service Life.</span></span><br><span class="line"><span class="string">6 Same as the description for BBU mode 5. The BBU mode 6 enables you to receive events when the battery capacity reaches suboptimal and critical thresholds.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli /cx bbu set autolearnmode=&lt;value&gt;</span></span><br><span class="line"><span class="string">where x= 0 – Enabled, 1 – Disabled, 2 – Warn though event.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## Manual learn battery</span></span><br><span class="line"><span class="string">$ storcli /c0/bbu start learn</span></span><br></pre></td></tr></table></figure>

<h4 id="Cachevault"><a href="#Cachevault" class="headerlink" title="Cachevault"></a><a href="https://www.thomas-krenn.com/en/wiki/CacheVault_Flash_Cache" target="_blank" rel="noopener">Cachevault</a></h4><p>Here is not install cachevalut, all FLASH devices don’t need it, because fastpath need writethrough mode</p>
<p>No battery is used with CacheVault technology. The cache content is protected by the following systems:</p>
<ul>
<li>A double-layer capacitor is connected to the RAID controller.</li>
<li>This will be fully loaded automatically during server startup.</li>
<li>In case of power failure the RAID controller uses the power of the capacitor to write the Flash-Memory to maintain all contents of the cache non-volatile (almost like a USB stick).</li>
<li>The next time the server is started the RAID controller writes the data from the Flash-Memory to the RAID Array.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0/cv show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Failure</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line">Detailed Status :</span><br><span class="line">===============</span><br><span class="line"></span><br><span class="line">----------------------------------------------------</span><br><span class="line">Ctrl Status Property ErrMsg                   ErrCd</span><br><span class="line">----------------------------------------------------</span><br><span class="line">   0 Failed -        Cachevault doesn<span class="string">'t exist   255</span></span><br><span class="line"><span class="string">----------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show all</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show learn</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show status</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv start learn</span></span><br></pre></td></tr></table></figure>

<h4 id="Get-all-megaraid-log-not-filter"><a href="#Get-all-megaraid-log-not-filter" class="headerlink" title="Get all megaraid log, not filter"></a>Get all megaraid log, not filter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shows the history of log files generated</span></span><br><span class="line">$ storcli64  /call show eventloginfo file=logfile</span><br><span class="line">$ MegaCli64 -AdpEventLog -GetEvents -f logfile -aALL</span><br><span class="line"></span><br><span class="line"><span class="comment">#prints the system log</span></span><br><span class="line">$ storcli64 /c0 show events file=megaraid_events.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># shows the firmware logs</span></span><br><span class="line">$ storcli64 /c0 show termlog </span><br><span class="line">$ storcli64 /c0 show termlog <span class="built_in">type</span>=config</span><br><span class="line">$ storcli64 /c0 show termlog <span class="built_in">type</span>=contents</span><br></pre></td></tr></table></figure>

<h4 id="Show-raid-logic-info"><a href="#Show-raid-logic-info" class="headerlink" title="Show raid logic info"></a>Show raid logic info</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx/dall show [all]</span><br><span class="line">$ MegaCli64 -cfgdsply -aALL</span><br></pre></td></tr></table></figure>

<h4 id="Import-license"><a href="#Import-license" class="headerlink" title="Import license"></a>Import license</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /c0 <span class="built_in">set</span> aso key=xxxxxxxxx</span><br></pre></td></tr></table></figure>

<h4 id="Backup-config"><a href="#Backup-config" class="headerlink" title="Backup config"></a>Backup config</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli -CfgSave -f filename -aN   </span><br><span class="line">$ MegaCli -CfgRestore -f filename -aN  </span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 <span class="built_in">set</span> config file=raid_adapter0.config</span><br><span class="line">$ storcli64 /c0 get config file=raid_adapter0.config</span><br></pre></td></tr></table></figure>

<h4 id="Interrupt-BGI"><a href="#Interrupt-BGI" class="headerlink" title="Interrupt BGI"></a>Interrupt BGI</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -LDBI -Abort -LALL -aALL</span><br><span class="line">$ storcli64 /call/vall <span class="built_in">set</span> autobgi=On|Off</span><br><span class="line">$ storcli64 /call/vall show autobgi</span><br><span class="line">$ storcli64 /call/vall stop bgi</span><br><span class="line">$ storcli64 /call/vall pause bgi</span><br><span class="line">$ storcli64 /call/vall resume bgi</span><br><span class="line">$ storcli64 /call/vall show bgi</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> bgirate=90</span><br></pre></td></tr></table></figure>

<h4 id="Rebuild-Migrate-init"><a href="#Rebuild-Migrate-init" class="headerlink" title="Rebuild/Migrate/init"></a>Rebuild/Migrate/init</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64  -AdpSetProp RebuildRate 50 -a0</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> rebuildrate=50</span><br><span class="line"></span><br><span class="line">$ MegaCli64  -AdpSetProp ReconRate 50 -a0</span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDRecon ShowProg L1  -a0</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> migraterate=50</span><br><span class="line">$ storcli /c0/v1 show migrate</span><br><span class="line">$ storcli /c0/v1 start migrate <span class="built_in">type</span>=raidx</span><br><span class="line"></span><br><span class="line"><span class="comment">#  automatic background initialization</span></span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDBI -ShowProg -LALL -aALL</span><br><span class="line">$ storcli /call/vall <span class="built_in">set</span> autobgi=on</span><br><span class="line">$ storcli /call/vall show autobgi</span><br><span class="line">$ storcli /call/vall stop bgi</span><br><span class="line">$ storcli /call/vall pause bgi</span><br><span class="line">$ storcli /call/vall resume bgi</span><br><span class="line">$ storcli /call/vall show bgi</span><br><span class="line"></span><br><span class="line"><span class="comment"># virtual drive initialization</span></span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDInit ShowProg Lall a0</span><br><span class="line">$ storcli /c0/vall start init</span><br><span class="line">$ storcli /c0/vall stop init</span><br><span class="line">$ storcli /c0/vall show init</span><br></pre></td></tr></table></figure>

<h4 id="Make-device-good"><a href="#Make-device-good" class="headerlink" title="Make device good"></a>Make device good</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -PDMakeGood -PhysDrv[20:12] -a0</span><br><span class="line">$ storcli64 /c0/e20/s12 <span class="built_in">set</span> good force</span><br></pre></td></tr></table></figure>

<h4 id="Make-JBOD"><a href="#Make-JBOD" class="headerlink" title="Make JBOD"></a>Make JBOD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -PDMakeJBOD -PhysDrv[E0:S0,E1:S1,...] -aN|-a0,1,2|-aALL</span><br><span class="line">$ storcli64 /c0/e20/s12 <span class="built_in">set</span> jbod</span><br><span class="line"></span><br><span class="line">$ MegaCli -AdpSetProp -EnableJBOD -val -aN|-a0,1,2|-aALL</span><br><span class="line">      val - 0=Disable JBOD mode.</span><br><span class="line">            1=Enable JBOD mode.</span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 <span class="built_in">set</span> jbod=on</span><br></pre></td></tr></table></figure>

<h4 id="Foreign-devices"><a href="#Foreign-devices" class="headerlink" title="Foreign devices"></a>Foreign devices</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64  -CfgForeign -Clear -aALL</span><br><span class="line">$ storcli64 /c0/fall del|delete</span><br><span class="line"></span><br><span class="line">$ storcli64 /c0/fall show</span><br><span class="line">$ storcli64 /c0/fall import</span><br></pre></td></tr></table></figure>

<h3 id="About-Consistency-and-patrol-read"><a href="#About-Consistency-and-patrol-read" class="headerlink" title="About Consistency and patrol read"></a><a href="https://www.digiliant.com/docs/MegaRAID-SAS-Software-User-Guide-12Gbs.pdf" target="_blank" rel="noopener">About Consistency and patrol read</a></h3><h4 id="Patrol-read"><a href="#Patrol-read" class="headerlink" title="Patrol read"></a>Patrol read</h4><p>Patrol read involves the review of your system for possible drive errors that could lead to drive failure and then action to correct errors. The goal is to protect data integrity by detecting drive failure before the failure can damage data. The corrective actions depend on the drive group configuration and the type of errors.<br>Patrol read starts only when the controller is idle for a defined period of time and no other background tasks are active, though it can continue to run during heavy I/O processes.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show patrolRead</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line">Ctrl_Prop               Value</span><br><span class="line">---------------------------------------------</span><br><span class="line">PR Mode                 Auto</span><br><span class="line">PR Execution Delay      168 hours</span><br><span class="line">PR iterations completed 0</span><br><span class="line">PR Next Start time      08/24/2019, 03:00:00</span><br><span class="line">PR on SSD               Disabled</span><br><span class="line">PR on EPD               Disabled</span><br><span class="line">PR Current State        Stopped</span><br><span class="line">---------------------------------------------</span><br><span class="line"><span class="comment"># mode=auto/manual</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=on mode=auto includessds=on maxconcurrentpd=255 delay=360 <span class="comment">#255 phy devs ,360 housr(15days)</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=on mode=auto starttime=2019/08/24 03</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=off</span><br><span class="line"></span><br><span class="line"><span class="comment">##show/stop/start/resume/pause</span></span><br><span class="line">$ storcli64 /call start patrolRead </span><br><span class="line">$ storcli64 /call show patrolRead</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">----------------------------------</span><br><span class="line">Ctrl_Prop               Value</span><br><span class="line">----------------------------------</span><br><span class="line">PR Mode                 Auto</span><br><span class="line">PR Execution Delay      360 hours</span><br><span class="line">PR iterations completed 352</span><br><span class="line">PR on SSD               Enabled</span><br><span class="line">PR Current State        Stopped</span><br><span class="line">----------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli64 /call show prrate</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> prrate=40</span><br></pre></td></tr></table></figure>

<h4 id="Consistency-Check"><a href="#Consistency-Check" class="headerlink" title="Consistency Check"></a>Consistency Check</h4><p>checking consistency means calculating the data on one drive and comparing the results to the contents of the parity drive<br>It is recommended that you perform a consistency check at least once a month.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-----------------------------------------------</span><br><span class="line">Ctrl_Prop                 Value</span><br><span class="line">-----------------------------------------------</span><br><span class="line">CC Operation Mode         Concurrent</span><br><span class="line">CC Execution Delay        168</span><br><span class="line">CC Next Starttime         08/24/2019, 02:00:00</span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   311</span><br><span class="line">CC Number of VD completed 2</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line">-----------------------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli64 /call show ccrate</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> ccrate=50</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#schedule</span></span><br><span class="line">$ storcli64 /call/vall <span class="built_in">set</span> cc=seq </span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> cc=seq starttime=2019/08/31 02 delay=1440  <span class="comment">## check interval 1440 hours ,60 days</span></span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">---------------------------------</span><br><span class="line">Ctrl_Prop    Value</span><br><span class="line">---------------------------------</span><br><span class="line">CC Mode      SEQ</span><br><span class="line">CC Starttime 2019/08/31 02:00:00</span><br><span class="line">CC delay     1440</span><br><span class="line">---------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">### if I 'm not set start time</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> cc=seq delay=1440</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line">Ctrl_Prop Value</span><br><span class="line">----------------</span><br><span class="line">CC Mode   SEQ</span><br><span class="line">CC delay  1440</span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">CC Operation Mode         Sequential</span><br><span class="line">CC Execution Delay        1440</span><br><span class="line">CC Next Starttime         08/24/2019, 03:00:00 <span class="comment">## first start after about 12 hours</span></span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   120</span><br><span class="line">CC Number of VD completed 1</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### show again</span></span><br><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-----------------------------------------------</span><br><span class="line">Ctrl_Prop                 Value</span><br><span class="line">-----------------------------------------------</span><br><span class="line">CC Operation Mode         Sequential</span><br><span class="line">CC Execution Delay        1440</span><br><span class="line">CC Next Starttime         08/31/2019, 02:00:00</span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   311</span><br><span class="line">CC Number of VD completed 2</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line">-----------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#manual start and show status</span></span><br><span class="line"><span class="comment">#show/stop/start/resume/pause</span></span><br><span class="line">$ storcli64 /call/vall start cc [force]</span><br><span class="line">$ storcli64 /call/vall show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VD Operation Status :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">-----------------------------------</span><br><span class="line">VD Operation Progress% Status</span><br><span class="line">-----------------------------------</span><br><span class="line"> 0 CC                0 In progress</span><br><span class="line"> 1 CC                1 In progress</span><br><span class="line">-----------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ storcli64 /call/vall stop cc</span><br></pre></td></tr></table></figure>

<h3 id="Montor-cache-ECC-count"><a href="#Montor-cache-ECC-count" class="headerlink" title="Montor cache ECC count"></a>Montor cache ECC count</h3><p>eccbucketleakrate 0 to 65535 Sets the leak rate of the single-bit bucket in minutes (one entry removed per leak-rate).<br>eccbucketsize 0 to 255 Sets the size of ECC single-bit-error bucket (logs event when full).</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show all | grep Bucket</span><br><span class="line">ECC Bucket Count = 0</span><br><span class="line">ECC Bucket Size = 255</span><br><span class="line">ECC Bucket Leak Rate (hrs) = 4</span><br></pre></td></tr></table></figure>

<h3 id="About-device-write-cache-in-megaraid"><a href="#About-device-write-cache-in-megaraid" class="headerlink" title="About device write cache in megaraid"></a><a href="https://utcc.utoronto.ca/~cks/space/blog/tech/ModernDiskWriteCaches" target="_blank" rel="noopener">About device write cache in megaraid</a></h3><p>drives can also support a write option called ‘Force Unit Access’ (FUA) that bypasses the write cache in order to force what you’re writing to be forced to disk. In general FUA is bundled with another feature called ‘Disable Page Out’ (DPO), which tells the drive that putting the data into cache is not useful.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ Virtual Drives = 2</span><br><span class="line"></span><br><span class="line">VD LIST :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">DG/VD TYPE  State Access Consist Cache Cac sCC     Size Name</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">0/0   RAID1 Optl  RW     Yes     NRWTD -   OFF 223.0 GB R1OS</span><br><span class="line">1/1   RAID5 Optl  RW     Yes     RWTD  -   OFF 1.307 TB</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">Cac=CacheCade|Rec=Recovery|OfLn=OffLine|Pdgd=Partially Degraded|dgrd=Degraded</span><br><span class="line">Optl=Optimal|RO=Read Only|RW=Read Write|HD=Hidden|B=Blocked|Consist=Consistent|</span><br><span class="line">R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack|</span><br><span class="line">AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled</span><br><span class="line">Check Consistency</span><br><span class="line"></span><br><span class="line">Physical Drives = 6</span><br><span class="line"></span><br><span class="line">PD LIST :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">EID:Slt DID State DG       Size Intf Med SED PI SeSz Model         Sp</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">32:0      0 Onln   0   223.0 GB SATA SSD N   N  512B THNSF8240CCSE U</span><br><span class="line">32:1      1 Onln   0   223.0 GB SATA SSD N   N  512B THNSF8240CCSE U</span><br><span class="line">32:2      2 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:3      3 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:4      4 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:5      5 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ dmesg -T | grep DPO</span><br><span class="line">[Tue Nov  5 23:21:47 2019] sd 0:2:1:0: [sdb] Write cache: disabled, <span class="built_in">read</span> cache: enabled, supports DPO and FUA</span><br><span class="line">[Tue Nov  5 23:21:47 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line"></span><br><span class="line">$ perccli64 /c0/vall <span class="built_in">set</span> wrcache=WT rdcache=nora iopolicy=direct</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line">Detailed Status :</span><br><span class="line">===============</span><br><span class="line"></span><br><span class="line">----------------------------------------</span><br><span class="line">VD Property Value  Status  ErrCd ErrMsg</span><br><span class="line">----------------------------------------</span><br><span class="line"> 0 wrCache  WT     Success     0 -</span><br><span class="line"> 0 rdCache  NoRA   Success     0 -</span><br><span class="line"> 0 IoPolicy Direct Success     0 -</span><br><span class="line"> 1 wrCache  WT     Success     0 -</span><br><span class="line"> 1 rdCache  NoRA   Success     0 -</span><br><span class="line"> 1 IoPolicy Direct Success     0 -</span><br><span class="line">----------------------------------------</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$ dmesg -T | grep DPO</span><br><span class="line">[Thu Nov  7 02:37:51 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line">[Thu Nov  7 02:37:51 2019] sd 0:2:1:0: [sdb] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line"></span><br><span class="line">Supported VD Operations :</span><br><span class="line">=======================</span><br><span class="line">Read Policy = Yes</span><br><span class="line">Write Policy = Yes</span><br><span class="line">IO Policy = Yes</span><br><span class="line">Access Policy = Yes</span><br><span class="line">Disk Cache Policy = Yes</span><br><span class="line"></span><br><span class="line">$ perccli64 /c0/v0 <span class="built_in">set</span> wrcache=WB</span><br><span class="line">VD Property Value Status  ErrCd ErrMsg</span><br><span class="line">---------------------------------------</span><br><span class="line"> 0 wrCache  WB    Success     0 -</span><br><span class="line">---------------------------------------</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$  dmesg -T | grep DPO</span><br><span class="line">[Thu Nov  7 02:53:00 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, doesn<span class="string">'t support DPO or FUA</span></span><br><span class="line"><span class="string">[Thu Nov  7 02:53:00 2019] sd 0:2:1:0: [sdb] Write cache: disabled, read cache: disabled, supports DPO and FUA</span></span><br></pre></td></tr></table></figure>
<p>You could change megaraid cache policy, let the device support DPO and FUA</p>
<h3 id="Backup-and-restore-config-file"><a href="#Backup-and-restore-config-file" class="headerlink" title="Backup and restore config file"></a>Backup and restore config file</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#backup controller config to file;</span></span><br><span class="line">./storcli64 /c0 get config file=megaraid.cfg</span><br><span class="line">./storcli64 /c0 <span class="built_in">set</span> config file=megaraid.cfg</span><br><span class="line"><span class="comment">#Clear a Configuration; dangerous</span></span><br><span class="line"><span class="comment">###./storcli64 /c0 delete config[force]</span></span><br></pre></td></tr></table></figure>

<h3 id="Flush-RAID-write-cache"><a href="#Flush-RAID-write-cache" class="headerlink" title="Flush RAID write cache"></a>Flush RAID write cache</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Cache Flush on Selected Controller</span><br><span class="line">$ MegaCli64 –AdpCacheFlush -aN|-a0,1,2|-aALL</span><br><span class="line">$ storcli64 &#x2F;c0 set flushwriteverify&#x3D;on</span><br><span class="line">$ storcli64 &#x2F;c0 show flushwriteverify </span><br><span class="line">$ storcli64 &#x2F;c0 flushcache</span><br><span class="line"></span><br><span class="line">$ perccli64 &#x2F;c0 flushcache</span><br><span class="line">Controller &#x3D; 0</span><br><span class="line">Status &#x3D; Success</span><br><span class="line">Description &#x3D; Adapter and&#x2F;or disk caches flushed successfully.</span><br><span class="line"></span><br><span class="line"># delete the preserved cache for a particular virtual driver on the controller in missing state</span><br><span class="line">$ perccli64 &#x2F;c0&#x2F;v1 delete preservedCache </span><br><span class="line">$ storcli64 &#x2F;c0&#x2F;v1 delete preservedCache force</span><br><span class="line"></span><br><span class="line">$ storcli64 &#x2F;c0 show preservedCache</span><br><span class="line">Controller &#x3D; 0</span><br><span class="line">Status &#x3D; Success</span><br><span class="line">Description &#x3D; No Virtual Drive has Preserved Cache Data.</span><br></pre></td></tr></table></figure>

<h3 id="show-PHY"><a href="#show-PHY" class="headerlink" title="show PHY"></a>show PHY</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /cx/px|pall <span class="built_in">set</span> linkspeed=0(auto)|1.5|3|6|12</span><br><span class="line">$ perccli64 /c0/pall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PhyInfo :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line">PhyNo SAS_Addr           Phy_Identifier Link_Speed Device_Type   Description</span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line">    0 0x500056B3BECB2FFF             17 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    1 0x500056B3BECB2FFF             20 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    2 0x500056B3BECB2FFF             18 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    3 0x500056B3BECB2FFF             23 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    4 0x500056B3BECB2FFF             19 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    5 0x500056B3BECB2FFF             21 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    6 0x500056B3BECB2FFF             16 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    7 0x500056B3BECB2FFF             22 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ perccli64 /call/eall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Properties :</span><br><span class="line">==========</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">EID State Slots PD PS Fans TSs Alms SIM Port<span class="comment"># ProdID VendorSpecific</span></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"> 32 OK        8  6  0    0   0    0   0 00 x1 BP14G+  !   ?</span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">EID-Enclosure Device ID |PD-Physical drive count |PS-Power Supply count|</span><br><span class="line">TSs-Temperature sensor count |Alms-Alarm count |SIM-SIM Count</span><br><span class="line"></span><br><span class="line">$ perccli64 /call/eall show status</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"><span class="comment"># upgrade the enclosure</span></span><br><span class="line">$ storcli64 /c0/e32 download src=enclosure_fw.bin</span><br></pre></td></tr></table></figure>

<h3 id="Foreign-configurations"><a href="#Foreign-configurations" class="headerlink" title="Foreign configurations"></a>Foreign configurations</h3><p>NOTE Provide the security key when importing a locked foreign configuration created in a different machine that is encrypted with a security key.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0/fall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = Couldn<span class="string">'t find any foreign Configuration</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli64 /c0/fall import</span></span><br><span class="line"><span class="string">$ storcli64 /cx/fx|fall show [all][ securitykey=sssssssssss ]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#This command deletes the foreign configuration of a controller.</span></span><br><span class="line"><span class="string">$ ### storcli64 /c0/fall delete</span></span><br></pre></td></tr></table></figure>

<h3 id="Dimmer-switch"><a href="#Dimmer-switch" class="headerlink" title="Dimmer switch"></a>Dimmer switch</h3><p>The Dimmer Switch is the power-saving policy for the virtual drive</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0 show ds</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-------------------------------</span><br><span class="line">Ctrl_Prop         Value</span><br><span class="line">-------------------------------</span><br><span class="line">SpnDwnUncDrv      Disabled</span><br><span class="line">SpnDwnHS          Disabled</span><br><span class="line">SpnDwnTm          30 minute(s)</span><br><span class="line">SpnDwnCfgDrv      Disabled</span><br><span class="line">DefaultLdPSPolicy None</span><br><span class="line">DsblLdPsInterval  0 hours</span><br><span class="line">DsblLdPsTime      0 minutes</span><br><span class="line">SpnUpEncDly       8 second(s)</span><br><span class="line">spinUpEncDrvCnt   6</span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli /cx/vx <span class="built_in">set</span> ds=default | auto | none | max | maxnocache</span><br><span class="line">auto: Logical device power savings are managed by the firmware.</span><br><span class="line">none: No power saving policy.</span><br><span class="line">max: Logical device uses maximum power savings.</span><br><span class="line">maxnocache: Logical device does not cache write to maximise power savings.</span><br><span class="line"></span><br><span class="line">$ storcli64  /c0 <span class="built_in">set</span> ds=on <span class="built_in">type</span>=1|2</span><br><span class="line">1: Unconfigured</span><br><span class="line">2: Hot spare</span><br><span class="line">3: Virtual drive</span><br><span class="line">4: All</span><br><span class="line"></span><br><span class="line">$ storcli /cx <span class="built_in">set</span> ds=on [properties]</span><br><span class="line">disableldps: Interval <span class="keyword">in</span> hours or time <span class="keyword">in</span> hh:mm format</span><br><span class="line">spinupdrivecount: Valid enclosure number (0 to 255)</span><br><span class="line">SpinUpEncDelay: Valid time <span class="keyword">in</span> seconds</span><br></pre></td></tr></table></figure>

<h3 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx[/ex]/sx spindown</span><br><span class="line">$ storcli64 /cx[/ex]/sx spinup</span><br><span class="line"></span><br><span class="line">$ storcli64 /cx[/ex]/sx secureerase [force]</span><br><span class="line">$ storcli64 /cx[/ex]/sx start erase [simple|normal|thorough] [erasepatternA=&lt;value1&gt;] [erasepatternB=&lt;value2&gt;]</span><br><span class="line">$ storcli64 /cx[/ex]/sx stop erase</span><br></pre></td></tr></table></figure>

<h3 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h3><h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>— 0 – NA<br>— 1– SET<br>— 2 – CLEAR<br>— 3 – CLEAR ALL<br>— 4 – DEBUG DUMP</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call show all logfile=megaraid.cfg</span><br><span class="line">$ storcli64 /c x <span class="built_in">set</span> debug reset all </span><br><span class="line">$ storcli64 /call x <span class="built_in">set</span> debug <span class="built_in">type</span> = &lt;value&gt; option = &lt;value&gt; level = [&lt;value <span class="keyword">in</span> hex&gt;]</span><br><span class="line"><span class="comment">#type – takes the value from 0 – 128, mapping each number to a particular debug variable in the firmware.</span></span><br><span class="line">$ <span class="comment">#storcli64 /call delete events</span></span><br><span class="line">$ <span class="comment">#storcli64 /call delete termlog</span></span><br><span class="line"></span><br><span class="line">$ storcli64 /call show events file=events.log</span><br><span class="line">$ storcli64 /call show termlog <span class="built_in">type</span>=config logfile=termlog.config</span><br><span class="line">$ storcli64 /call show termlog <span class="built_in">type</span>=contents logfile=termlog.contents</span><br><span class="line">$ storcli64 /call show eventloginfo logfile=event.info</span><br><span class="line">$ storcli64 /call show dequeuelog file=dequeue.log</span><br><span class="line">$ storcli64 /call show alilog logfile=ali.log</span><br></pre></td></tr></table></figure>
<h4 id="Encolsure"><a href="#Encolsure" class="headerlink" title="Encolsure"></a>Encolsure</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call/eall download src=encolsure.bin</span><br><span class="line">$ perccli64 /call/eall show all</span><br><span class="line">$ perccli64 /call/eall show status</span><br></pre></td></tr></table></figure>

<h4 id="phy-err"><a href="#phy-err" class="headerlink" title="phy err"></a>phy err</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">storcli /cx/px|pall show</span><br><span class="line">storcli /cx/px|pall show all</span><br><span class="line">storcli /cx/ex show phyerrorcounters</span><br><span class="line">storcli /cx[/ex]/sx show phyerrorcounters</span><br><span class="line">storcli /cx[/ex]/sx reset phyerrorcounters</span><br><span class="line">storcli /cx/px|pall <span class="built_in">set</span> linkspeed=0(auto)|1.5|3|6|12</span><br></pre></td></tr></table></figure>

<h4 id="diag-adapter"><a href="#diag-adapter" class="headerlink" title="diag adapter"></a>diag adapter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0 start diag duration=10 logfile=diag.log</span><br></pre></td></tr></table></figure>

<h4 id="Enable-SEP-for-Dell-PERC"><a href="#Enable-SEP-for-Dell-PERC" class="headerlink" title="Enable SEP for Dell PERC"></a>Enable SEP for Dell PERC</h4><p>Configures enclosure detection on a non-SES/expander backplane.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call <span class="built_in">set</span> backplane mode=0 expose=on</span><br><span class="line">0: Use autodetect logic of backplanes, such as SGPIO and I2C SEP using GPIO pins.</span><br><span class="line">1: Disable autodetect SGPIO.</span><br><span class="line">2: Disable I2C SEP autodetect.</span><br><span class="line">3: Disable both the autodetects.</span><br><span class="line"></span><br><span class="line">backplane expose: Enables or disables device drivers to expose enclosure devices; <span class="keyword">for</span> example, expanders, SEPs.</span><br><span class="line"></span><br><span class="line">$ perccli64 /call <span class="built_in">set</span> sesmonitoring=on</span><br></pre></td></tr></table></figure>


<h3 id="Enable-JBOD"><a href="#Enable-JBOD" class="headerlink" title="Enable JBOD"></a>Enable JBOD</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call <span class="built_in">set</span> jbod=on</span><br></pre></td></tr></table></figure>

<h3 id="SAS-loading-balance"><a href="#SAS-loading-balance" class="headerlink" title="SAS loading balance"></a>SAS loading balance</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx <span class="built_in">set</span> loadbalancemode=on</span><br></pre></td></tr></table></figure>

<h3 id="Driver-performance"><a href="#Driver-performance" class="headerlink" title="Driver performance"></a>Driver performance</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call <span class="built_in">set</span> DPM=on</span><br><span class="line"><span class="comment">#Enables or disables drive performance monitoring</span></span><br><span class="line"></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> ncq=on</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> perfmode=1 <span class="comment">#default was 0</span></span><br><span class="line">0: Tuned to provide best IOPS, currently applicable to non-FastPath</span><br><span class="line">1: Tuned to provide least latency, currently applicable to non-FastPath</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>raid</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title>The block device</title>
    <url>/2016/08/10/block_device/</url>
    <content><![CDATA[<h3 id="Hardware-driver"><a href="#Hardware-driver" class="headerlink" title="Hardware driver"></a>Hardware driver</h3><h4 id="mpt3sas-mpt2sas-driver-parameter"><a href="#mpt3sas-mpt2sas-driver-parameter" class="headerlink" title="mpt3sas/mpt2sas driver parameter"></a>mpt3sas/mpt2sas driver parameter</h4><p>parm: command_retry_count: Device discovery TUR command retry count: (default=144) (int)<br>retry count default was 144, it ‘s too large</p>
<p>parm: max_queue_depth: max controller queue depth (int) </p>
<p>The Linux “scatter/gather” table size needs to be large enough to allow IO_SIZE IO, if possible. For most drivers, this typically requires a “sg_tablesize” value of 256 or greater for 4MB IO. Different vendors have different defaults for this parameter, and may require a modprobe.conf entry to increase the value. The QLogic driver defaults to a value of 1024. For Emulex, a modeprobe.conf entry needs to be added to increase the value to 256 or greater, such as:<br>options lpfc lpfc_sg_seg_cnt=256</p>
<p>The LSI SAS driver, “mpt2sas”, uses the parameter named “max_sgl_entries” to control this value.<br>Its maximum value in RHEL 6.x currently only 128<br>options mpt2sas max_sgl_entries=256<br><a href="https://www.ibm.com/developerworks/community/forums/html/topic?id=77777777-0000-0000-0000-000014889306" target="_blank" rel="noopener">not make sure</a><br>looks like the setting will not work, I ‘m not sure the driver has update.</p>
<p>it ‘s could not be modify, if you want reduce memory overhead, you cold turn it down</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lspci | grep 2308</span><br><span class="line">01:00.0 Serial Attached SCSI controller: LSI Logic &#x2F; Symbios Logic SAS2308 PCI-Express Fusion-MPT SAS-2 (rev 05)</span><br><span class="line">$ cat &#x2F;sys&#x2F;devices&#x2F;pci0000:00&#x2F;0000:00:02.1&#x2F;0000:01:00.0&#x2F;host1&#x2F;scsi_host&#x2F;host1&#x2F;sg_tablesize</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line">echo 256 &gt; &#x2F;sys&#x2F;devices&#x2F;pci0000:00&#x2F;0000:00:02.1&#x2F;0000:01:00.0&#x2F;host1&#x2F;scsi_host&#x2F;host1&#x2F;sg_tablesize</span><br><span class="line">echo: write error: Input&#x2F;output error</span><br></pre></td></tr></table></figure>

<h4 id="Qlogic-driver-setting"><a href="#Qlogic-driver-setting" class="headerlink" title="Qlogic driver setting"></a>Qlogic driver setting</h4><p>Adpater reset time e.g. Qlogic reset time<br>echo options qla2xxx ql2xextended_error_logging=1 qlport_down_retry=10 ql2xloginretrycount=10 &gt;&gt;  /etc/modprobe.d/qlogic.conf<br>Multipath check_timeout  (reduce to 10 seconds from default 60 seconds)</p>
<p>Set the max_segments for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sgl_entries|<br>| Emulex    |  lpfc_sg_seg_cnt|<br>| ib_srp    |  cmd_sg_entries |<br>| Brocade   |  bfa_io_max_sge |</p>
<p>Set max_hw_sectors_kb for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sectors    |<br>| ib_srp    |  max_sect       |<br>| Brocade   |  max_xfer_size  |</p>
<p>mpt3sas<br>max_sectors:max sectors, range 64 to 32767  default=32767 (ushort)</p>
<h4 id="SAN-controller-setting-MD3460"><a href="#SAN-controller-setting-MD3460" class="headerlink" title="SAN controller setting MD3460"></a>SAN controller setting MD3460</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RAID 6               8+2</span><br><span class="line">segment              128K</span><br><span class="line">cache block size     32K</span><br><span class="line">cache flush          90%</span><br><span class="line">Write cache mirror   enabled</span><br><span class="line">Read cache           disabled</span><br></pre></td></tr></table></figure>
<p>`</p>
<h3 id="Linux-setting"><a href="#Linux-setting" class="headerlink" title="Linux setting"></a>Linux setting</h3><h4 id="block-driver"><a href="#block-driver" class="headerlink" title="block driver"></a>block driver</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/sys/block/sdz/device/queue_depth</span><br><span class="line"></span><br><span class="line">/sys/block/sda/queue/nomerges </span><br><span class="line"><span class="comment">#set nomerges to 0 for HDDs or to 1 for SSDs</span></span><br><span class="line"></span><br><span class="line">/sys/block/sdd/queue/add_random </span><br><span class="line"><span class="comment">#The default value is 1. Set add_random=0 for SSDs because random entropy pool does not optimize SSD performance</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check scsi state</span></span><br><span class="line">$ cat /sys/block/sdX/device/state</span><br><span class="line">running</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 30 &gt; /sys/block/sdX/device/timeout</span><br><span class="line"><span class="comment"># I don 't think set the value too long</span></span><br><span class="line"></span><br><span class="line">$ cat /etc/udev/rules.d/50-udev.rules</span><br><span class="line">ACTION==<span class="string">"add"</span>, SUBSYSTEM==<span class="string">"scsi"</span> , SYSFS&#123;<span class="built_in">type</span>&#125;==<span class="string">"0|7|14"</span>, RUN+=<span class="string">"/bin/sh -c 'echo 60 &gt; /sys$<span class="variable">$DEVPATH</span>/timeout'"</span></span><br><span class="line"><span class="comment"># Note that the default timeout for normal file system commands is 60 seconds when udev is being used. If udev is not in use, the default timeout is 30 seconds.</span></span><br></pre></td></tr></table></figure>

<h4 id="block-driver-2"><a href="#block-driver-2" class="headerlink" title="block driver 2"></a><a href="https://library.netapp.com/ecmdocs/ECMP12404601/html/GUID-436F7286-AD26-4A8D-A2D1-2BC8B5CFC023.html" target="_blank" rel="noopener">block driver 2</a></h4><ul>
<li><p>max_hw_sectors_kb (RO) - This parameter sets the maximum number of kilobytes that the hardware allows for request.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ubuntu 18.04 server, sas 10TB 512e HDD</span><br><span class="line">$ cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;max_hw_sectors_kb </span><br><span class="line">16383</span><br></pre></td></tr></table></figure>
</li>
<li><p>max_sectors_kb (RW) - This parameter sets the maximum number of kilobytes that the block layer allows for a file system request. The value of this parameter must be less than or equal to the maximum size allowed by the hardware. The kernel also places an upper bound on this value with the BLK_DEF_MAX_SECTORS macro. This value varies from distribution to distribution, for example, it is 1024 on RHEL 6.3, 2048 on SLES 11 SP2.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_sectors_kb </span><br><span class="line">1280</span><br></pre></td></tr></table></figure>
</li>
<li><p>max_segments (RO) - This parameter enables low level driver to set an upper limit on the number of hardware data segments in a request. In the HBA drivers, this is also known as sg_tablesize.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_segments</span><br><span class="line">128</span><br></pre></td></tr></table></figure>
</li>
<li><p>max_segment_size (RO) - This parameter enables low level driver to set an upper limit on the size of each data segment in an I/O request in bytes. If clustering is enabled on the low level driver it is set to 65536 or it is set to system PAGE_SIZE by default, which is typically 4K. The maximum I/O size is determined by the following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_segment_size </span><br><span class="line">65536</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>MAX_IO_SIZE_KB = MIN(max_sectors_kb, (max_segment_size * max_segments)/1024)<br> 1280 KB       = MIN(1280, (65536*128)/1024) = MIN(1280, 8192)</p>
<p>In this command, PAGE_SIZE is architecture independent. It is 4096 for x86_64.</p>
<h1 id="or-you-could-set-it-from-1280-to-8192"><a href="#or-you-could-set-it-from-1280-to-8192" class="headerlink" title="or you could set it from 1280 to 8192"></a>or you could set it from 1280 to 8192</h1><p>$ echo 8192 &gt; max_sectors_kb</p>
<h5 id="Hot-plugin-the-scsi-device"><a href="#Hot-plugin-the-scsi-device" class="headerlink" title="Hot plugin the scsi device"></a>Hot plugin the scsi device</h5><p>What is h c t l<br>          h == hostadapter id (first one being 0)<br>          c == SCSI channel on hostadapter (first one being 0)<br>          t == ID (target)<br>          l == LUN (first one being 0)<br>Generic SCSI devices can also be accessed via the bsg driver in Linux.  By default, the bsg driver’s device node names  are  of  the  form ‘/dev/bsg/H:C:T:L’.  So,  for example, the SCSI device shown by this utility on a line starting with the tuple ‘6:0:1:2’ could be accessed via the bsg driver with the ‘/dev/bsg/6:0:1:2’ device node name.</p>
<h5 id="Add-the-scsi-device"><a href="#Add-the-scsi-device" class="headerlink" title="Add the scsi device"></a>Add the scsi device</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;- - -&quot; &gt; &#x2F;sys&#x2F;class&#x2F;scsi_host&#x2F;host&lt;h&gt;&#x2F;scan</span><br><span class="line">echo &quot;c t l&quot; &gt;  &#x2F;sys&#x2F;class&#x2F;scsi_host&#x2F;host&lt;h&gt;&#x2F;scan</span><br></pre></td></tr></table></figure>

<h5 id="Refresh-the-scsi-device"><a href="#Refresh-the-scsi-device" class="headerlink" title="Refresh the scsi device"></a>Refresh the scsi device</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdau/device/rescan</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/rescan</span><br></pre></td></tr></table></figure>

<h5 id="Remove-the-scsi-device"><a href="#Remove-the-scsi-device" class="headerlink" title="Remove the scsi device"></a>Remove the scsi device</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/delete</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/&lt;dev&gt;/device/delete</span><br></pre></td></tr></table></figure>

<h4 id="IO-schdule"><a href="#IO-schdule" class="headerlink" title="IO schdule"></a>IO schdule</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">schdule = deadline</span><br><span class="line">nr_request = 1024</span><br><span class="line">max_sector_kb = 1024</span><br><span class="line">read_ahead_kb = 8192</span><br><span class="line">rq_affinity = 2</span><br><span class="line"><span class="comment"># For storage configurations that need to maximize distribution of completion processing setting this option to '2' forces the completion to run on the requesting cpu (bypassing the "group" aggregation logic).</span></span><br><span class="line"></span><br><span class="line">vm.dirty_ratio = 40</span><br><span class="line"><span class="comment"># dirty_ratio     "Dirty" memory is that waiting to be written to disk. dirty_ratio is the number of memory pages at which a process will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. A default of 20 is reasonable. Increase to 40 to improve throughput, decrease it to 5 to 10 to improve latency, even lower on systems with a lot of memory.</span></span><br><span class="line"></span><br><span class="line">vm.dirty_background_ratio = 20</span><br><span class="line"><span class="comment"># dirty_background_ratio  Similar, but this is the number of memory pages at which the kernel background flusher thread will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. Set this lower than dirty_ratio, dirty_ratio/2 makes sense and is what the kernel does by default. This page shows that dirty_ratio has the greater effect. Tune dirty_ratio for performance, then set dirty_background_ratio to half that value.</span></span><br><span class="line"></span><br><span class="line">vm.vfs_cache_pressure = 50</span><br><span class="line">This sets the <span class="string">"pressure"</span> or the importance the kernel places upon reclaiming memory used <span class="keyword">for</span> caching directory and inode objects. The default of 100 or relative <span class="string">"fair"</span> is appropriate <span class="keyword">for</span> compute servers. Set to lower than 100 <span class="keyword">for</span> file servers on <span class="built_in">which</span> the cache should be a priority. Set higher, maybe 500 to 1000, <span class="keyword">for</span> interactive systems.</span><br><span class="line"></span><br><span class="line">overcommit_memory       Allows <span class="keyword">for</span> poorly designed programs <span class="built_in">which</span> malloc() huge amounts of memory <span class="string">"just in case"</span> but never really use it. Set this to 0 (disabled) unless you really need it.</span><br></pre></td></tr></table></figure>

<h4 id="deadline-parameters"><a href="#deadline-parameters" class="headerlink" title="deadline parameters"></a><a href="https://cromwell-intl.com/open-source/performance-tuning/disks.html" target="_blank" rel="noopener">deadline parameters</a></h4><p>fifo_batch      Number of read or write operations to issue in one batch.  Lower values may further reduce latency. Higher values can increase throughput on rotating mechanical disks, but at the cost of worse latency. You selected the deadline scheduler to limit latency, so you probably don’t want to increase this, at least not by very much.</p>
<p>read_expire     Number of milliseconds within which a read request should be served. Reduce this from the default of 500 to 100 on a system with interactive users.</p>
<p>rite_expire     Number of milliseconds within which a write request should be served.<br>Leave at default of 5000, let write operations be done asynchronously in the background unless your specialized application uses many synchronous writes.</p>
<p>writes_starved  Number read batches that can be processed before handling a write batch. Increase this from default of 2 to give higher priority to read operations.</p>
<p>nr_requests     Maximum number of read and write requests that can be queued at one time before the next process requesting a read or write is put to sleep. Default value of 128 means 128 read requests and 128 write requests can be queued at once. Larger values may increase throughput for workloads writing many small files, smaller values increase throughput with larger I/O operations. You could decrease this if you are using latency-sensitive applications, but then you shouldn’t be using NOOP if latency is sensitive!</p>
<p>read_ahead_kb   Number of kilobytes the kernel will read ahead during a sequential read operation. 128 kbytes by default, if the disk is used with LVM the device mapper may benefit from a higher value. If your workload does a lot of large streaming reads, larger values may improve performance.</p>
<p>max_sectors_kb  Maximum allowed size of an I/O request in kilobytes, which must be within these bounds:<br>Min value = max(1, logical_block_size/1024)<br>Max value = max_hw_sectors_kb</p>
<p>rotational      Should be 0 (no) for solid-state disks, but some do not correctly report their status to the kernel. If incorrectly set to 1 for an SSD, set it to 0 to disable unneeded scheduler logic meant to reduce number of seeks.</p>
<h3 id="scsi-driver-error-handaling-EH"><a href="#scsi-driver-error-handaling-EH" class="headerlink" title="scsi_driver error handaling (EH)"></a>scsi_driver error handaling (EH)</h3><p>scsi driver error Handaling (EH) timeout – eh_timeout (from default 10 second to 5 seconds)<br>HBA reset time - eh_deadline (from disable/0 to 5 seconds, default was off)</p>
<p>The SCSI error handling (EH) mechanism attempts to perform error recovery on failed SCSI devices. The SCSI host object eh_deadline parameter enables you to configure the maximum amount of time for the recovery. After the configured time expires, SCSI EH stops and resets the entire host bus adapter (HBA).</p>
<h3 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a><a href="https://stackoverflow.com/questions/4458183/how-the-util-of-iostat-is-computed" target="_blank" rel="noopener">iostat</a></h3><p>%util = blkio.ticks / deltams * 100%</p>
<p>deltams is the time elapsed since last snapshot in ms. It uses CPU stats from /proc/stat presumably because it gives better results than to rely on system time, but I don’t know for sure. (Side note: for some reason the times are divided by HZ, while the documentation states it’s in USER_HZ, I don’t understand that.)<br>blkio.ticks is “# of milliseconds spent doing I/Os”, from /proc/diskstats docs:</p>
<p>Field  9 – # of I/Os currently in progress<br>  The only field that should go to zero. Incremented as requests are<br>  given to appropriate struct request_queue and decremented as they finish.<br>Field 10 – # of milliseconds spent doing I/Os<br>  This field increases so long as field 9 is nonzero.</p>
<p>struct ext_disk_stats *xds<br>xds-&gt;util</p>
<p>Hypothesis:<br>Simple understand about util% =  IO time/the time(the clock) , if IO time &gt;= the time, the utils = 100%(in 1s), else, that means some times there is no IO ops in this second.<br>Why no ops in this sec ? maybe something hang, maybe just no any IO loading.</p>
<p>In SAS arch, there are multiple lane/phy in it, if single lane/phy has full IO loading, the util% will reach 100%.<br>Yes the device could be parallel, there are a lot of lane/phy are free, but the single lane or phy has full. I think the util% was right</p>
<ol>
<li>The single path(resource) was full (nvme,sas ssd)</li>
<li>If the block device ‘s performance is infinity,  All of CPU/Mem/NIC speed behind it. the bottle-neck not in the block device. some of syscall cause the util%=100% too</li>
<li>Could the util% show the device are busy ? That right, util% is not enough, Could the iostat show the device are busy ? Yes, it can<br>You can watch the await and util%, you could know the device busy or free. it ‘s so easy.</li>
</ol>
<p>“iostat was not correct, it just show the wrong value”. That ‘s alarmist for iostat ,and it ‘s so funny</p>
<p>This guy analyzed the code, but it ‘s not enough, he was not understand the iostat output.<br>The <a href="https://bean-li.github.io/dive-into-iostat/" target="_blank" rel="noopener">example</a> is good.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ID      Time    Ops                                  in_flight  stamp   stamp_delta           io_ticks            time_in_queue</span><br><span class="line">0       100     new request in the queue                0       0       no need caculate          0                 0</span><br><span class="line">1       100.10  another request go to the queue         1       100     100.10-100 &#x3D; 0.1        0.1                 0.1</span><br><span class="line">2       101.20  finish the first request                2       100.10  101.20-100.10 &#x3D; 1.1     1.2(1.1+0.1)        0.1+1.1*2 (total 2x io requests) &#x3D; 2.3</span><br><span class="line">3       103.60  finish the second request               1       101.20  103.60-101.20 &#x3D; 2.4     3.6                 2.3+2.4*1&#x3D;4.7</span><br><span class="line">4       153.60  The third request go to the queue       0       103.60  no need caculate        3.6                 4.7</span><br><span class="line">5       153.90  Finish the third request                1       153.60  153.90 - 153.60 &#x3D; 0.3   3.9                 4.7+0.3 * 1&#x3D; 5</span><br></pre></td></tr></table></figure>
<p>In 53.9s, All io requests in 3.9s, the other times has no any IO in the queue.<br>io_ticks  –&gt; util %<br>time_in_queue –&gt; avgqu-sz</p>
]]></content>
      <categories>
        <category>Hardware</category>
      </categories>
      <tags>
        <tag>scsi</tag>
        <tag>block</tag>
        <tag>smart</tag>
        <tag>nvme</tag>
      </tags>
  </entry>
</search>
