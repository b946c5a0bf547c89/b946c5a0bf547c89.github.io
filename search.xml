<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2020/01/04/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>lfs command</title>
    <url>/2019/12/29/lfs_command/</url>
    <content><![CDATA[<h3 id="Public"><a href="#Public" class="headerlink" title="Public"></a>Public</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=260</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span></span><br><span class="line"><span class="comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lustre servers is also 40 seconds</span></span><br><span class="line"><span class="comment">#Adaptive Timeouts: In a Lustre file system servers keep track of the time it takes for RPCs to be completed</span></span><br><span class="line"><span class="comment">#The quiescent_time in this formula is to account for the time it takes all Lustre clients to reestablish connections with all Lustre targets following an HSN quiesce. We've experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lustre clients, the number of Lustre targets, the number of Lustre file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># readonly mount</span></span><br><span class="line">$ mount.lfs <span class="variable">$zpool</span> /ost0 -o rdonly_dev</span><br></pre></td></tr></table></figure>
<p><a href="http://wiki.lfs.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency" target="_blank" rel="noopener">Understanding Lustre Message Loss and Tuning for Resiliency</a></p>
<a id="more"></a>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span><br><span class="line">options ptlrpc ldlm_num_threads&#x3D;16</span><br><span class="line"></span><br><span class="line"># Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size&#x3D;clear</span><br><span class="line"></span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 &#x2F;mnt&#x2F;striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  obdfilter.*OST0000*.stats</span><br><span class="line"></span><br><span class="line"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span><br><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;lib64&#x2F;openmpi&#x2F;bin&#x2F;:$PATH</span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear &#x2F;tmp&#x2F;tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? &#96;too stupid&#96; bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ .&#x2F;configure --enable-client --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;kernels&#x2F;$(uname -r);make rpms&#x2F;deps</span><br><span class="line">## install in ubuntu 18.04</span><br><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;&#x2F;lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ .&#x2F;configure --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  &#x2F;lib&#x2F;modules&#x2F;4.15.0-64-generic&#x2F;kernel&#x2F;drivers&#x2F;staging&#x2F;lfs&#x2F;</span><br><span class="line">$ depmod -a</span><br><span class="line"></span><br><span class="line">### set lfs client for a lot metadata ops</span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size&#x3D;10000 ldlm.namespaces.*.lru_max_age&#x3D;600000</span><br><span class="line"></span><br><span class="line"># check object server status</span><br><span class="line">client $ lfs osts</span><br><span class="line">client $ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;lov&#x2F;$fsname-clilov-fffff882037467800&#x2F;target_obd</span><br><span class="line">mds    $ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;lov&#x2F;$fsname-MDT0000-mdtlov&#x2F;target_obd</span><br><span class="line">client $ lctl get_param osc.*-OST*.active</span><br><span class="line"></span><br><span class="line"># check object server status in my production env</span><br><span class="line">$ (lfs osts | awk -F &#39;[ :_]+&#39; &#39;$0~&#x2F;OST&#x2F;&#123;print $2&#125;&#39; | while read line; do grep &quot;FULL&quot; &#x2F;proc&#x2F;fs&#x2F;lustre&#x2F;osc&#x2F;$&#123;line&#125;-*&#x2F;state &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 || echo -e &quot;Got these bad OSTs:&quot; $&#123;RED&#125;$line$&#123;NC&#125; ; done) &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line"># show ost ip  addr</span><br><span class="line">$ lctl dl -t</span><br><span class="line"></span><br><span class="line"># list nids</span><br><span class="line">$ lctl lst_nids</span><br><span class="line">$ lctl which_nid $your_ipaddr@tcp</span><br><span class="line">$ lctl ping $your_ipaddr@tcp </span><br><span class="line"></span><br><span class="line"># mount namespace &quot;D&quot; and clear mds info from client</span><br><span class="line">client $ echo 0 &gt; cat &#x2F;sys&#x2F;fs&#x2F;lustre&#x2F;mdc&#x2F;$FNAME-*&#x2F;active</span><br><span class="line">client $ lctl set_param mdc.$FNAME-*.active&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="metadata-server"><a href="#metadata-server" class="headerlink" title="metadata server"></a>metadata server</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Get all client info from mds</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;nodemap&#x2F;default&#x2F;exports</span><br><span class="line"></span><br><span class="line"># No root squash</span><br><span class="line">mds $ lctl set_param $fsname.mdt.root_squash&#x3D;108:108</span><br><span class="line">mds $ lctl set_param $fsname.mdt.nosquash_nids&#x3D;&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line">mds $ cat &#x2F;proc&#x2F;fs&#x2F;lf&#x2F;mdt&#x2F;$FNAME-MDT0000&#x2F;nosquash_nids</span><br><span class="line">ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp</span><br><span class="line"></span><br><span class="line">client $ lctl set_param llite.$FNAME-*.nosquash_nids&#x3D;&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line"></span><br><span class="line"># MDS to totally avoid new object creation on that OST</span><br><span class="line">$ lctl set_param osp.$FNAME-OST00XX.max_create_count&#x3D;0</span><br><span class="line"></span><br><span class="line">#degrade will only prefer to skip the OST</span><br><span class="line">$ lctl set_param obdfilter.$FNAME-OST00XX.degraded&#x3D;1</span><br><span class="line"></span><br><span class="line">$ lctl set_param -P timeout&#x3D;300</span><br><span class="line">$ lctl set_param timeout&#x3D;300 </span><br><span class="line"># if you want it work ,you have set it twice...</span><br><span class="line"></span><br><span class="line"># Monitor status</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;mdc&#x2F;$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800&#x2F;state </span><br><span class="line">$ lctl get_param mdc.$fname-MDT*.state</span><br><span class="line">$ lctl get_param mdc.$fname-MDT0000-mdc-*.rpc_stats</span><br><span class="line"></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             1556726087.189561170 secs.nsecs</span><br><span class="line">open                      3412130101 samples [reqs]</span><br><span class="line">close                     2926922120 samples [reqs]</span><br><span class="line">mknod                     293730475 samples [reqs]</span><br><span class="line">link                      20713305 samples [reqs]</span><br><span class="line">unlink                    316042257 samples [reqs]</span><br><span class="line">mkdir                     3275032 samples [reqs]</span><br><span class="line">rmdir                     2731821 samples [reqs]</span><br><span class="line">rename                    7687699 samples [reqs]</span><br><span class="line">getattr                   2060900881 samples [reqs]</span><br><span class="line">setattr                   320658776 samples [reqs]</span><br><span class="line">getxattr                  1080139037 samples [reqs]</span><br><span class="line">setxattr                  222105 samples [reqs]</span><br><span class="line">statfs                    11587278 samples [reqs]</span><br><span class="line">sync                      20670980 samples [reqs]</span><br><span class="line">samedir_rename            7199107 samples [reqs]</span><br><span class="line">crossdir_rename           488592 samples [reqs]</span><br></pre></td></tr></table></figure>

<h3 id="object-server"><a href="#object-server" class="headerlink" title="object server"></a>object server</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size&#x3D;1048576</span><br><span class="line"></span><br><span class="line">options ost oss_num_threads&#x3D;0</span><br><span class="line"># you can limit the server performance by num_threads </span><br><span class="line"></span><br><span class="line">#limit ost io threads</span><br><span class="line">$ lctl set_param ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line">$ lctl set_param -P ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line"></span><br><span class="line"># rpc info</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;osc&#x2F;lfs-OST0000-osc-ffff88103a993000&#x2F;import</span><br><span class="line"></span><br><span class="line"># Get status</span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* timeout</span><br><span class="line">$ lctl get_param llite.*$FNAME*.stats</span><br><span class="line">$ lctl get_param obdfilter.*OST005e*.brw_stats</span><br><span class="line"></span><br><span class="line">#Read and print the last_rcvd file from a device</span><br><span class="line">#display client information</span><br><span class="line">$ lr_reader -c &#x2F;dev&#x2F;sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r &#x2F;dev&#x2F;sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line"># disable ost cache</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line"></span><br><span class="line">#make sure ost mount parameters</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;ldiskfs&#x2F;dm-xx&#x2F;options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid&#x3D;0</span><br><span class="line">resgid&#x3D;0</span><br><span class="line">errors&#x3D;remount-ro</span><br><span class="line">commit&#x3D;5</span><br><span class="line">min_batch_time&#x3D;0</span><br><span class="line">max_batch_time&#x3D;15000</span><br><span class="line">stripe&#x3D;0</span><br><span class="line">data&#x3D;ordered</span><br><span class="line">inode_readahead_blks&#x3D;32</span><br><span class="line">init_itable&#x3D;10</span><br><span class="line">max_dir_size_kb&#x3D;0</span><br></pre></td></tr></table></figure>

<h3 id="net"><a href="#net" class="headerlink" title="net"></a>net</h3><p>I think in some the bad network quality env, you must improve them</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Peer Credits</span></span><br><span class="line"><span class="comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers </span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue </span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available <span class="keyword">for</span> this peer</span><br><span class="line">min is the smallest number of peer credits seen </span><br><span class="line">Negative credit count indicates the number of messages awaiting a credit</span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Interface Credits</span></span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min </span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 </span><br><span class="line">max is total available (i.e. value of ko2iblnd credits) </span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line"><span class="comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span></span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="comment"># the number of concurrent sends (to all peers), defaults:64</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">peer_buffer_credits=256</span><br><span class="line">concurrent_sends=256 - send work-queue sizing</span><br><span class="line"><span class="comment"># not test</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">the number of concurrent sends to a single peer, <span class="comment">#default:8</span></span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line"></span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line"><span class="comment">## Acceptor's listen backlog, the number of the connections the server instance can buffer in the wait queue.</span></span><br><span class="line">options lnet accept_timeout=128</span><br><span class="line"><span class="comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span></span><br><span class="line">k</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="comment"># 1024 = 1024*4KB =4MB per RPC</span></span><br><span class="line"><span class="comment">#Max RPCS in flight between OSC and OST</span></span><br><span class="line">$ lctl set_param -P <span class="variable">$FNAME</span>.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"><span class="comment"># Max number of 4K pages per RPC</span></span><br><span class="line"><span class="comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span></span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"><span class="comment"># Maximum MBs of dirty data that can be written and queued on a client</span></span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ modinfo mdt | grep max_mod_rpcs_per_client</span><br><span class="line">parm:           max_mod_rpcs_per_client:maximum number of modify RPCs <span class="keyword">in</span> flight allowed per client (uint)</span><br><span class="line"></span><br><span class="line">mds $ <span class="built_in">echo</span> 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show --verbose</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane</span></span><br><span class="line">options lnet networks=<span class="string">"tcp1(eth1),tcp2(eth2),o2ib0(ib0)"</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">"tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]"</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span></span><br><span class="line"><span class="comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span></span><br><span class="line"><span class="comment"># fs.pipe-max-size=104857600</span></span><br><span class="line"><span class="comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it 's pipe buffer size in the ulimit, not pipe size</span></span><br><span class="line"><span class="comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here 2 tools for pipe</span></span><br><span class="line"><span class="comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span></span><br><span class="line"><span class="comment"># process1 | pv -pterbTCB 1G | process2</span></span><br><span class="line"></span><br><span class="line">$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat</span><br><span class="line">      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]</span><br><span class="line">  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            </span><br><span class="line">1538323+1 records <span class="keyword">in</span></span><br><span class="line">1538323+1 records out</span><br><span class="line">787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ mkfifo -m 777 /tmp/lfs.log</span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it 'll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:user@my.server.example.org">user@my.server.example.org</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">* MARK – Internal record keeping</span><br><span class="line">* CREAT – Regular file creation</span><br><span class="line">* MKDIR – Directory creation</span><br><span class="line">* HLINK – Hard link</span><br><span class="line">* SLINK – Soft link</span><br><span class="line">* OPEN – open file</span><br><span class="line">* CLOSE – close file</span><br><span class="line">* MKNOD – Other file creation</span><br><span class="line">* UNLNK – Regular file removal</span><br><span class="line">* RMDIR – Directory removal</span><br><span class="line">* RNMFM – Rename, original</span><br><span class="line">* RNMTO – Rename, final</span><br><span class="line">* IOCTL – ioctl on file or directory</span><br><span class="line">* TRUNC – Regular file truncated</span><br><span class="line">* SATTR – Attribute change</span><br><span class="line">* XATTR – Extended Attribute change</span><br><span class="line">* HSM – HSM action</span><br><span class="line">* UNKNW – Unkown operation</span><br></pre></td></tr></table></figure>

<h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm.max_requests&#x3D;8</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT0000.changelog_users mdd.$FNAME-MDT0000.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask&#x3D;MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure>

<h4 id="Disable"><a href="#Disable" class="headerlink" title="Disable"></a>Disable</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Notify a device that user cl1 no longer needs records (up toand including 3)</span><br><span class="line">$ lfs changelog_clear $FNAME-MDT0000 cl1 3</span><br><span class="line"></span><br><span class="line">#To stop changelogs, changelog_mask should be set to MARK only</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line">mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line"></span><br><span class="line">#or youcan set it -all</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;-all</span><br></pre></td></tr></table></figure>

<h3 id="FSCK"><a href="#FSCK" class="headerlink" title="FSCK"></a>FSCK</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 <span class="keyword">in</span> dir <span class="comment">#145170469</span></span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem <span class="built_in">read</span>-only</span><br></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure>

<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy &#x2F;dev&#x2F;sdx</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>
<p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p>
<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.mdt=ug</span><br><span class="line">mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.ost=ug</span><br><span class="line"></span><br><span class="line">client $ lfs setquota –u user1 –b 307200 –B 309200 –i 10000 –I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota –g group1 –b 5120000 –B 5150000 –i 100000 –I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota –u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 deactivate</span><br><span class="line">mds $ lctl --device 8 activate</span><br></pre></td></tr></table></figure>

<h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or </span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont 't use lfs migrate, it 's too dangerous, it will cause data loss</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Job status</span></span><br><span class="line">```bash</span><br><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn't any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats for job "dd.0" on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[client]# lfs fid2path &#x2F;mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid &#x2F;mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>

<h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line"></span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lustre to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>lfs</tag>
      </tags>
  </entry>
  <entry>
    <title>smartctl</title>
    <url>/2019/11/26/smartctl/</url>
    <content><![CDATA[<h3 id="smart-test"><a href="#smart-test" class="headerlink" title="smart test"></a>smart test</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ smartctl -t long /dev/sdx</span><br><span class="line">$ smartctl -t short /dev/sdx</span><br><span class="line"></span><br><span class="line"><span class="comment">#self test result</span></span><br><span class="line">$ smartctl --capabilities /dev/sdxx</span><br><span class="line">$ smartctl --<span class="built_in">log</span>=selftest /dev/sdxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># During selected tests the specified range of LBAs is checked. The LBAs to be scanned are specified in the following formats:</span></span><br><span class="line">$ smartctl -t select,0-10 -t select,5-15 -t select,10-20 /dev/sdxx</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="disable-self-test-and-smart"><a href="#disable-self-test-and-smart" class="headerlink" title="disable self test and smart"></a>disable self test and smart</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#disable smart</span></span><br><span class="line">$ smartctl --smart=off /dev/sdxx</span><br><span class="line"></span><br><span class="line"><span class="comment">#disbale automatic test</span></span><br><span class="line">$ smartctl --offlineauto=off /dev/sdxx</span><br></pre></td></tr></table></figure>

<h3 id="check-status"><a href="#check-status" class="headerlink" title="check status"></a>check status</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">smartctl -x /dev/sdxx</span><br><span class="line">smartctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-693.5.2.el7_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">=== START OF INFORMATION SECTION ===</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">Error counter <span class="built_in">log</span>:</span><br><span class="line">           Errors Corrected by           Total   Correction     Gigabytes    Total</span><br><span class="line">               ECC          rereads/    errors   algorithm      processed    uncorrected</span><br><span class="line">           fast | delayed   rewrites  corrected  invocations   [10^9 bytes]  errors</span><br><span class="line"><span class="built_in">read</span>:          0        0         0         0     187056      15672.508           0</span><br><span class="line">write:         0        0         0         0      94787       5143.822           0</span><br><span class="line">verify:        0        0         0         0        478        101.563           0</span><br><span class="line"></span><br><span class="line">Non-medium error count:        0</span><br><span class="line"></span><br><span class="line">SMART Self-test <span class="built_in">log</span></span><br><span class="line">Num  Test              Status                 segment  LifeTime  LBA_first_err [SK ASC ASQ]</span><br><span class="line">     Description                              number   (hours)</span><br><span class="line"><span class="comment"># 1  Reserved(7)       Completed                  64       5                 - [-   -    -]</span></span><br><span class="line"></span><br><span class="line">Long (extended) Self Test duration: 65305 seconds [1088.4 minutes]</span><br><span class="line"></span><br><span class="line">Background scan results <span class="built_in">log</span></span><br><span class="line">  Status: scan is active</span><br><span class="line">    Accumulated power on time, hours:minutes 344:48 [20688 minutes]</span><br><span class="line">    Number of background scans performed: 2,  scan progress: 18.40%</span><br><span class="line">    Number of background medium scans performed: 2</span><br><span class="line"></span><br><span class="line">Protocol Specific port <span class="built_in">log</span> page <span class="keyword">for</span> SAS SSP</span><br><span class="line">relative target port id = 1</span><br><span class="line">  generation code = 2</span><br><span class="line">  number of phys = 1</span><br><span class="line">  phy identifier = 0</span><br><span class="line">    attached device <span class="built_in">type</span>: expander device</span><br><span class="line">    attached reason: power on</span><br><span class="line">    reason: unknown</span><br><span class="line">    negotiated logical link rate: phy enabled; 6 Gbps</span><br><span class="line">    attached initiator port: ssp=0 stp=0 smp=1</span><br><span class="line">    attached target port: ssp=0 stp=0 smp=1</span><br><span class="line">    SAS address = 0x5000cca2735b5ba1</span><br><span class="line">    attached SAS address = 0x50050cc11a5481ff</span><br><span class="line">    attached phy identifier = 32</span><br><span class="line">    Invalid DWORD count = 0</span><br><span class="line">    Running disparity error count = 0</span><br><span class="line">    Loss of DWORD synchronization = 0</span><br><span class="line">    Phy reset problem = 0</span><br><span class="line">    Phy event descriptors:</span><br><span class="line">     Invalid word count: 0</span><br><span class="line">     Running disparity error count: 0</span><br><span class="line">     Loss of dword synchronization count: 0</span><br><span class="line">     Phy reset problem count: 0</span><br><span class="line">relative target port id = 2</span><br><span class="line">  generation code = 2</span><br><span class="line">  number of phys = 1</span><br><span class="line">  phy identifier = 1</span><br><span class="line">    attached device <span class="built_in">type</span>: expander device</span><br><span class="line">    attached reason: power on</span><br><span class="line">    reason: unknown</span><br><span class="line">    negotiated logical link rate: phy enabled; 6 Gbps</span><br><span class="line">    attached initiator port: ssp=0 stp=0 smp=1</span><br><span class="line">    attached target port: ssp=0 stp=0 smp=1</span><br><span class="line">    SAS address = 0x5000cca2735b5ba2</span><br><span class="line">    attached SAS address = 0x50050cc11a919cff</span><br><span class="line">    attached phy identifier = 10</span><br><span class="line">    Invalid DWORD count = 0</span><br><span class="line">    Running disparity error count = 0</span><br><span class="line">    Loss of DWORD synchronization = 0</span><br><span class="line">    Phy reset problem = 0</span><br><span class="line">    Phy event descriptors:</span><br><span class="line">     Invalid word count: 0</span><br><span class="line">     Running disparity error count: 0</span><br><span class="line">     Loss of dword synchronization count: 0</span><br><span class="line">     Phy reset problem count: 0</span><br></pre></td></tr></table></figure>


<h3 id="SSD-write-monitor-for-more-info-go-to-vendor-spec-file"><a href="#SSD-write-monitor-for-more-info-go-to-vendor-spec-file" class="headerlink" title="SSD write monitor, for more info go to vendor spec file"></a>SSD write monitor, for more info go to vendor spec file</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">233 Media_Wearout_Indicator 0x0032   097   097   000    Old_age   Always       -       0</span><br><span class="line">251 NAND_Writes             -O--CK   100   100   000    -    7546253904</span><br></pre></td></tr></table></figure>
<p>diff vendor has the diff defines</p>
]]></content>
      <categories>
        <category>scsi</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>sata</tag>
        <tag>sas</tag>
      </tags>
  </entry>
  <entry>
    <title>xfs info</title>
    <url>/2019/11/26/xfs/</url>
    <content><![CDATA[<p><a href="http://fibrevillage.com/storage" target="_blank" rel="noopener">reference</a></p>
<h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkfs.xfs /dev/device</span><br><span class="line">$ mkfs. xfs -d su= 64k,sw= 4 /dev/device</span><br></pre></td></tr></table></figure>
<p>su = value<br>    Specifies a stripe unit or RAID chunk size. The value must be specified in bytes, with an optional k, m, or g suffix.<br>sw= value<br>    Specifies the number of data disks in a RAID device, or the number of stripe units in the stripe.</p>
<a id="more"></a>

<h4 id="Grow"><a href="#Grow" class="headerlink" title="Grow"></a>Grow</h4><p>While XFS file systems can be grown while mounted</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_growfs /mount/point -D size</span><br></pre></td></tr></table></figure>

<h4 id="Suspend-file-system"><a href="#Suspend-file-system" class="headerlink" title="Suspend file system"></a>Suspend file system</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_freeze -f /mount/point</span><br><span class="line"></span><br><span class="line"><span class="comment">#unfreeze</span></span><br><span class="line">$ xfs_freeze -u /mount/point</span><br></pre></td></tr></table></figure>
<p>When taking an LVM snapshot, it is not necessary to use xfs_freeze to suspend the file system first.</p>
<h4 id="Quota"><a href="#Quota" class="headerlink" title="Quota"></a>Quota</h4><p>uquota User quotas<br>gquota Group quotas<br>pquota Project quota</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount -o uquota /dev/mapper/LUN13 /lun13</span><br><span class="line">$ xfs_quota  /lun13</span><br><span class="line">$ xfs_quota -x -c <span class="string">'limit isoft=500 ihard=700 trteam'</span> /lun13</span><br><span class="line">$ xfs_quota -x -c <span class="string">' limit -g bsoft=1000 m bhard=1200m test '</span> /lun13</span><br><span class="line"></span><br><span class="line"><span class="comment"># report</span></span><br><span class="line">$ xfs_quota -x -c <span class="string">'report -bih'</span> /lun13</span><br><span class="line"></span><br><span class="line"><span class="comment">#project quota</span></span><br><span class="line">$ xfs_quota -x -c <span class="string">' project -s projectname'</span> project_path</span><br><span class="line">$ xfs_quota -x -c <span class="string">' limit -p bsoft=1000m bhard=1200m projectname'</span></span><br></pre></td></tr></table></figure>

<h4 id="Backup-and-restore"><a href="#Backup-and-restore" class="headerlink" title="Backup and restore"></a>Backup and restore</h4><p>To perform a full backup, perform a level 0 dump on the file system.<br>A level 1 dump is the first incremental backup after a full backup. The next incremental backup would be level 2, which only backs up files that have changed since the last level 1 dump; and so on, to a maximum of level 9.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfsdump -l level [-L label] -f backup-destination path-to-xfs-filesystem</span><br><span class="line">$ xfsdump -l 0 -f /backup-files/data.xfsdump /data</span><br><span class="line">$ xfsdump -l 0 -L <span class="string">"backup_data"</span> -f /dev/st0 /data</span><br><span class="line"></span><br><span class="line"><span class="comment"># restore</span></span><br><span class="line">$ xfsrestore [-r] [-S session-id] [-L session-label] [-i] -f backup-location restoration-path</span><br><span class="line">$ xfsrestore -f /backup-files/data.xfsdump /mnt/data/</span><br><span class="line">$ xfsrestore -f /dev/st0 -L <span class="string">"backup_data"</span> /mnt/data/</span><br><span class="line"></span><br><span class="line">$ xfsrestore -I <span class="comment"># get the session ID</span></span><br><span class="line">$ xfsrestore -f /dev/st0 -S <span class="string">"45e9af35-efd2-4244-87bc-4762e476cbab"</span> /mnt/data/</span><br><span class="line"></span><br><span class="line">$ xfs_metadump /dev/mapper/vg0-data /home/vg0-data.metadump</span><br><span class="line">$ xfs_mdrestore /home/vg0-data.metadump /home/vg0-data.img</span><br></pre></td></tr></table></figure>

<h4 id="Compare-with-ext4"><a href="#Compare-with-ext4" class="headerlink" title="Compare with ext4"></a>Compare with ext4</h4><table>
<thead>
<tr>
<th>Task</th>
<th>Ext3/4</th>
<th>XFS</th>
</tr>
</thead>
<tbody><tr>
<td>File System check</td>
<td>e2fsck</td>
<td>xfs_repair</td>
</tr>
<tr>
<td>REsizing a File System</td>
<td>resize2fs</td>
<td>xfs_growfs</td>
</tr>
<tr>
<td>Save an image of a file system</td>
<td>e2image</td>
<td>xfs_metadump and xfs_mdrestore</td>
</tr>
<tr>
<td>Label or tune a File System</td>
<td>tune2fs</td>
<td>xfs_admin</td>
</tr>
<tr>
<td>Backup a File System</td>
<td>dump and restore</td>
<td>xfsdump xfsrestore</td>
</tr>
</tbody></table>
<h4 id="Defragment"><a href="#Defragment" class="headerlink" title="Defragment"></a>Defragment</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_db -r  /dev/sdb</span><br><span class="line">$ xfs_db&gt; frag</span><br><span class="line"></span><br><span class="line"><span class="comment"># Defragment</span></span><br><span class="line">$ xfs_fsr -t 3600  -v /lun10</span><br></pre></td></tr></table></figure>
<h4 id="xfs-admin"><a href="#xfs-admin" class="headerlink" title="xfs_admin"></a>xfs_admin</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_admin -l /dev/sdb</span><br><span class="line">$ xfs_admin -L <span class="string">"data"</span> /dev/sdb</span><br><span class="line"></span><br><span class="line">$ xfs_admin -u /dev/sdb</span><br><span class="line">$ xfs_admin -U generate-id /dev/sdb</span><br><span class="line">$ xfs_admin -U nil /dev/sdb</span><br><span class="line"></span><br><span class="line">$ xfs_admin -c 0 /dev/sdb</span><br><span class="line"></span><br><span class="line"><span class="comment">#Enable xfs lazy counters</span></span><br><span class="line">$ xfs_admin -c 1 /dev/sdb</span><br></pre></td></tr></table></figure>

<h4 id="Repair"><a href="#Repair" class="headerlink" title="Repair"></a>Repair</h4><p>-n     No modify mode. Specifies that xfs_repair should not modify the filesystem  but  should  only  scan the filesystem and indicate what repairs would have been made</p>
<p>-L The xfs_repair utility cannot repair an XFS file system with a dirty log. To clear the log, mount and unmount the XFS file system. force log zeroing to clean log<br>-v option gives you xfs_repair verbose outpt, inaddition to the regular repair message +extra verbosity, it has a summary for each repair phase.<br>-d Repair  dangerously.  Allow  xfs_repair to repair an XFS filesystem mounted read only. This is typically done on a root filesystem from single user mode, immediately followed by a reboot.<br>-l option, external log<br>-f Specifies  that  the  filesystem  image  to  be processed is stored in a regular file at device (see the mkfs.xfs -d file option). This might happen if an image copy of a filesystem has been copied or written into an ordinary file.  This option implies that any external log or realtime section is also in an ordinary file.<br>-P option, disable prefetching<br>-t interval Modify reporting interval, specified in seconds. During long runs xfs_repair outputs its progress every 15 minutes. Reporting is only activated when ag_stride is enabled.<br>-m Specifies the approximate maximum amount of memory, in megabytes, to use for xfs_repair.  xfs_repair has its own internal block cache which  will  scale  out  up  to  the lesser of the process’s virtual address limit or about 75% of the system’s physical RAM.  This option overrides these limits. NOTE: These memory limits are only approximate and may use more than the specified limit</p>
<p>If xfs_repair failed in phase 1, you must restore lost files from backups.<br>If xfs_repair failed in phase 2 or later, you may be able to restore files from the disk by backing up and restoring the files on the file system<br>    * Mount the file system using mount -r (read-only).<br>    * Make a file system backup with xfsdump.<br>    * Use mkfs to a make new file system on the same disk partition or XLV logical volume.<br>    * Restore the files from the backup with xfsrestore.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount -r /dev/sdb /mnt</span><br><span class="line">$ mount /dev/sdb /mnt -o norecover</span><br></pre></td></tr></table></figure>


<p>Without any option, xfs_repair will check and repair errors that it encounters.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ xfs_repair -n &#x2F;dev&#x2F;device</span><br><span class="line">Phase 1 - find and verify superblock...</span><br><span class="line">Phase 2 - using internal log</span><br><span class="line">        - scan filesystem freespace and inode maps...</span><br><span class="line">        - found root inode chunk</span><br><span class="line">Phase 3 - for each AG...</span><br><span class="line">        - scan (but don&#39;t clear) agi unlinked lists...</span><br><span class="line">        - process known inodes and perform inode discovery...</span><br><span class="line">        - agno &#x3D; 0</span><br><span class="line">        - agno &#x3D; 1</span><br><span class="line">        - agno &#x3D; 2</span><br><span class="line">        - agno &#x3D; 3</span><br><span class="line">        - process newly discovered inodes...</span><br><span class="line">Phase 4 - check for duplicate blocks...</span><br><span class="line">        - setting up duplicate extent list...</span><br><span class="line">        - check for inodes claiming duplicate blocks...</span><br><span class="line">        - agno &#x3D; 0</span><br><span class="line">        - agno &#x3D; 1</span><br><span class="line">        - agno &#x3D; 2</span><br><span class="line">        - agno &#x3D; 3</span><br><span class="line">No modify flag set, skipping phase 5</span><br><span class="line">Phase 6 - check inode connectivity...</span><br><span class="line">        - traversing filesystem ...</span><br><span class="line">        - traversal finished ...</span><br><span class="line">        - moving disconnected inodes to lost+found ...</span><br><span class="line">Phase 7 - verify link counts...</span><br><span class="line">No modify flag set, skipping filesystem flush and exiting.</span><br><span class="line"></span><br><span class="line">$ xfs_repair -L &#x2F;dev&#x2F;device</span><br><span class="line">$ xfs_repair -d &#x2F;</span><br></pre></td></tr></table></figure>
<ol>
<li>Inode  and inode blockmap (addressing) checks:bad magic number  in inode, bad magic numbers in inode blockmap  blocks,  extents out  of  order,  incorrect  number of records in inode blockmap blocks, blocks claimed that are not in a legal data area of the filesystem, blocks that are claimed by more than one inode.</li>
<li>Inode  allocation  map  checks:bad  magic number in inode map blocks, inode state as indicated by map (free or in-use) inconsistent with state indicated by the inode, inodes referenced by the filesystem that do not appear in the inode allocation  map, inode  allocation  map referencing blocks that do not appear to contain inodes.</li>
<li>Size checks:number of blocks  claimed  by  inode  inconsistent with  inode  size, directory size not block aligned, inode size not consistent with inode format.</li>
<li>Directory checks:bad magic numbers in directory blocks, incorrect  number  of  entries  in  a directory block, bad freespace information in a directory leaf block,  entry  pointing  to  an unallocated  (free) or out of range inode, overlapping entries, missing or incorrect dot and dotdot  entries,  entries  out  of hashvalue  order, incorrect internal directory pointers, directory type not consistent with inode format and size.</li>
<li>Pathname checks:files or directories not referenced by a pathname starting from the filesystem root, illegal pathname components.</li>
<li>Link count checks:link counts that do not agree with the  number of directory references to the inode.</li>
<li>Freemap  checks: blocks  claimed  free by the freemap but also claimed by an inode, blocks unclaimed  by  any  inode  but  not appearing in the freemap.</li>
<li>Super  Block  checks: total free block and/or free i-node count incorrect, filesystem geometry inconsistent, secondary and primary superblocks contradictory.</li>
</ol>
<h4 id="Recover-file"><a href="#Recover-file" class="headerlink" title="Recover file"></a>Recover file</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ls -ih</span><br><span class="line">70179 -r-xr-xr-x 1 solexa solexa    0 Jan 10  2013 report.htm</span><br><span class="line"></span><br><span class="line">$ xfs_db -r /dev/sdb -c <span class="string">"inode 70179"</span> -c <span class="string">"bmap"</span></span><br><span class="line">data offset 0 startblock 2459337662 (9/43418558) count 1 flag 0a</span><br><span class="line"></span><br><span class="line">$ agblocks=$(xfs_db -r /dev/sdb -c sb -c p | grep ^agblocks | sed <span class="string">'s/.* = //'</span>)</span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$agblocks</span></span><br><span class="line">268435455</span><br><span class="line">$ agcount=9</span><br><span class="line">$ b=43418558</span><br><span class="line">$ c=1</span><br><span class="line">$ <span class="built_in">echo</span> $((<span class="variable">$agblocks</span>*<span class="variable">$agcount</span>+<span class="variable">$b</span>)) </span><br><span class="line">2459337653</span><br><span class="line">$ <span class="built_in">echo</span> $((2459337662-9))</span><br><span class="line">2459337653</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/sdb bs=4096 skip=$((<span class="variable">$agblocks</span>*<span class="variable">$agcount</span>+<span class="variable">$b</span>)) count=<span class="variable">$c</span> of=/tmp/report.htm</span><br><span class="line">1+0 records <span class="keyword">in</span></span><br><span class="line">1+0 records out</span><br><span class="line">4096 bytes (4.1 kB) copied, 4.6345e-05 s, 88.4 MB/s</span><br><span class="line"></span><br><span class="line">$ cat /tmp/report.htm   <span class="comment"># file come back</span></span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;meta http-equiv=<span class="string">"Content-Type"</span> content=<span class="string">"text/html; charset=gbk"</span>/&gt;</span><br><span class="line">&lt;style&gt;</span><br></pre></td></tr></table></figure>

<h3 id="xfs-estimate"><a href="#xfs-estimate" class="headerlink" title="xfs_estimate"></a>xfs_estimate</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xfs_estimate -v &#x2F;mnt</span><br><span class="line">directory                               bsize   blocks    megabytes    logsize</span><br><span class="line">&#x2F;mnt                                     4096     1138        4.4MB    4096000</span><br><span class="line">$ xfs_estimate -v -b 512 &#x2F;mnt</span><br><span class="line">directory                               bsize   blocks    megabytes    logsize</span><br><span class="line">&#x2F;mnt                                      512     1138        0.6MB     512000</span><br><span class="line">$ xfs_estimate -v -b 4096 &#x2F;mnt</span><br><span class="line">directory                               bsize   blocks    megabytes    logsize</span><br><span class="line">&#x2F;mnt                                     4096     1138        4.4MB    4096000</span><br></pre></td></tr></table></figure>

<h3 id="xfs-info"><a href="#xfs-info" class="headerlink" title="xfs_info"></a>xfs_info</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_info /mnt</span><br><span class="line">meta-data=/dev/sdb               isize=512    agcount=16, agsize=4799984 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0 spinodes=0</span><br><span class="line">data     =                       bsize=4096   blocks=76799744, imaxpct=25</span><br><span class="line">         =                       sunit=16     swidth=32 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line"><span class="built_in">log</span>      =internal               bsize=4096   blocks=37504, version=2</span><br><span class="line">         =                       sectsz=512   sunit=16 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br></pre></td></tr></table></figure>

<h3 id="xfs-logprint"><a href="#xfs-logprint" class="headerlink" title="xfs_logprint"></a>xfs_logprint</h3><p>-t print out transactional view of an xfs log</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xfs_logprint -t  /dev/sdb</span><br><span class="line">xfs_logprint:</span><br><span class="line">xfs_logprint: /dev/sdb contains a mounted and writable filesystem</span><br><span class="line">    data device: 0x810</span><br><span class="line">    <span class="built_in">log</span> device: 0x810 daddr: 307199104 length: 300032</span><br><span class="line"></span><br><span class="line">    <span class="built_in">log</span> tail: 256 head: 512 state: &lt;DIRTY&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">LOG REC AT LSN cycle 1 block 256 (0x1, 0x100)</span><br><span class="line">============================================================================</span><br><span class="line">TRANS: tid:0xb4cdd37c  <span class="built_in">type</span>:SWAPEXT  <span class="comment">#items:2  trans:0xb4cdd37c  q:0x20bcbe0</span></span><br><span class="line">BUF: cnt:2 total:2 a:0x20bd010 len:24 a:0x20bd060 len:384</span><br><span class="line">	BUF:  <span class="comment">#regs:2   start blkno:0x0   len:1   bmap size:1   flags:0x9000</span></span><br><span class="line">	SUPER Block Buffer:</span><br><span class="line"></span><br><span class="line">LOG REC AT LSN cycle 1 block 384 (0x1, 0x180)</span><br><span class="line">============================================================================</span><br><span class="line">TRANS: tid:0xb84fc956  <span class="built_in">type</span>:SWAPEXT  <span class="comment">#items:2  trans:0xb84fc956  q:0x20bcbe0</span></span><br><span class="line">BUF: cnt:2 total:2 a:0x20bd010 len:24 a:0x20cd250 len:384</span><br><span class="line">	BUF:  <span class="comment">#regs:2   start blkno:0x0   len:1   bmap size:1   flags:0x9000</span></span><br><span class="line">	SUPER Block Buffer:</span><br></pre></td></tr></table></figure>
<p>-i extract and print inode info<br>-q extract and print quota info<br>-e, exit when an error is found in the log<br>-c, Attempt to continue when an error is detected</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ xfs_logprint -C xfs.log /dev/sdb</span><br></pre></td></tr></table></figure>

<h4 id="About-barriers"><a href="#About-barriers" class="headerlink" title="About barriers"></a><a href="https://xfs.org/index.php/XFS_FAQ#Write_barrier_support." target="_blank" rel="noopener">About barriers</a></h4><p>Write barrier support is enabled by default in XFS since kernel version 2.6.17. It is disabled by mounting the filesystem with “nobarrier”. Barrier support will flush the write back cache at the appropriate times (such as on XFS log writes). </p>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>xfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Replace megaraid by strocli</title>
    <url>/2019/05/24/megaraid/</url>
    <content><![CDATA[<h3 id="Set-time"><a href="#Set-time" class="headerlink" title="Set time"></a>Set time</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call show time</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> time=yyyymmdd hh:mm:ss|systemtime</span><br><span class="line">$ timedatectl <span class="built_in">set</span>-timezone Asia/XXXXXX</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> time=systemtime</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="Set-cache-policy"><a href="#Set-cache-policy" class="headerlink" title="Set cache policy"></a>Set cache policy</h3><p>Disable write back cache</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ storcli64 &#x2F;c0&#x2F;v0 set wrcache&#x3D;wb&#x2F;wt&#x2F;awb rdcache&#x3D;ra iopolicy&#x3D;cached pdcache&#x3D;on</span><br></pre></td></tr></table></figure>

<h3 id="Create-raid10"><a href="#Create-raid10" class="headerlink" title="Create raid10"></a>Create raid10</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 add vd <span class="built_in">type</span>=raid10 drives=252:0-7 pdperarray=2 WT strip=64</span><br><span class="line">$ storcli64 /c0 add vd <span class="built_in">type</span>=raid5 drives=252:0,2,4,6 WT strip=64</span><br><span class="line">$ storcli64 /c0/vXX del force</span><br></pre></td></tr></table></figure>

<p><a href="https://support.huawei.com/enterprise/en/doc/EDOC1000111853?section=j005" target="_blank" rel="noopener">reference</a><br><img src="/img/lsi-3108-example-20191010.png" alt=""><br>Read Policy</p>
<ul>
<li>No Read Ahead: disables the Read Ahead function.</li>
<li>Read Ahead: enables the Read Ahead function. The controller pre-reads sequential data or the data predicted to be used and saves it in the cache.</li>
</ul>
<p>Write Policy</p>
<ul>
<li>Write Back: After the controller cache receives all data, the controller sends the host a message indicating that data transmission is complete.</li>
<li>Write Through: After the drive subsystem receives all data, the controller sends the host a message indicating that data transmission is complete.</li>
<li>Always Write Back: Virtual drive remains in Write Back mode if the capacitor fails or no capacitor exists.</li>
</ul>
<p>I/O Policy</p>
<ul>
<li>Direct: Data is read directly and is not cached. This is the recommended mode for common configuration.</li>
<li>Cached: All data is read from cache. This is the recommended mode for CacheCade.</li>
</ul>
<p>Access Policy</p>
<ul>
<li>Read/Write: Read and write operations are allowed.</li>
<li>Read Only: The virtual drive is read-only.</li>
<li>Blocked: The virtual drive is locked from access.</li>
</ul>
<p>Drive Cache</p>
<ul>
<li>Unchanged: uses the current cache policy.</li>
<li>Enable: writes data to the cache before writing data to the hard drive. This option improves data write performance. However, data may be lost if there is no protection mechanism against power failures.</li>
<li>Disable: writes data to a hard drive without caching the data. Data is not lost if power failures occur.</li>
</ul>
<p>Emulation Type<br>Set the sector size reported to the OS.<br>50</p>
<ul>
<li><p>If the member drive is 512B/512B:</p>
<ul>
<li>Default: The logical drive sector is 512B/512B.</li>
<li>None: The logical drive sector is 512B/512B.</li>
<li>Force: The logical drive sector is 512B/4KB.</li>
</ul>
</li>
<li><p>If the member drive is 512B/4KB:</p>
<ul>
<li>Default: The logical drive sector is 512B/4KB.</li>
<li>None: The logical drive sector is 512B/512B.</li>
<li>Force: The logical drive sector is 512B/4KB.</li>
</ul>
</li>
</ul>
<h3 id="Megaraid-FastPATH-for-SAS-SATA-SSD"><a href="#Megaraid-FastPATH-for-SAS-SATA-SSD" class="headerlink" title="Megaraid FastPATH for SAS/SATA SSD"></a>Megaraid FastPATH for SAS/SATA SSD</h3><h4 id="case-1"><a href="#case-1" class="headerlink" title="case 1"></a><a href="https://lenovopress.com/lp0592.pdf" target="_blank" rel="noopener">case 1</a></h4><p>Suggest RAID5<br>To benefit from FastPath, an array must be defined using these specific parameters:</p>
<ul>
<li>Write Through</li>
<li>Direct IO</li>
<li>No Read Ahead</li>
<li>64KB stripe size (for most workloads)</li>
<li>RAID 5(fastpath support all raid level)</li>
<li>Disk Cache Policy: This controls the write cache policy for the drives in the arrays (as opposed to the write cache policy of the RAID adapter). Enabling disk write caching can put data at risk.<ul>
<li>full initialization</li>
</ul>
</li>
</ul>
<p>With FastPath enabled, and when certain conditions are met, the MegaRAID software stack running on the RAID controller can bypass portions of the software stack, resulting in a highly efficient, streamlined code path that the RAID controller can execute very quickly. This means that the RAID controller can handle more trips though its code in a given amount of time, which equates to higher possible IOPS.</p>
<p>FastPath is not designed to speed up slow storage. It will not have an effect on storage performance unless the RAID controller is the bottleneck. If the controller itself is limiting storage performance, then enabling FastPath will allow the controller to run faster, raising performance. FastPath is not meant for use with HDD storage nor is it meant for sequential workloads.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show aso</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Premium Feature Key :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Adv S/W Opt                 Time Remaining  Mode</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">MegaRAID FastPath           Unlimited       Secured</span><br><span class="line">MegaRAID CacheCade Pro 2.0  Unlimited       Secured</span><br><span class="line">MegaRAID RAID6              Unlimited       Factory Installed</span><br><span class="line">MegaRAID RAID5              Unlimited       Factory Installed</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Re-host Information :</span><br><span class="line">===================</span><br><span class="line">Needs Re-hosting = NO</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">#### [Case 2: Aerospike Certification Tool](https://www.aerospike.com/docs/operations/plan/ssd/lsi_megacli.html)</span></span><br><span class="line">RAID Level	0</span><br><span class="line">Write Policy	Write Through</span><br><span class="line">Read Policy	No Read Ahead</span><br><span class="line">IO Policy	Direct IO</span><br><span class="line">Other	        No write to cache <span class="keyword">if</span> bad BBU</span><br><span class="line"></span><br><span class="line">Dell R720xd</span><br><span class="line">PERC H710p RAID controller</span><br><span class="line">8 x 200 GB Intel s3700 SSDs</span><br><span class="line"></span><br><span class="line">Latency measures during torture tests (96,000 reads/sec and 48,000 concurrent writes/second).</span><br><span class="line">RAID setting	% &gt;1 ms	% &gt;2 ms	% &gt;4 ms	% &gt;8 ms	% &gt;16 ms</span><br><span class="line">Default	        22.20	13.96	4.29	0.22	0.00</span><br><span class="line">FastPath™	4.31	0.85	0.17	0.00	0.00</span><br><span class="line"></span><br><span class="line">The latencies <span class="keyword">for</span> the FastPath™ are much better. In general better latency results mean much higher threshold <span class="keyword">for</span> throughput.</span><br><span class="line"></span><br><span class="line"><span class="comment">### Storcli replace Megacli</span></span><br><span class="line"><span class="comment">#### BBU info</span></span><br><span class="line">```bash</span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -AdpBbuCmd -getBbuProperties -aALL</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU Properties <span class="keyword">for</span> Adapter: 0</span><br><span class="line"></span><br><span class="line">  Auto Learn Period: 30 Days</span><br><span class="line">  Next Learn time: Mon Jun  3 12:02:38 2019</span><br><span class="line"></span><br><span class="line">  Learn Delay Interval:0 Hours</span><br><span class="line">  Auto-Learn Mode: Enabled</span><br><span class="line">  BBU Mode = 4</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show properties</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Properties :</span><br><span class="line">==============</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Property             Value</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line">Auto Learn Period    30d (2592000 seconds)</span><br><span class="line">Next Learn time      2019/06/03  12:02:38 (612878558 seconds)</span><br><span class="line">Learn Delay Interval 0 hour(s)</span><br><span class="line">Auto-Learn Mode      Enabled</span><br><span class="line">BBU Mode             4</span><br><span class="line">--------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show status</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Info :</span><br><span class="line">========</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">Property      Value</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">BatteryType   iBBU08</span><br><span class="line">Voltage       4056 mV</span><br><span class="line">Current       0 mA</span><br><span class="line">Temperature   31 C</span><br><span class="line">Battery State Optimal</span><br><span class="line">Design Mode   1: 12+ Hrs retention with a transparent learn cycle</span><br><span class="line">                 and best service life.</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BBU_Firmware_Status :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Property                                   Value</span><br><span class="line">-------------------------------------------------</span><br><span class="line">Charging Status                            None</span><br><span class="line">Voltage                                    OK</span><br><span class="line">Temperature                                OK</span><br><span class="line">Learn Cycle Requested                      No</span><br><span class="line">Learn Cycle Active                         No</span><br><span class="line">Learn Cycle Status                         OK</span><br><span class="line">Learn Cycle Timeout                        No</span><br><span class="line">I2C Errors Detected                        No</span><br><span class="line">Battery Pack Missing                       No</span><br><span class="line">Battery Replacement required               No</span><br><span class="line">Remaining Capacity Low                     No</span><br><span class="line">Periodic Learn Required                    No</span><br><span class="line">Transparent Learn                          No</span><br><span class="line">No space to cache offload                  No</span><br><span class="line">Pack is about to fail &amp; should be replaced No</span><br><span class="line">Cache Offload premium feature required     No</span><br><span class="line">Module microcode update required           No</span><br><span class="line">-------------------------------------------------</span><br><span class="line"></span><br><span class="line">GasGaugeStatus :</span><br><span class="line">==============</span><br><span class="line"></span><br><span class="line">--------------------------------------</span><br><span class="line">Property                   Value</span><br><span class="line">--------------------------------------</span><br><span class="line">Fully Discharged           No</span><br><span class="line">Fully Charged              No</span><br><span class="line">Discharging                No</span><br><span class="line">Initialized                Yes</span><br><span class="line">Remaining Time Alarm       No</span><br><span class="line">Terminate Discharge Alarm  No</span><br><span class="line">Over Temperature           No</span><br><span class="line">Charging Terminated        No</span><br><span class="line">Over Charged               No</span><br><span class="line">Relative State of Charge   97%</span><br><span class="line">Charger System State       1</span><br><span class="line">Charger System Ctrl        0</span><br><span class="line">Charging current           0 mA</span><br><span class="line">Absolute state of charge   78%</span><br><span class="line">Max Error                  0%</span><br><span class="line">Battery backup charge time 48 hours +</span><br><span class="line">--------------------------------------</span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show status | grep -i <span class="built_in">type</span></span><br><span class="line">BatteryType   iBBU08</span><br><span class="line"></span><br><span class="line"><span class="comment">#important value</span></span><br><span class="line">Remaining Capacity Low                  : No</span><br><span class="line"></span><br><span class="line"><span class="comment">#design capacity: 1500, max</span></span><br><span class="line">Relative State of Charge ＝ 1055 ／ 1198 ～ 89%</span><br><span class="line">Absolute State of charge ＝ 1055 ／ 1500 ～ 70%</span><br></pre></td></tr></table></figure>

<h4 id="Setting-BBU"><a href="#Setting-BBU" class="headerlink" title="Setting BBU"></a>Setting BBU</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /cx/bbu <span class="built_in">set</span> learnDelayInterval=168 <span class="comment">#hours</span></span><br><span class="line"></span><br><span class="line">$ /opt/MegaRAID/storcli/storcli64 /call/bbu show modes</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = Get BBU Modes Succeeded.</span><br><span class="line"></span><br><span class="line">Available BBU Modes :</span><br><span class="line">===================</span><br><span class="line">Mode-1 = 12+ Hrs retention with a transparent learn cycle and best service life.</span><br><span class="line">Mode-3 = 24+ Hrs retention with a transparent learn cycle and balanced service life.</span><br><span class="line">Mode-4 = 48+ Hrs retention with a non-transparent learn cycle and balanced service life</span><br><span class="line"></span><br><span class="line">$ storcli /cx/bbu <span class="built_in">set</span> bbuMode=&lt;value&gt;</span><br><span class="line">0 48 hours of retentiona at 60 °C, 1-year Service Life.</span><br><span class="line">a. Indicates how long the battery can hold data <span class="keyword">in</span> the controller<span class="string">'s memory in case of accidental system shutdown.</span></span><br><span class="line"><span class="string">1 12 hours of retention at 45 °C, 5-year Service Life, transparent learn.b</span></span><br><span class="line"><span class="string">b. The controller'</span>s performance is not affected during the battery<span class="string">'s learn cycle.</span></span><br><span class="line"><span class="string">2 12 hours of retention at 55 °C, 3-year Service Life, transparent learn.</span></span><br><span class="line"><span class="string">3 24 hours of retention at 45 °C, 3-year Service Life, transparent learn.</span></span><br><span class="line"><span class="string">4 48 hours of retention at 45 °C, 3-year Service Life.</span></span><br><span class="line"><span class="string">5 48 hours of retention at 55 °C, 1-year Service Life.</span></span><br><span class="line"><span class="string">6 Same as the description for BBU mode 5. The BBU mode 6 enables you to receive events when the battery capacity reaches suboptimal and critical thresholds.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli /cx bbu set autolearnmode=&lt;value&gt;</span></span><br><span class="line"><span class="string">where x= 0 – Enabled, 1 – Disabled, 2 – Warn though event.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## Manual learn battery</span></span><br><span class="line"><span class="string">$ storcli /c0/bbu start learn</span></span><br></pre></td></tr></table></figure>

<h4 id="Cachevault"><a href="#Cachevault" class="headerlink" title="Cachevault"></a><a href="https://www.thomas-krenn.com/en/wiki/CacheVault_Flash_Cache" target="_blank" rel="noopener">Cachevault</a></h4><p>Here is not install cachevalut, all FLASH devices don’t need it, because fastpath need writethrough mode</p>
<p>No battery is used with CacheVault technology. The cache content is protected by the following systems:</p>
<ul>
<li>A double-layer capacitor is connected to the RAID controller.</li>
<li>This will be fully loaded automatically during server startup.</li>
<li>In case of power failure the RAID controller uses the power of the capacitor to write the Flash-Memory to maintain all contents of the cache non-volatile (almost like a USB stick).</li>
<li>The next time the server is started the RAID controller writes the data from the Flash-Memory to the RAID Array.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0/cv show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Failure</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line">Detailed Status :</span><br><span class="line">===============</span><br><span class="line"></span><br><span class="line">----------------------------------------------------</span><br><span class="line">Ctrl Status Property ErrMsg                   ErrCd</span><br><span class="line">----------------------------------------------------</span><br><span class="line">   0 Failed -        Cachevault doesn<span class="string">'t exist   255</span></span><br><span class="line"><span class="string">----------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show all</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show learn</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv show status</span></span><br><span class="line"><span class="string">$ storcli64 /c0/cv start learn</span></span><br></pre></td></tr></table></figure>

<h4 id="Get-all-megaraid-log-not-filter"><a href="#Get-all-megaraid-log-not-filter" class="headerlink" title="Get all megaraid log, not filter"></a>Get all megaraid log, not filter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#shows the history of log files generated</span></span><br><span class="line">$ storcli64  /call show eventloginfo file=logfile</span><br><span class="line">$ MegaCli64 -AdpEventLog -GetEvents -f logfile -aALL</span><br><span class="line"></span><br><span class="line"><span class="comment">#prints the system log</span></span><br><span class="line">$ storcli64 /c0 show events file=megaraid_events.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># shows the firmware logs</span></span><br><span class="line">$ storcli64 /c0 show termlog </span><br><span class="line">$ storcli64 /c0 show termlog <span class="built_in">type</span>=config</span><br><span class="line">$ storcli64 /c0 show termlog <span class="built_in">type</span>=contents</span><br></pre></td></tr></table></figure>

<h4 id="Show-raid-logic-info"><a href="#Show-raid-logic-info" class="headerlink" title="Show raid logic info"></a>Show raid logic info</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx/dall show [all]</span><br><span class="line">$ MegaCli64 -cfgdsply -aALL</span><br></pre></td></tr></table></figure>

<h4 id="Import-license"><a href="#Import-license" class="headerlink" title="Import license"></a>Import license</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /c0 <span class="built_in">set</span> aso key=xxxxxxxxx</span><br></pre></td></tr></table></figure>

<h4 id="Backup-config"><a href="#Backup-config" class="headerlink" title="Backup config"></a>Backup config</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli -CfgSave -f filename -aN   </span><br><span class="line">$ MegaCli -CfgRestore -f filename -aN  </span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 <span class="built_in">set</span> config file=raid_adapter0.config</span><br><span class="line">$ storcli64 /c0 get config file=raid_adapter0.config</span><br></pre></td></tr></table></figure>

<h4 id="Interrupt-BGI"><a href="#Interrupt-BGI" class="headerlink" title="Interrupt BGI"></a>Interrupt BGI</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -LDBI -Abort -LALL -aALL</span><br><span class="line">$ storcli64 /call/vall <span class="built_in">set</span> autobgi=On|Off</span><br><span class="line">$ storcli64 /call/vall show autobgi</span><br><span class="line">$ storcli64 /call/vall stop bgi</span><br><span class="line">$ storcli64 /call/vall pause bgi</span><br><span class="line">$ storcli64 /call/vall resume bgi</span><br><span class="line">$ storcli64 /call/vall show bgi</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> bgirate=90</span><br></pre></td></tr></table></figure>

<h4 id="Rebuild-Migrate-init"><a href="#Rebuild-Migrate-init" class="headerlink" title="Rebuild/Migrate/init"></a>Rebuild/Migrate/init</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64  -AdpSetProp RebuildRate 50 -a0</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> rebuildrate=50</span><br><span class="line"></span><br><span class="line">$ MegaCli64  -AdpSetProp ReconRate 50 -a0</span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDRecon ShowProg L1  -a0</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> migraterate=50</span><br><span class="line">$ storcli /c0/v1 show migrate</span><br><span class="line">$ storcli /c0/v1 start migrate <span class="built_in">type</span>=raidx</span><br><span class="line"></span><br><span class="line"><span class="comment">#  automatic background initialization</span></span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDBI -ShowProg -LALL -aALL</span><br><span class="line">$ storcli /call/vall <span class="built_in">set</span> autobgi=on</span><br><span class="line">$ storcli /call/vall show autobgi</span><br><span class="line">$ storcli /call/vall stop bgi</span><br><span class="line">$ storcli /call/vall pause bgi</span><br><span class="line">$ storcli /call/vall resume bgi</span><br><span class="line">$ storcli /call/vall show bgi</span><br><span class="line"></span><br><span class="line"><span class="comment"># virtual drive initialization</span></span><br><span class="line">$ /opt/MegaRAID/MegaCli/MegaCli64 -LDInit ShowProg Lall a0</span><br><span class="line">$ storcli /c0/vall start init</span><br><span class="line">$ storcli /c0/vall stop init</span><br><span class="line">$ storcli /c0/vall show init</span><br></pre></td></tr></table></figure>

<h4 id="Make-device-good"><a href="#Make-device-good" class="headerlink" title="Make device good"></a>Make device good</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -PDMakeGood -PhysDrv[20:12] -a0</span><br><span class="line">$ storcli64 /c0/e20/s12 <span class="built_in">set</span> good force</span><br></pre></td></tr></table></figure>

<h4 id="Make-JBOD"><a href="#Make-JBOD" class="headerlink" title="Make JBOD"></a>Make JBOD</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64 -PDMakeJBOD -PhysDrv[E0:S0,E1:S1,...] -aN|-a0,1,2|-aALL</span><br><span class="line">$ storcli64 /c0/e20/s12 <span class="built_in">set</span> jbod</span><br><span class="line"></span><br><span class="line">$ MegaCli -AdpSetProp -EnableJBOD -val -aN|-a0,1,2|-aALL</span><br><span class="line">      val - 0=Disable JBOD mode.</span><br><span class="line">            1=Enable JBOD mode.</span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 <span class="built_in">set</span> jbod=on</span><br></pre></td></tr></table></figure>

<h4 id="Foreign-devices"><a href="#Foreign-devices" class="headerlink" title="Foreign devices"></a>Foreign devices</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ MegaCli64  -CfgForeign -Clear -aALL</span><br><span class="line">$ storcli64 /c0/fall del|delete</span><br><span class="line"></span><br><span class="line">$ storcli64 /c0/fall show</span><br><span class="line">$ storcli64 /c0/fall import</span><br></pre></td></tr></table></figure>

<h3 id="About-Consistency-and-patrol-read"><a href="#About-Consistency-and-patrol-read" class="headerlink" title="About Consistency and patrol read"></a><a href="https://www.digiliant.com/docs/MegaRAID-SAS-Software-User-Guide-12Gbs.pdf" target="_blank" rel="noopener">About Consistency and patrol read</a></h3><h4 id="Patrol-read"><a href="#Patrol-read" class="headerlink" title="Patrol read"></a>Patrol read</h4><p>Patrol read involves the review of your system for possible drive errors that could lead to drive failure and then action to correct errors. The goal is to protect data integrity by detecting drive failure before the failure can damage data. The corrective actions depend on the drive group configuration and the type of errors.<br>Patrol read starts only when the controller is idle for a defined period of time and no other background tasks are active, though it can continue to run during heavy I/O processes.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show patrolRead</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line">Ctrl_Prop               Value</span><br><span class="line">---------------------------------------------</span><br><span class="line">PR Mode                 Auto</span><br><span class="line">PR Execution Delay      168 hours</span><br><span class="line">PR iterations completed 0</span><br><span class="line">PR Next Start time      08/24/2019, 03:00:00</span><br><span class="line">PR on SSD               Disabled</span><br><span class="line">PR on EPD               Disabled</span><br><span class="line">PR Current State        Stopped</span><br><span class="line">---------------------------------------------</span><br><span class="line"><span class="comment"># mode=auto/manual</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=on mode=auto includessds=on maxconcurrentpd=255 delay=360 <span class="comment">#255 phy devs ,360 housr(15days)</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=on mode=auto starttime=2019/08/24 03</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> patrolread=off</span><br><span class="line"></span><br><span class="line"><span class="comment">##show/stop/start/resume/pause</span></span><br><span class="line">$ storcli64 /call start patrolRead </span><br><span class="line">$ storcli64 /call show patrolRead</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">----------------------------------</span><br><span class="line">Ctrl_Prop               Value</span><br><span class="line">----------------------------------</span><br><span class="line">PR Mode                 Auto</span><br><span class="line">PR Execution Delay      360 hours</span><br><span class="line">PR iterations completed 352</span><br><span class="line">PR on SSD               Enabled</span><br><span class="line">PR Current State        Stopped</span><br><span class="line">----------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli64 /call show prrate</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> prrate=40</span><br></pre></td></tr></table></figure>

<h4 id="Consistency-Check"><a href="#Consistency-Check" class="headerlink" title="Consistency Check"></a>Consistency Check</h4><p>checking consistency means calculating the data on one drive and comparing the results to the contents of the parity drive<br>It is recommended that you perform a consistency check at least once a month.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-----------------------------------------------</span><br><span class="line">Ctrl_Prop                 Value</span><br><span class="line">-----------------------------------------------</span><br><span class="line">CC Operation Mode         Concurrent</span><br><span class="line">CC Execution Delay        168</span><br><span class="line">CC Next Starttime         08/24/2019, 02:00:00</span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   311</span><br><span class="line">CC Number of VD completed 2</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line">-----------------------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli64 /call show ccrate</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> ccrate=50</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#schedule</span></span><br><span class="line">$ storcli64 /call/vall <span class="built_in">set</span> cc=seq </span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> cc=seq starttime=2019/08/31 02 delay=1440  <span class="comment">## check interval 1440 hours ,60 days</span></span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">---------------------------------</span><br><span class="line">Ctrl_Prop    Value</span><br><span class="line">---------------------------------</span><br><span class="line">CC Mode      SEQ</span><br><span class="line">CC Starttime 2019/08/31 02:00:00</span><br><span class="line">CC delay     1440</span><br><span class="line">---------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">### if I 'm not set start time</span></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> cc=seq delay=1440</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line">Ctrl_Prop Value</span><br><span class="line">----------------</span><br><span class="line">CC Mode   SEQ</span><br><span class="line">CC delay  1440</span><br><span class="line">----------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">CC Operation Mode         Sequential</span><br><span class="line">CC Execution Delay        1440</span><br><span class="line">CC Next Starttime         08/24/2019, 03:00:00 <span class="comment">## first start after about 12 hours</span></span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   120</span><br><span class="line">CC Number of VD completed 1</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### show again</span></span><br><span class="line">$ storcli64 /c0 show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-----------------------------------------------</span><br><span class="line">Ctrl_Prop                 Value</span><br><span class="line">-----------------------------------------------</span><br><span class="line">CC Operation Mode         Sequential</span><br><span class="line">CC Execution Delay        1440</span><br><span class="line">CC Next Starttime         08/31/2019, 02:00:00</span><br><span class="line">CC Current State          Stopped</span><br><span class="line">CC Number of iterations   311</span><br><span class="line">CC Number of VD completed 2</span><br><span class="line">CC Excluded VDs           None</span><br><span class="line">-----------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">#manual start and show status</span></span><br><span class="line"><span class="comment">#show/stop/start/resume/pause</span></span><br><span class="line">$ storcli64 /call/vall start cc [force]</span><br><span class="line">$ storcli64 /call/vall show cc</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">VD Operation Status :</span><br><span class="line">===================</span><br><span class="line"></span><br><span class="line">-----------------------------------</span><br><span class="line">VD Operation Progress% Status</span><br><span class="line">-----------------------------------</span><br><span class="line"> 0 CC                0 In progress</span><br><span class="line"> 1 CC                1 In progress</span><br><span class="line">-----------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ storcli64 /call/vall stop cc</span><br></pre></td></tr></table></figure>

<h3 id="Montor-cache-ECC-count"><a href="#Montor-cache-ECC-count" class="headerlink" title="Montor cache ECC count"></a>Montor cache ECC count</h3><p>eccbucketleakrate 0 to 65535 Sets the leak rate of the single-bit bucket in minutes (one entry removed per leak-rate).<br>eccbucketsize 0 to 255 Sets the size of ECC single-bit-error bucket (logs event when full).</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /c0 show all | grep Bucket</span><br><span class="line">ECC Bucket Count = 0</span><br><span class="line">ECC Bucket Size = 255</span><br><span class="line">ECC Bucket Leak Rate (hrs) = 4</span><br></pre></td></tr></table></figure>

<h3 id="About-device-write-cache-in-megaraid"><a href="#About-device-write-cache-in-megaraid" class="headerlink" title="About device write cache in megaraid"></a><a href="https://utcc.utoronto.ca/~cks/space/blog/tech/ModernDiskWriteCaches" target="_blank" rel="noopener">About device write cache in megaraid</a></h3><p>drives can also support a write option called ‘Force Unit Access’ (FUA) that bypasses the write cache in order to force what you’re writing to be forced to disk. In general FUA is bundled with another feature called ‘Disable Page Out’ (DPO), which tells the drive that putting the data into cache is not useful.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ Virtual Drives = 2</span><br><span class="line"></span><br><span class="line">VD LIST :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">DG/VD TYPE  State Access Consist Cache Cac sCC     Size Name</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line">0/0   RAID1 Optl  RW     Yes     NRWTD -   OFF 223.0 GB R1OS</span><br><span class="line">1/1   RAID5 Optl  RW     Yes     RWTD  -   OFF 1.307 TB</span><br><span class="line">-------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">Cac=CacheCade|Rec=Recovery|OfLn=OffLine|Pdgd=Partially Degraded|dgrd=Degraded</span><br><span class="line">Optl=Optimal|RO=Read Only|RW=Read Write|HD=Hidden|B=Blocked|Consist=Consistent|</span><br><span class="line">R=Read Ahead Always|NR=No Read Ahead|WB=WriteBack|</span><br><span class="line">AWB=Always WriteBack|WT=WriteThrough|C=Cached IO|D=Direct IO|sCC=Scheduled</span><br><span class="line">Check Consistency</span><br><span class="line"></span><br><span class="line">Physical Drives = 6</span><br><span class="line"></span><br><span class="line">PD LIST :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">EID:Slt DID State DG       Size Intf Med SED PI SeSz Model         Sp</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line">32:0      0 Onln   0   223.0 GB SATA SSD N   N  512B THNSF8240CCSE U</span><br><span class="line">32:1      1 Onln   0   223.0 GB SATA SSD N   N  512B THNSF8240CCSE U</span><br><span class="line">32:2      2 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:3      3 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:4      4 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">32:5      5 Onln   1 446.625 GB SATA SSD N   N  512B MTFDDAK480TDC U</span><br><span class="line">----------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ dmesg -T | grep DPO</span><br><span class="line">[Tue Nov  5 23:21:47 2019] sd 0:2:1:0: [sdb] Write cache: disabled, <span class="built_in">read</span> cache: enabled, supports DPO and FUA</span><br><span class="line">[Tue Nov  5 23:21:47 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line"></span><br><span class="line">$ perccli64 /c0/vall <span class="built_in">set</span> wrcache=WT rdcache=nora iopolicy=direct</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line">Detailed Status :</span><br><span class="line">===============</span><br><span class="line"></span><br><span class="line">----------------------------------------</span><br><span class="line">VD Property Value  Status  ErrCd ErrMsg</span><br><span class="line">----------------------------------------</span><br><span class="line"> 0 wrCache  WT     Success     0 -</span><br><span class="line"> 0 rdCache  NoRA   Success     0 -</span><br><span class="line"> 0 IoPolicy Direct Success     0 -</span><br><span class="line"> 1 wrCache  WT     Success     0 -</span><br><span class="line"> 1 rdCache  NoRA   Success     0 -</span><br><span class="line"> 1 IoPolicy Direct Success     0 -</span><br><span class="line">----------------------------------------</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$ dmesg -T | grep DPO</span><br><span class="line">[Thu Nov  7 02:37:51 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line">[Thu Nov  7 02:37:51 2019] sd 0:2:1:0: [sdb] Write cache: disabled, <span class="built_in">read</span> cache: disabled, supports DPO and FUA</span><br><span class="line"></span><br><span class="line">Supported VD Operations :</span><br><span class="line">=======================</span><br><span class="line">Read Policy = Yes</span><br><span class="line">Write Policy = Yes</span><br><span class="line">IO Policy = Yes</span><br><span class="line">Access Policy = Yes</span><br><span class="line">Disk Cache Policy = Yes</span><br><span class="line"></span><br><span class="line">$ perccli64 /c0/v0 <span class="built_in">set</span> wrcache=WB</span><br><span class="line">VD Property Value Status  ErrCd ErrMsg</span><br><span class="line">---------------------------------------</span><br><span class="line"> 0 wrCache  WB    Success     0 -</span><br><span class="line">---------------------------------------</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line">$  dmesg -T | grep DPO</span><br><span class="line">[Thu Nov  7 02:53:00 2019] sd 0:2:0:0: [sda] Write cache: disabled, <span class="built_in">read</span> cache: disabled, doesn<span class="string">'t support DPO or FUA</span></span><br><span class="line"><span class="string">[Thu Nov  7 02:53:00 2019] sd 0:2:1:0: [sdb] Write cache: disabled, read cache: disabled, supports DPO and FUA</span></span><br></pre></td></tr></table></figure>
<p>You could change megaraid cache policy, let the device support DPO and FUA</p>
<h3 id="Backup-and-restore-config-file"><a href="#Backup-and-restore-config-file" class="headerlink" title="Backup and restore config file"></a>Backup and restore config file</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#backup controller config to file;</span></span><br><span class="line">./storcli64 /c0 get config file=megaraid.cfg</span><br><span class="line">./storcli64 /c0 <span class="built_in">set</span> config file=megaraid.cfg</span><br><span class="line"><span class="comment">#Clear a Configuration; dangerous</span></span><br><span class="line"><span class="comment">###./storcli64 /c0 delete config[force]</span></span><br></pre></td></tr></table></figure>

<h3 id="Flush-RAID-write-cache"><a href="#Flush-RAID-write-cache" class="headerlink" title="Flush RAID write cache"></a>Flush RAID write cache</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Cache Flush on Selected Controller</span><br><span class="line">$ MegaCli64 –AdpCacheFlush -aN|-a0,1,2|-aALL</span><br><span class="line">$ storcli64 &#x2F;c0 set flushwriteverify&#x3D;on</span><br><span class="line">$ storcli64 &#x2F;c0 show flushwriteverify </span><br><span class="line">$ storcli64 &#x2F;c0 flushcache</span><br><span class="line"></span><br><span class="line">$ perccli64 &#x2F;c0 flushcache</span><br><span class="line">Controller &#x3D; 0</span><br><span class="line">Status &#x3D; Success</span><br><span class="line">Description &#x3D; Adapter and&#x2F;or disk caches flushed successfully.</span><br><span class="line"></span><br><span class="line"># delete the preserved cache for a particular virtual driver on the controller in missing state</span><br><span class="line">$ perccli64 &#x2F;c0&#x2F;v1 delete preservedCache </span><br><span class="line">$ storcli64 &#x2F;c0&#x2F;v1 delete preservedCache force</span><br><span class="line"></span><br><span class="line">$ storcli64 &#x2F;c0 show preservedCache</span><br><span class="line">Controller &#x3D; 0</span><br><span class="line">Status &#x3D; Success</span><br><span class="line">Description &#x3D; No Virtual Drive has Preserved Cache Data.</span><br></pre></td></tr></table></figure>

<h3 id="show-PHY"><a href="#show-PHY" class="headerlink" title="show PHY"></a>show PHY</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli /cx/px|pall <span class="built_in">set</span> linkspeed=0(auto)|1.5|3|6|12</span><br><span class="line">$ perccli64 /c0/pall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PhyInfo :</span><br><span class="line">=======</span><br><span class="line"></span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line">PhyNo SAS_Addr           Phy_Identifier Link_Speed Device_Type   Description</span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line">    0 0x500056B3BECB2FFF             17 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    1 0x500056B3BECB2FFF             20 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    2 0x500056B3BECB2FFF             18 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    3 0x500056B3BECB2FFF             23 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    4 0x500056B3BECB2FFF             19 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    5 0x500056B3BECB2FFF             21 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    6 0x500056B3BECB2FFF             16 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">    7 0x500056B3BECB2FFF             22 No <span class="built_in">limit</span>   Edge Expander -</span><br><span class="line">-----------------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">$ perccli64 /call/eall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Properties :</span><br><span class="line">==========</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line">EID State Slots PD PS Fans TSs Alms SIM Port<span class="comment"># ProdID VendorSpecific</span></span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"> 32 OK        8  6  0    0   0    0   0 00 x1 BP14G+  !   ?</span><br><span class="line">--------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">EID-Enclosure Device ID |PD-Physical drive count |PS-Power Supply count|</span><br><span class="line">TSs-Temperature sensor count |Alms-Alarm count |SIM-SIM Count</span><br><span class="line"></span><br><span class="line">$ perccli64 /call/eall show status</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"><span class="comment"># upgrade the enclosure</span></span><br><span class="line">$ storcli64 /c0/e32 download src=enclosure_fw.bin</span><br></pre></td></tr></table></figure>

<h3 id="Foreign-configurations"><a href="#Foreign-configurations" class="headerlink" title="Foreign configurations"></a>Foreign configurations</h3><p>NOTE Provide the security key when importing a locked foreign configuration created in a different machine that is encrypted with a security key.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0/fall show</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = Couldn<span class="string">'t find any foreign Configuration</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ storcli64 /c0/fall import</span></span><br><span class="line"><span class="string">$ storcli64 /cx/fx|fall show [all][ securitykey=sssssssssss ]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#This command deletes the foreign configuration of a controller.</span></span><br><span class="line"><span class="string">$ ### storcli64 /c0/fall delete</span></span><br></pre></td></tr></table></figure>

<h3 id="Dimmer-switch"><a href="#Dimmer-switch" class="headerlink" title="Dimmer switch"></a>Dimmer switch</h3><p>The Dimmer Switch is the power-saving policy for the virtual drive</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0 show ds</span><br><span class="line">Controller = 0</span><br><span class="line">Status = Success</span><br><span class="line">Description = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Controller Properties :</span><br><span class="line">=====================</span><br><span class="line"></span><br><span class="line">-------------------------------</span><br><span class="line">Ctrl_Prop         Value</span><br><span class="line">-------------------------------</span><br><span class="line">SpnDwnUncDrv      Disabled</span><br><span class="line">SpnDwnHS          Disabled</span><br><span class="line">SpnDwnTm          30 minute(s)</span><br><span class="line">SpnDwnCfgDrv      Disabled</span><br><span class="line">DefaultLdPSPolicy None</span><br><span class="line">DsblLdPsInterval  0 hours</span><br><span class="line">DsblLdPsTime      0 minutes</span><br><span class="line">SpnUpEncDly       8 second(s)</span><br><span class="line">spinUpEncDrvCnt   6</span><br><span class="line">-------------------------------</span><br><span class="line"></span><br><span class="line">$ storcli /cx/vx <span class="built_in">set</span> ds=default | auto | none | max | maxnocache</span><br><span class="line">auto: Logical device power savings are managed by the firmware.</span><br><span class="line">none: No power saving policy.</span><br><span class="line">max: Logical device uses maximum power savings.</span><br><span class="line">maxnocache: Logical device does not cache write to maximise power savings.</span><br><span class="line"></span><br><span class="line">$ storcli64  /c0 <span class="built_in">set</span> ds=on <span class="built_in">type</span>=1|2</span><br><span class="line">1: Unconfigured</span><br><span class="line">2: Hot spare</span><br><span class="line">3: Virtual drive</span><br><span class="line">4: All</span><br><span class="line"></span><br><span class="line">$ storcli /cx <span class="built_in">set</span> ds=on [properties]</span><br><span class="line">disableldps: Interval <span class="keyword">in</span> hours or time <span class="keyword">in</span> hh:mm format</span><br><span class="line">spinupdrivecount: Valid enclosure number (0 to 255)</span><br><span class="line">SpinUpEncDelay: Valid time <span class="keyword">in</span> seconds</span><br></pre></td></tr></table></figure>

<h3 id="Device"><a href="#Device" class="headerlink" title="Device"></a>Device</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx[/ex]/sx spindown</span><br><span class="line">$ storcli64 /cx[/ex]/sx spinup</span><br><span class="line"></span><br><span class="line">$ storcli64 /cx[/ex]/sx secureerase [force]</span><br><span class="line">$ storcli64 /cx[/ex]/sx start erase [simple|normal|thorough] [erasepatternA=&lt;value1&gt;] [erasepatternB=&lt;value2&gt;]</span><br><span class="line">$ storcli64 /cx[/ex]/sx stop erase</span><br></pre></td></tr></table></figure>

<h3 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h3><h4 id="log"><a href="#log" class="headerlink" title="log"></a>log</h4><p>— 0 – NA<br>— 1– SET<br>— 2 – CLEAR<br>— 3 – CLEAR ALL<br>— 4 – DEBUG DUMP</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call show all logfile=megaraid.cfg</span><br><span class="line">$ storcli64 /c x <span class="built_in">set</span> debug reset all </span><br><span class="line">$ storcli64 /call x <span class="built_in">set</span> debug <span class="built_in">type</span> = &lt;value&gt; option = &lt;value&gt; level = [&lt;value <span class="keyword">in</span> hex&gt;]</span><br><span class="line"><span class="comment">#type – takes the value from 0 – 128, mapping each number to a particular debug variable in the firmware.</span></span><br><span class="line">$ <span class="comment">#storcli64 /call delete events</span></span><br><span class="line">$ <span class="comment">#storcli64 /call delete termlog</span></span><br><span class="line"></span><br><span class="line">$ storcli64 /call show events file=events.log</span><br><span class="line">$ storcli64 /call show termlog <span class="built_in">type</span>=config logfile=termlog.config</span><br><span class="line">$ storcli64 /call show termlog <span class="built_in">type</span>=contents logfile=termlog.contents</span><br><span class="line">$ storcli64 /call show eventloginfo logfile=event.info</span><br><span class="line">$ storcli64 /call show dequeuelog file=dequeue.log</span><br><span class="line">$ storcli64 /call show alilog logfile=ali.log</span><br></pre></td></tr></table></figure>
<h4 id="Encolsure"><a href="#Encolsure" class="headerlink" title="Encolsure"></a>Encolsure</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call/eall download src=encolsure.bin</span><br><span class="line">$ perccli64 /call/eall show all</span><br><span class="line">$ perccli64 /call/eall show status</span><br></pre></td></tr></table></figure>

<h4 id="phy-err"><a href="#phy-err" class="headerlink" title="phy err"></a>phy err</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">storcli /cx/px|pall show</span><br><span class="line">storcli /cx/px|pall show all</span><br><span class="line">storcli /cx/ex show phyerrorcounters</span><br><span class="line">storcli /cx[/ex]/sx show phyerrorcounters</span><br><span class="line">storcli /cx[/ex]/sx reset phyerrorcounters</span><br><span class="line">storcli /cx/px|pall <span class="built_in">set</span> linkspeed=0(auto)|1.5|3|6|12</span><br></pre></td></tr></table></figure>

<h4 id="diag-adapter"><a href="#diag-adapter" class="headerlink" title="diag adapter"></a>diag adapter</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /c0 start diag duration=10 logfile=diag.log</span><br></pre></td></tr></table></figure>

<h4 id="Enable-SEP-for-Dell-PERC"><a href="#Enable-SEP-for-Dell-PERC" class="headerlink" title="Enable SEP for Dell PERC"></a>Enable SEP for Dell PERC</h4><p>Configures enclosure detection on a non-SES/expander backplane.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ perccli64 /call <span class="built_in">set</span> backplane mode=0 expose=on</span><br><span class="line">0: Use autodetect logic of backplanes, such as SGPIO and I2C SEP using GPIO pins.</span><br><span class="line">1: Disable autodetect SGPIO.</span><br><span class="line">2: Disable I2C SEP autodetect.</span><br><span class="line">3: Disable both the autodetects.</span><br><span class="line"></span><br><span class="line">backplane expose: Enables or disables device drivers to expose enclosure devices; <span class="keyword">for</span> example, expanders, SEPs.</span><br><span class="line"></span><br><span class="line">$ perccli64 /call <span class="built_in">set</span> sesmonitoring=on</span><br></pre></td></tr></table></figure>


<h3 id="Enable-JBOD"><a href="#Enable-JBOD" class="headerlink" title="Enable JBOD"></a>Enable JBOD</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call <span class="built_in">set</span> jbod=on</span><br></pre></td></tr></table></figure>

<h3 id="SAS-loading-balance"><a href="#SAS-loading-balance" class="headerlink" title="SAS loading balance"></a>SAS loading balance</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /cx <span class="built_in">set</span> loadbalancemode=on</span><br></pre></td></tr></table></figure>

<h3 id="Driver-performance"><a href="#Driver-performance" class="headerlink" title="Driver performance"></a>Driver performance</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call <span class="built_in">set</span> DPM=on</span><br><span class="line"><span class="comment">#Enables or disables drive performance monitoring</span></span><br><span class="line"></span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> ncq=on</span><br><span class="line">$ storcli64 /call <span class="built_in">set</span> perfmode=1 <span class="comment">#default was 0</span></span><br><span class="line">0: Tuned to provide best IOPS, currently applicable to non-FastPath</span><br><span class="line">1: Tuned to provide least latency, currently applicable to non-FastPath</span><br></pre></td></tr></table></figure>

<h4 id="Get-smart-info-from-LSI-megaraid"><a href="#Get-smart-info-from-LSI-megaraid" class="headerlink" title="Get smart info from LSI megaraid"></a>Get smart info from LSI megaraid</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ smartctl -s on -a -d megaraid,1 /dev/sda -T permissive</span><br><span class="line">$ smartctl -s on -a -d megaraid,0 /dev/sda -T permissive</span><br><span class="line">smartctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-957.1.3.el7.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">=== START OF INFORMATION SECTION ===</span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUSMM1616ASS204</span><br><span class="line">Revision:             L380</span><br><span class="line">Compliance:           SPC-4</span><br><span class="line">User Capacity:        1,600,321,314,816 bytes [1.60 TB]</span><br><span class="line">Logical block size:   512 bytes</span><br><span class="line">LU is resource provisioned, LBPRZ=1</span><br><span class="line">Rotation Rate:        Solid State Device</span><br><span class="line">Form Factor:          2.5 inches</span><br><span class="line">Logical Unit id:      0x0000000000000000</span><br><span class="line">Serial number:        0000000</span><br><span class="line">Device <span class="built_in">type</span>:          disk</span><br><span class="line">Transport protocol:   SAS (SPL-3)</span><br><span class="line">Local Time is:        Thu Dec 27 15:44:50 2018 CST</span><br><span class="line">SMART support is:     Available - device has SMART capability.</span><br><span class="line">SMART support is:     Enabled</span><br><span class="line">Temperature Warning:  Disabled or Not Supported</span><br><span class="line"></span><br><span class="line">=== START OF ENABLE/DISABLE COMMANDS SECTION ===</span><br><span class="line">Informational Exceptions (SMART) enabled</span><br><span class="line">Temperature warning enabled</span><br><span class="line"></span><br><span class="line">=== START OF READ SMART DATA SECTION ===</span><br><span class="line">SMART Health Status: OK</span><br><span class="line"></span><br><span class="line">Percentage used endurance indicator: 0%</span><br><span class="line">Current Drive Temperature:     32 C</span><br><span class="line">Drive Trip Temperature:        60 C</span><br><span class="line"></span><br><span class="line">Manufactured <span class="keyword">in</span> week 53 of year 2016</span><br><span class="line">Specified cycle count over device lifetime:  0</span><br><span class="line">Accumulated start-stop cycles:  0</span><br><span class="line">Specified load-unload count over device lifetime:  0</span><br><span class="line">Accumulated load-unload cycles:  0</span><br><span class="line">Vendor (Seagate) cache information</span><br><span class="line">  Blocks sent to initiator = 6325615015755776</span><br><span class="line"></span><br><span class="line">Error counter <span class="built_in">log</span>:</span><br><span class="line">           Errors Corrected by           Total   Correction     Gigabytes    Total</span><br><span class="line">               ECC          rereads/    errors   algorithm      processed    uncorrected</span><br><span class="line">           fast | delayed   rewrites  corrected  invocations   [10^9 bytes]  errors</span><br><span class="line"><span class="built_in">read</span>:          0        0         0         0          0       6874.841           0</span><br><span class="line">write:         0        0         0         0          0      10898.936           0</span><br><span class="line">verify:        0        0         0         0          0         32.879           0</span><br><span class="line"></span><br><span class="line">Non-medium error count:        0</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>raid</category>
      </categories>
      <tags>
        <tag>raid</tag>
        <tag>megaraid</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux container with zfs</title>
    <url>/2019/04/02/lxc_with_zfs/</url>
    <content><![CDATA[<h3 id="Create-bridge"><a href="#Create-bridge" class="headerlink" title="Create bridge"></a>Create bridge</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ brctl show</span><br><span class="line">bridge name	bridge id		STP enabled	interfaces</span><br><span class="line">br0		8000.ba02751fd447	no		eno2</span><br><span class="line"></span><br><span class="line">$ cat /etc/netplan/50-cloud-init.yaml</span><br><span class="line"><span class="comment"># This file is generated from information provided by</span></span><br><span class="line"><span class="comment"># the datasource.  Changes to it will not persist across an instance.</span></span><br><span class="line"><span class="comment"># To disable cloud-init's network configuration capabilities, write a file</span></span><br><span class="line"><span class="comment"># /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:</span></span><br><span class="line"><span class="comment"># network: &#123;config: disabled&#125;</span></span><br><span class="line">network:</span><br><span class="line">    version: 2</span><br><span class="line">    ethernets:</span><br><span class="line">        eno1:</span><br><span class="line">            addresses: []</span><br><span class="line">            dhcp4: <span class="literal">false</span></span><br><span class="line">            dhcp6: <span class="literal">false</span></span><br><span class="line">        eno2:</span><br><span class="line">            addresses: []</span><br><span class="line">            dhcp4: <span class="literal">false</span></span><br><span class="line">            dhcp6: <span class="literal">false</span></span><br><span class="line">        eno3:</span><br><span class="line">            addresses: [AA.AA.AA.AA/BB]</span><br><span class="line">            dhcp4: <span class="literal">false</span></span><br><span class="line">            dhcp6: <span class="literal">false</span></span><br><span class="line">        eno4:</span><br><span class="line">            addresses: [BB.BB.BB.BB/CC]</span><br><span class="line">            <span class="comment">#gateway4: your.gatway</span></span><br><span class="line">            <span class="comment">#nameservers:</span></span><br><span class="line">            <span class="comment">#    addresses:</span></span><br><span class="line">            <span class="comment">#    - your.dns.addr</span></span><br><span class="line">            <span class="comment">#    search: []</span></span><br><span class="line">    bridges:</span><br><span class="line">      br0:</span><br><span class="line">        interfaces: [eno2]</span><br><span class="line">        dhcp4: <span class="literal">true</span></span><br><span class="line">        dhcp6: <span class="literal">false</span></span><br><span class="line">        addresses: [CC.CC.CC.CC/DD]</span><br><span class="line">        gateway4: your.gatway</span><br><span class="line">        nameservers:</span><br><span class="line">          addresses: [your.dns.addr]</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="Add-bridge"><a href="#Add-bridge" class="headerlink" title="Add bridge"></a>Add bridge</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lxc init</span><br><span class="line">$ cat &lt;&lt;EOF | lxc profile edit default</span><br><span class="line">description: Bridged networking LXD profile</span><br><span class="line">devices:</span><br><span class="line">  eth0:</span><br><span class="line">    name: eth0</span><br><span class="line">    nictype: bridged</span><br><span class="line">    parent: br0</span><br><span class="line">    <span class="built_in">type</span>: nic</span><br><span class="line">EOF</span><br><span class="line">$ lxc profile list</span><br><span class="line">$ lxc profile show default</span><br><span class="line"></span><br><span class="line"><span class="comment"># if you are not default</span></span><br><span class="line">$ lxc profile delete your_custom_config</span><br></pre></td></tr></table></figure>

<h3 id="Add-zfs-zpool"><a href="#Add-zfs-zpool" class="headerlink" title="Add zfs zpool"></a>Add zfs zpool</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ zfs create tank&#x2F;lxd</span><br><span class="line">$ lxc storage create zfs-pool zfs source&#x3D;tank&#x2F;lxd</span><br><span class="line"></span><br><span class="line">$ lxc profile show default</span><br><span class="line">config: &#123;&#125;</span><br><span class="line">description: Default LXD profile</span><br><span class="line">devices:</span><br><span class="line">  eth0:</span><br><span class="line">    name: eth0</span><br><span class="line">    nictype: bridged</span><br><span class="line">    parent: br0</span><br><span class="line">    type: nic</span><br><span class="line">  root:</span><br><span class="line">    path: &#x2F;</span><br><span class="line">    pool: zfs-pool</span><br><span class="line">    size: &quot;0&quot;</span><br><span class="line">    type: disk</span><br><span class="line">name: default</span><br><span class="line">used_by:</span><br><span class="line">- &#x2F;1.0&#x2F;containers&#x2F;centos7-01</span><br></pre></td></tr></table></figure>

<h3 id="Custom-centos-7-image"><a href="#Custom-centos-7-image" class="headerlink" title="Custom centos 7 image"></a>Custom centos 7 image</h3><p>Found a centos 7 server</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount CentOS-7-x86_64-Everything-1804.iso /mnt -o loop</span><br><span class="line">$ cat /tank/vms/lxc-images/centos7/etc/yum.repos.d/base.repo</span><br><span class="line">[base]</span><br><span class="line">gpgcheck=0</span><br><span class="line">baseurl=file:/mnt</span><br><span class="line">name=CentOS-7</span><br><span class="line"></span><br><span class="line">$ yum -y --installroot=/tank/vms/lxc-images/centos7 install systemd passwd yum redhat-release vim-minimal openssh-server screen qperf sysstat lsof nfs-utils rsync libselinux-python openssh-clients net-tools strace attr yum-utils coreutils pciutils</span><br><span class="line">$ rm -f /tank/vms/lxc-images/centos7/etc/yum.repos.d/CentOS-*</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /tank/vms/lxc-images/centos7/</span><br><span class="line">$ tar cjvf  centos-7-x86_64.tar.gz *</span><br><span class="line"></span><br><span class="line">$ cat metadata.yaml</span><br><span class="line">architecture: <span class="string">"x86_64"</span></span><br><span class="line">creation_date: 1554097535 <span class="comment"># To get current date in Unix time, use `date +%s` command</span></span><br><span class="line">properties:</span><br><span class="line">architecture: <span class="string">"x86_64"</span></span><br><span class="line">description: <span class="string">"CentOS 7.6 add openssh"</span></span><br><span class="line">os: <span class="string">"centos"</span></span><br><span class="line">release: <span class="string">"sid"</span></span><br><span class="line"></span><br><span class="line">$ tar czvf metadata.tar.gz metadata.yaml</span><br><span class="line">$ lxc image import metadata.tar.gz centos-7-x86_64.tar.gz --<span class="built_in">alias</span> custom-for-centos7</span><br><span class="line"></span><br><span class="line">$ lxc image list</span><br><span class="line">+--------------------+--------------+--------+-------------+--------+----------+-----------------------------+</span><br><span class="line">|       ALIAS        | FINGERPRINT  | PUBLIC | DESCRIPTION |  ARCH  |   SIZE   |         UPLOAD DATE         |</span><br><span class="line">+--------------------+--------------+--------+-------------+--------+----------+-----------------------------+</span><br><span class="line">| custom-for-centos7 | 662e0004303e | no     |             | x86_64 | 174.80MB | Apr 1, 2019 at 5:46am (UTC) |</span><br><span class="line">+--------------------+--------------+--------+-------------+--------+----------+-----------------------------+</span><br><span class="line"></span><br><span class="line">$ lxc launch custom-for-centos7 centos-7-0</span><br><span class="line"></span><br><span class="line"><span class="comment">### You need set it to 0 before you launch the custom image</span></span><br><span class="line">$ lxc profile device <span class="built_in">set</span> default root size 0</span><br><span class="line">$ lxc launch custom-for-centos7 centos-7-0</span><br><span class="line">Creating centos-7-71</span><br><span class="line">Starting centos-7-71</span><br></pre></td></tr></table></figure>

<h3 id="Connect-bash-tty"><a href="#Connect-bash-tty" class="headerlink" title="Connect bash tty"></a>Connect bash tty</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lxc exec centos-7-0 &#x2F;bin&#x2F;bash</span><br><span class="line">$ yum install openssh-server</span><br></pre></td></tr></table></figure>

<h3 id="Limit-the-container-resources"><a href="#Limit-the-container-resources" class="headerlink" title="Limit the container resources"></a>Limit the container resources</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lxc config set centos-7-0 limits.cpu 4</span><br><span class="line">$ lxc config set centos-7-0 limits.cpu 0,3,7-9</span><br><span class="line">$ lxc config set centos-7-0 limits.cpu.allowance 20%</span><br><span class="line">$ lxc config set centos-7-0 limits.cpu.allowance 100ms&#x2F;200ms</span><br><span class="line">$ lxc config set centos-7-0 limits.cpu.priority 5</span><br><span class="line">$ lxc config set centos-7-0 limits.memory 8GB</span><br><span class="line">$ lxc config set centos-7-0 limits.memory.swap true</span><br><span class="line">$ lxc config device set centos-7-0 root limits.read 125MB</span><br><span class="line">$ lxc config device set centos-7-0 root limits.write 125MB</span><br></pre></td></tr></table></figure>

<h4 id="Show-the-expanded-config"><a href="#Show-the-expanded-config" class="headerlink" title="Show the expanded config"></a>Show the expanded config</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lxc config show --expanded centos-7-0</span><br></pre></td></tr></table></figure>

<h4 id="Limit-root-partition"><a href="#Limit-root-partition" class="headerlink" title="Limit root partition"></a>Limit root partition</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lxc config device add centos-7-0 root disk path&#x3D;&#x2F; pool&#x3D;zfs-pool size&#x3D;200GB</span><br><span class="line">## for all root partition</span><br><span class="line">$ lxc profile device set default root size 200GB</span><br></pre></td></tr></table></figure>

<h4 id="List-contanier-device"><a href="#List-contanier-device" class="headerlink" title="List contanier device"></a>List contanier device</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lxc profile device list default</span><br><span class="line">eth0</span><br><span class="line">root</span><br></pre></td></tr></table></figure>

<h4 id="Clone-container-to-image"><a href="#Clone-container-to-image" class="headerlink" title="Clone container to image"></a>Clone container to image</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lxc stop centos-7-0</span><br><span class="line">$ lxc-stop -n centos-7-0 -k <span class="comment">## force</span></span><br><span class="line">$ lxc publish centos-7-0 --<span class="built_in">alias</span>=centos-7.6-1 description=<span class="string">"Test base image update1"</span></span><br><span class="line">$ lxc image list</span><br><span class="line">+--------------------+--------------+--------+-------------------------+--------+----------+-----------------------------+</span><br><span class="line">|       ALIAS        | FINGERPRINT  | PUBLIC |       DESCRIPTION       |  ARCH  |   SIZE   |         UPLOAD DATE         |</span><br><span class="line">+--------------------+--------------+--------+-------------------------+--------+----------+-----------------------------+</span><br><span class="line">| centos7.6-1        | 1741cafe0ae2 | no     | Test base image update1 | x86_64 | 213.75MB | Apr 1, 2019 at 7:09am (UTC) |</span><br><span class="line">+--------------------+--------------+--------+-------------------------+--------+----------+-----------------------------+</span><br><span class="line">| custom-for-centos7 | 662e0004303e | no     |                         | x86_64 | 174.80MB | Apr 1, 2019 at 5:46am (UTC) |</span><br><span class="line">+--------------------+--------------+--------+-------------------------+--------+----------+-----------------------------+</span><br><span class="line"></span><br><span class="line">$ lxc launch centos7.6-1 centos-7-2</span><br><span class="line">$ lxc delete centos-7-0</span><br><span class="line">$ lxc info centos-7-2</span><br><span class="line"></span><br><span class="line"><span class="comment">#Show internet image</span></span><br><span class="line">lxc image list images:</span><br></pre></td></tr></table></figure>

<h4 id="Show-container-info"><a href="#Show-container-info" class="headerlink" title="Show container info"></a>Show container info</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lxc list</span><br></pre></td></tr></table></figure>

<h3 id="Container-config-file"><a href="#Container-config-file" class="headerlink" title="Container config file"></a>Container config file</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;var&#x2F;lib&#x2F;lxd&#x2F;storage-pools&#x2F;zfs-pool&#x2F;containers&#x2F;centos-7-0</span><br></pre></td></tr></table></figure>

<h3 id="Enable-the-container-support-NFS"><a href="#Enable-the-container-support-NFS" class="headerlink" title="Enable the container support NFS"></a>Enable the container support NFS</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lxc config <span class="built_in">set</span> centos-7-71 security.privileged <span class="literal">true</span></span><br><span class="line"><span class="built_in">printf</span> <span class="string">"mount fstype=nfs,\nmount fstype=nfs4,"</span> | lxc config <span class="built_in">set</span> centos-7-71 raw.apparmor -</span><br></pre></td></tr></table></figure>

<h3 id="Create-a-new-profile-and-assign-the-config-to-the-container"><a href="#Create-a-new-profile-and-assign-the-config-to-the-container" class="headerlink" title="Create a new profile and assign the config to the container"></a>Create a new profile and assign the config to the container</h3><p>``<br>$ lxc profile show ipmi-manager<br>config: {}<br>description: ipmitool manager node<br>devices:<br>  eth0:<br>    name: eth0<br>    nictype: bridged<br>    parent: br1<br>    type: nic<br>  eth1:<br>    name: eth1<br>    nictype: bridged<br>    parent: br2<br>    type: nic<br>  root:<br>    path: /<br>    pool: zfs-pool<br>    size: “0”<br>    type: disk<br>name: ipmi-manager</p>
<p>$ lxc profile remove c1 default<br>$ lxc profile assign c1 ipmi-manager<br>$ lxc exec c1 /bin/bash<br>$ ls /sys/class/net<br>eth0  eth1  lo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### Auto start</span><br></pre></td></tr></table></figure>
<p>lxc config set $container boot.autostart true<br>lxc config set $container boot.autostart.delay 10</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### mount iso</span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line"># in physical server</span><br><span class="line">$ lxc config device add centos7-samba-105 loop0 unix-block path&#x3D;&#x2F;dev&#x2F;loop0</span><br><span class="line"></span><br><span class="line"># in container</span><br><span class="line">$ mknod &#x2F;dev&#x2F;loop0 b 7 0</span><br></pre></td></tr></table></figure>

<h3 id="Mount-host-lustre"><a href="#Mount-host-lustre" class="headerlink" title="Mount host lustre"></a><a href="https://www.cyberciti.biz/faq/how-to-add-or-mount-directory-in-lxd-linux-container/?nsukey=eiv2%2Fuile9LfG55dOvcg55UtqOWQ19bxIsDY%2BcQHH0EMOu865QP86rrt%2F4aONbgQSMEg8ZarwNEM4Co5kKScLxO3eE3FlG3d9V9iWG2UTcog%2B3x2ZWXy5itNokLMg8%2F1KVsyd05MctDSgYCklGPMNeT%2BmuNtI5ICfGvH%2BnheN7AIRFdmDWdo6CI0%2F00wWjDhsgsSVRZx2qEfz1UKB1kkOA%3D%3D" target="_blank" rel="noopener">Mount host lustre</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount.lustre xx.xx.xx.xx@tcp:/lustre /mnt -o localflock,user_xattr</span><br><span class="line">$ df -h <span class="comment"># make sure has mountted</span></span><br><span class="line">$ lxc list</span><br><span class="line">$ lxc config device add c1 mytestdir disk <span class="built_in">source</span>=/mnt path=/dest/</span><br><span class="line">$ lxc config device show c1</span><br><span class="line">$ lxc <span class="built_in">exec</span> c1 bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># in container</span></span><br><span class="line">$ df -h</span><br><span class="line">xx.xx.xx.xx@tcp:/lustre              lustre    10T  491M  10T   1% /dest</span><br><span class="line">$ umount /dest</span><br><span class="line"></span><br><span class="line"><span class="comment"># in host</span></span><br><span class="line">xx.xx.xx.xx@tcp:/lustre                        10T  491M  10T   1% /var/lib/lxd/devices/c1/disk.mytestdir.dest-</span><br><span class="line"></span><br><span class="line">$ lxc config device remove c1 mytestdir</span><br><span class="line"></span><br><span class="line">$ cat /etc/subgid /etc/subuid</span><br><span class="line"><span class="comment">#login name or UID</span></span><br><span class="line"><span class="comment">#numerical subordinate group ID</span></span><br><span class="line"><span class="comment">#numerical subordinate group ID count</span></span><br><span class="line"><span class="comment">#modify root to nobody(65534)</span></span><br><span class="line">root:100000:65534</span><br><span class="line">root:100000:65534</span><br><span class="line"></span><br><span class="line"><span class="comment">## The container root and ubuntu physical root has the same permission</span></span><br><span class="line"><span class="comment">## Lustre has the same setting with the permission</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ubuntu lustre client version</span></span><br><span class="line">$ cat /sys/fs/lustre/version</span><br><span class="line">2.12.55_dirty</span><br></pre></td></tr></table></figure>

<p><a href="https://gist.github.com/jeanlouisferey/15be1f421eb9f9a66f1c74d410de2675" target="_blank" rel="noopener">https://gist.github.com/jeanlouisferey/15be1f421eb9f9a66f1c74d410de2675</a><br><a href="https://angristan.xyz/lxc-zfs-pool-lxd/" target="_blank" rel="noopener">https://angristan.xyz/lxc-zfs-pool-lxd/</a><br><a href="https://blog.simos.info/how-to-make-your-lxd-containers-get-ip-addresses-from-your-lan-using-a-bridge/" target="_blank" rel="noopener">https://blog.simos.info/how-to-make-your-lxd-containers-get-ip-addresses-from-your-lan-using-a-bridge/</a><br><a href="https://www.tomvanbrienen.nl/installing-and-configuring-lxd-3-on-ubuntu-18-04/" target="_blank" rel="noopener">https://www.tomvanbrienen.nl/installing-and-configuring-lxd-3-on-ubuntu-18-04/</a><br><a href="https://tutorials.ubuntu.com/tutorial/create-custom-lxd-images#4" target="_blank" rel="noopener">https://tutorials.ubuntu.com/tutorial/create-custom-lxd-images#4</a><br><a href="https://github.com/lxc/lxd/issues/3096" target="_blank" rel="noopener">https://github.com/lxc/lxd/issues/3096</a><br><a href="https://cwiki.apache.org/confluence/display/CLOUDSTACK/LXC+Template+creation#LXCTemplatecreation-StepstocreateminimalCentOS6containertemplate" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/CLOUDSTACK/LXC+Template+creation#LXCTemplatecreation-StepstocreateminimalCentOS6containertemplate</a>:<br><a href="https://blog.simos.info/using-distrobuilder-to-create-container-images-for-lxc-and-lxd/" target="_blank" rel="noopener">https://blog.simos.info/using-distrobuilder-to-create-container-images-for-lxc-and-lxd/</a></p>
]]></content>
      <categories>
        <category>container</category>
      </categories>
      <tags>
        <tag>zfs</tag>
        <tag>lxc</tag>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>the HDD power mode</title>
    <url>/2019/01/03/hdd-power-mode/</url>
    <content><![CDATA[<h3 id="About-HDD-power-save-mode"><a href="#About-HDD-power-save-mode" class="headerlink" title="About HDD power save mode"></a><a href="https://www.seagate.com/files/docs/pdf/whitepaper/tp608-powerchoice-tech-provides-us.pdf" target="_blank" rel="noopener">About HDD power save mode</a></h3><p>Idle_0</p>
<ul>
<li>Drive Ready, but not performing IO, drive may power down selected electronics to reduce power without increasing response time</li>
</ul>
<p>Idle_A</p>
<ul>
<li>Disables most of the servo system, reduces processor and channel power consumption</li>
<li>Disks rotating at full speed (7200 RPM)</li>
</ul>
<p>Idle_B</p>
<ul>
<li>Disables most of the servo system, reduces processor and channel power consumption</li>
<li>Heads are unloaded to drive ramp.</li>
<li>Disks rotating at full speed (7200 RPM)</li>
</ul>
<p>Idle_C/Standby_Y (SAS Only)</p>
<ul>
<li>Disables most of the servo system, reduces processor and channel power consumption</li>
<li>Heads are unloaded to drive ramp.</li>
<li>Drive speed reduced to a lower RPM (reduced RPM)</li>
</ul>
<p>Standby_Z</p>
<ul>
<li>Heads are unloaded to drive ramp.</li>
<li>Drive motor is spun down.</li>
<li>Drive still responds to non-media access host commands.</li>
</ul>
<a id="more"></a>

<p>SAS</p>
<ul>
<li>Host-definable timers via mode pages</li>
<li>Immediate host-commanded power transitions via Start/Stop Unit (SSU) command</li>
</ul>
<table>
<thead>
<tr>
<th align="center">From</th>
<th align="center">To</th>
<th align="center">RPM</th>
<th align="center">Typical(sec)</th>
<th align="center">Max(sec)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">idle_b</td>
<td align="center">Active</td>
<td align="center">7200</td>
<td align="center">1</td>
<td align="center">30</td>
</tr>
<tr>
<td align="center">idle_c</td>
<td align="center">Active</td>
<td align="center">6300 -&gt;7200</td>
<td align="center">4</td>
<td align="center">30</td>
</tr>
<tr>
<td align="center">standby_y</td>
<td align="center">Active</td>
<td align="center">6300 -&gt;7200</td>
<td align="center">4</td>
<td align="center">30</td>
</tr>
<tr>
<td align="center">standby_z</td>
<td align="center">Active</td>
<td align="center">0 -&gt; 7200</td>
<td align="center">15</td>
<td align="center">30</td>
</tr>
</tbody></table>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_logs -p 0x1a /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Power condition transitions page  [0x1a]</span><br><span class="line">  Accumulated transitions to active = 700283</span><br><span class="line">  Accumulated transitions to idle_a = 698441</span><br><span class="line">  Accumulated transitions to idle_b = 1855</span><br><span class="line">  Accumulated transitions to idle_c = 0</span><br><span class="line">  Accumulated transitions to standby_z = 0</span><br><span class="line">  Accumulated transitions to standby_y = 0</span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x1a /dev/sdj</span><br><span class="line">    SEAGATE   ST10000NM0256     TT54</span><br><span class="line">Power condition transitions page  (spc-4) [0x1a]</span><br><span class="line">  Accumulated transitions to idle_a = 13934971</span><br><span class="line">  Accumulated transitions to idle_b = 13934971</span><br><span class="line">  Accumulated transitions to idle_c = 99</span><br><span class="line">  Reserved [0x4] = 0</span><br><span class="line">  Accumulated transitions to standby_z = 0</span><br><span class="line">  Accumulated transitions to standby_y = 0</span><br><span class="line"></span><br><span class="line">$ sdparm -p po -l  /dev/sdj</span><br><span class="line">    /dev/sdj: HGST      HUH721010AL5200   LS14</span><br><span class="line">    Direct access device specific parameters: WP=0  DPOFUA=1</span><br><span class="line">Power condition [po] mode page:</span><br><span class="line">  PM_BG         0  [cha: n, def:  0, sav:  0]  Power management, background <span class="built_in">functions</span>, precedence</span><br><span class="line">  STANDBY_Y     0  [cha: n, def:  0, sav:  0]  Standby_y timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_C        0  [cha: y, def:  0, sav:  0]  Idle_c timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_B        1  [cha: y, def:  1, sav:  1]  Idle_b timer <span class="built_in">enable</span></span><br><span class="line">  IDLE          1  [cha: y, def:  1, sav:  1]  Idle_a timer <span class="built_in">enable</span></span><br><span class="line">  STANDBY       0  [cha: y, def:  0, sav:  0]  Standby_z timer <span class="built_in">enable</span></span><br><span class="line">  ICT           20  [cha: n, def: 20, sav: 20]  Idle_a condition timer (100 ms)</span><br><span class="line">  SCT           0  [cha: y, def:  0, sav:  0]  Standby_z condition timer (100 ms)</span><br><span class="line">  IBCT          6000  [cha: n, def:6000, sav:6000]  Idle_b condition timer (100 ms)</span><br><span class="line">  ICCT          9000  [cha: n, def:9000, sav:9000]  Idle_c condition timer (100 ms)</span><br><span class="line">  SYCT          0  [cha: n, def:  0, sav:  0]  Standby_y condition timer (100 ms)</span><br><span class="line">  CCF_IDLE      1  [cha: y, def:  1, sav:  1]  check condition on transition from idle</span><br><span class="line">  CCF_STAND     2  [cha: y, def:  2, sav:  2]  check condition on transition from standby</span><br><span class="line">  CCF_STOPP     2  [cha: y, def:  2, sav:  2]  check condition on transition from stopped</span><br><span class="line"></span><br><span class="line"><span class="comment"># looks like cha: n means could not be modified</span></span><br><span class="line"></span><br><span class="line">$ sdparm -p po -l  /dev/sdj</span><br><span class="line">    /dev/sdj: HGST      HUH721010AL5200   LS14</span><br><span class="line">    Direct access device specific parameters: WP=0  DPOFUA=1</span><br><span class="line">Power condition [po] mode page:</span><br><span class="line">  PM_BG         0  [cha: n, def:  0, sav:  0]  Power management, background <span class="built_in">functions</span>, precedence</span><br><span class="line">  STANDBY_Y     0  [cha: n, def:  0, sav:  0]  Standby_y timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_C        0  [cha: y, def:  0, sav:  0]  Idle_c timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_B        1  [cha: y, def:  1, sav:  1]  Idle_b timer <span class="built_in">enable</span></span><br><span class="line">  IDLE          0  [cha: y, def:  1, sav:  0]  Idle_a timer <span class="built_in">enable</span></span><br><span class="line">  STANDBY       0  [cha: y, def:  0, sav:  0]  Standby_z timer <span class="built_in">enable</span></span><br><span class="line">  ICT           20  [cha: n, def: 20, sav: 20]  Idle_a condition timer (100 ms)</span><br><span class="line">  SCT           0  [cha: y, def:  0, sav:  0]  Standby_z condition timer (100 ms)</span><br><span class="line">  IBCT          6000  [cha: n, def:6000, sav:6000]  Idle_b condition timer (100 ms)</span><br><span class="line">  ICCT          9000  [cha: n, def:9000, sav:9000]  Idle_c condition timer (100 ms)</span><br><span class="line">  SYCT          0  [cha: n, def:  0, sav:  0]  Standby_y condition timer (100 ms)</span><br><span class="line">  CCF_IDLE      1  [cha: y, def:  1, sav:  1]  check condition on transition from idle</span><br><span class="line">  CCF_STAND     2  [cha: y, def:  2, sav:  2]  check condition on transition from standby</span><br><span class="line">  CCF_STOPP     2  [cha: y, def:  2, sav:  2]  check condition on transition from stopped</span><br><span class="line"></span><br><span class="line">$ sdparm -v -S -p po --<span class="built_in">set</span>=IDLE=1 /dev/sdj</span><br><span class="line">mp_settings: page,subpage=0x1a,0x0  num=1</span><br><span class="line">  pdt=-1 start_byte=0x3 start_bit=1 num_bits=1  val=1  acronym: IDLE</span><br><span class="line">    /dev/sdj: HGST      HUH721010AL5200   LS14</span><br><span class="line">    mode sense (10) cdb: 5a 00 1a 00 00 00 00 00 04 00</span><br><span class="line">    mode sense (10) cdb: 5a 00 1a 00 00 00 00 00 38 00</span><br><span class="line">    mode select (10) cdb: 55 11 00 00 00 00 00 00 38 00</span><br><span class="line"></span><br><span class="line">$ sdparm -p po -l  /dev/sdj</span><br><span class="line">    /dev/sdj: HGST      HUH721010AL5200   LS14</span><br><span class="line">    Direct access device specific parameters: WP=0  DPOFUA=1</span><br><span class="line">Power condition [po] mode page:</span><br><span class="line">  PM_BG         0  [cha: n, def:  0, sav:  0]  Power management, background <span class="built_in">functions</span>, precedence</span><br><span class="line">  STANDBY_Y     0  [cha: n, def:  0, sav:  0]  Standby_y timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_C        0  [cha: y, def:  0, sav:  0]  Idle_c timer <span class="built_in">enable</span></span><br><span class="line">  IDLE_B        1  [cha: y, def:  1, sav:  1]  Idle_b timer <span class="built_in">enable</span></span><br><span class="line">  IDLE          1  [cha: y, def:  1, sav:  1]  Idle_a timer <span class="built_in">enable</span></span><br><span class="line">  STANDBY       0  [cha: y, def:  0, sav:  0]  Standby_z timer <span class="built_in">enable</span></span><br><span class="line">  ICT           20  [cha: n, def: 20, sav: 20]  Idle_a condition timer (100 ms)</span><br><span class="line">  SCT           0  [cha: y, def:  0, sav:  0]  Standby_z condition timer (100 ms)</span><br><span class="line">  IBCT          6000  [cha: n, def:6000, sav:6000]  Idle_b condition timer (100 ms)</span><br><span class="line">  ICCT          9000  [cha: n, def:9000, sav:9000]  Idle_c condition timer (100 ms)</span><br><span class="line">  SYCT          0  [cha: n, def:  0, sav:  0]  Standby_y condition timer (100 ms)</span><br><span class="line">  CCF_IDLE      1  [cha: y, def:  1, sav:  1]  check condition on transition from idle</span><br><span class="line">  CCF_STAND     2  [cha: y, def:  2, sav:  2]  check condition on transition from standby</span><br><span class="line">  CCF_STOPP     2  [cha: y, def:  2, sav:  2]  check condition on transition from stopped</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable IDLE_C</span></span><br><span class="line">$ sdparm --flexible -6  -v -p po --<span class="built_in">set</span>=IDLE_C=0 /dev/sdj</span><br><span class="line"></span><br><span class="line"><span class="comment"># Got all page name </span></span><br><span class="line">$ sdparm -e /dev/sdj</span><br></pre></td></tr></table></figure>

<p><a href="https://wiki.archlinux.org/index.php/hdparm#Power_management_configuration" target="_blank" rel="noopener">SATA HDD</a></p>
<ul>
<li>Host-definable timers via Set Features commands</li>
<li>Immediate host-commanded power transitions via Set Features commands </li>
</ul>
<p>-B Set the Advanced Power Management feature. Possible values are between 1 and 255, low values mean more aggressive power management and higher values mean better performance. Values from 1 to 127 permit spin-down, whereas values from 128 to 254 do not. A value of 255 completely disables the feature</p>
<p>-S Set the standby (spindown) timeout for the drive. The timeout specifies how long to wait in idle (with no disk activity) before turning off the motor to save power. The value of 0 disables spindown, the values from 1 to 240 specify multiples of 5 seconds and values from 241 to 251 specify multiples of 30 minutes. </p>
<p>-M Set the Automatic Acoustic Management feature. Most modern hard disk drives have the ability to speed down the head movements to reduce their noise output. The possible value depends on the disk, some disks may not support this feature. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hdparm -B 127 /dev/sdb</span><br><span class="line"></span><br><span class="line">/dev/sdb:</span><br><span class="line"> setting Advanced Power Management level to 0x7f (127)</span><br><span class="line"> APM_level	= 127</span><br><span class="line"></span><br><span class="line">$ hdparm -B 255 /dev/sdb</span><br><span class="line"></span><br><span class="line">/dev/sdb:</span><br><span class="line"> setting Advanced Power Management level to disabled</span><br><span class="line"> APM_level	= off</span><br><span class="line"></span><br><span class="line">$ hdparm -S 0 /dev/sdb</span><br><span class="line"></span><br><span class="line">/dev/sdb:</span><br><span class="line"> setting standby to 0 (off)</span><br><span class="line"></span><br><span class="line">$ hdparm -M /dev/sdb</span><br><span class="line"></span><br><span class="line">/dev/sdb:</span><br><span class="line"> acoustic      = not supported</span><br></pre></td></tr></table></figure>

<h3 id="smart"><a href="#smart" class="headerlink" title="smart"></a>smart</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">smartctl -i -n standby /dev/sdc</span><br><span class="line"></span><br><span class="line"><span class="comment">#never - check the device always, but print the power mode if ´-i´ is specified</span></span><br><span class="line"><span class="comment">#sleep - check the device unless it is in SLEEP mode</span></span><br><span class="line"><span class="comment">#standby - check the device unless it is in SLEEP or STANDBY mode.  In these modes most disks are not spinning, so if you want to prevent a disk from spinning up, this is probably what you want</span></span><br><span class="line"><span class="comment">#idle - check the device unless it is in SLEEP, STANDBY or IDLE mode.  In the IDLE state,  most disks are still spinning, so this is probably not what you want</span></span><br></pre></td></tr></table></figure>

<h3 id="udev"><a href="#udev" class="headerlink" title="udev"></a>udev</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/etc/udev/rules.d/69-hdparm.rules</span><br><span class="line">ACTION==<span class="string">"add"</span>, SUBSYSTEM==<span class="string">"block"</span>, KERNEL==<span class="string">"sda"</span>, RUN+=<span class="string">"/usr/bin/hdparm -B 254 -S 0 /dev/sda"</span></span><br></pre></td></tr></table></figure>

<h3 id="systemd"><a href="#systemd" class="headerlink" title="systemd"></a>systemd</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /etc/systemd/system/hdparm.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=hdparm sleep</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=oneshot</span><br><span class="line">ExecStart=/usr/bin/hdparm -q -S 120 -y /dev/sdb</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>scsi</category>
      </categories>
      <tags>
        <tag>block</tag>
      </tags>
  </entry>
  <entry>
    <title>multipath</title>
    <url>/2018/12/11/multipath/</url>
    <content><![CDATA[<p><a href="https://access.redhat.com/solutions/137073" target="_blank" rel="noopener">multipath is not detecting path failures fast enough which results in application failure and system reboots</a><br>There are multiple parameters that affect error detection and failover times:<br>dev_loss_tmo<br>fast_io_fail_tmo<br>checker_timeout</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#default setting</span><br><span class="line">defaults &#123;</span><br><span class="line">        user_friendly_names yes</span><br><span class="line">        fast_io_fail_tmo 5</span><br><span class="line">        dev_loss_tmo 10</span><br><span class="line">        no_path_retry fail</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>The dev_loss_tmo (rport) affects extended link timeout, <code>in-flight I/O is held after a link-down event for seconds before the driver gives up waiting for the port to come back. Default is 30-35s, so in-flight I/O can be held seconds before being killed off. After timeout expiration, rport is put in offline (down) state.</code></p>
<p>The fast_io_fail_tmo (rport) affects how long io is queued and held while rport is in blocked state.</p>
<p>The checker_timeout specify the timeout to user for path checkers that issue SCSI commands with an explicit timeout, in seconds; default is taken from /sys/block/sd<x>/device/timeout.</p>
<p>The polling_interval is the interval between path checks in seconds.</p>
<p>no_path_retry:<br>Specify  the  number of retries until disable queueing, or fail for immediate failure (no queueing),  queue for never stop queueing. Default config entry is fail.</p>
<p>flush_on_last_del:<br>If set to yes , multipathd will disable queueing when the last path to a device has been deleted. Default is no.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ modinfo ib_srp</span><br><span class="line">InfiniBand SCSI RDMA Protocol initiator</span><br><span class="line">$ cat &#x2F;sys&#x2F;module&#x2F;ib_srp&#x2F;parameters&#x2F;dev_loss_tmo</span><br><span class="line">600</span><br><span class="line">$ cat &#x2F;sys&#x2F;module&#x2F;ib_srp&#x2F;parameters&#x2F;fast_io_fail_tmo</span><br><span class="line">15</span><br></pre></td></tr></table></figure>
<p>dev_loss_tmo:<br>Maximum number of seconds that the SRP transport should insulate transport layer errors. After this time has been exceeded the SCSI host is removed. Should be between 1 and SCSI_DEVICE_BLOCK_MAX_TIMEOUT if fast_io_fail_tmo has not been set. “off” means that this functionality is disabled.<br>fast_io_fail_tmo:<br>Number of seconds between the observation of a transport layer error and failing all I/O. “off” means that this functionality is disabled.</p>
<p>redhat FC example</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># for f in /sys/class/fc_remote_ports/rport-*/dev_loss_tmo; do d=$(dirname $f); echo $(basename $d):$(cat $d/node_name):$(cat $f); done</span></span><br><span class="line">rport-8:0-0:0x50014380113622b0:10</span><br><span class="line">rport-8:0-1:0x50014380113622b0:10</span><br><span class="line">rport-8:0-2:0x50014380113622b0:10</span><br><span class="line">rport-8:0-3:0x50014380113622b0:10</span><br></pre></td></tr></table></figure>
<p>All setting base your hardware and software efficient</p>
<h3 id="switch-multipath"><a href="#switch-multipath" class="headerlink" title="switch multipath"></a>switch multipath</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ multipathd -k </span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">$ multipathd show map 3600a098000aaaaaa0000012345dc topology</span><br><span class="line">3600a098000aaaaaa0000012345dc dm-2 DELL,MD34xx</span><br><span class="line">size=5.8T features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'0'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'round-robin 0'</span> prio=14 status=active</span><br><span class="line">| `- 2:0:0:0 sdc 8:32 active ready  running</span><br><span class="line">`-+- policy=<span class="string">'round-robin 0'</span> prio=9 status=enabled</span><br><span class="line">  `- 1:0:0:0 sdb 8:16 active ready  running</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here only single mutibus , let 's look the group_by_prio</span></span><br><span class="line">$ multipath -ll</span><br><span class="line">3600a098000aaaaaa0000012345dc dm-2 DELL,MD34xx</span><br><span class="line">size=5.8T features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'0'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'round-robin 0'</span> prio=14 status=active</span><br><span class="line">| `- 2:0:0:0 sdc 8:32 active ready  running</span><br><span class="line">`-+- policy=<span class="string">'round-robin 0'</span> prio=9 status=enabled</span><br><span class="line">  `- 1:0:0:0 sdb 8:16 active ready  running</span><br><span class="line"></span><br><span class="line">$ multipathd -k </span><br><span class="line"></span><br><span class="line">multipathd&gt; switch map 3600a098000aaaaaa0000012345dc group 2</span><br><span class="line">ok</span><br><span class="line">multipathd&gt; show topology</span><br><span class="line">3600a098000aaaaaa0000012345dc dm-2 DELL,MD34xx</span><br><span class="line">size=5.8T features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'0'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'round-robin 0'</span> prio=14 status=enabled</span><br><span class="line">| `- 2:0:0:0 sdc 8:32 active ready running</span><br><span class="line">`-+- policy=<span class="string">'round-robin 0'</span> prio=9 status=active</span><br><span class="line">  `- 1:0:0:0 sdb 8:16 active ready running</span><br><span class="line">multipathd&gt; switch map 3600a098000aaaaaa0000012345dc group 1</span><br><span class="line">ok</span><br><span class="line">multipathd&gt; show topology</span><br><span class="line">3600a098000aaaaaa0000012345dc dm-2 DELL,MD34xx</span><br><span class="line">size=5.8T features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'0'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'round-robin 0'</span> prio=14 status=active</span><br><span class="line">| `- 2:0:0:0 sdc 8:32 active ready running</span><br><span class="line">`-+- policy=<span class="string">'round-robin 0'</span> prio=9 status=enabled</span><br><span class="line">  `- 1:0:0:0 sdb 8:16 active ready running</span><br><span class="line"></span><br><span class="line"><span class="comment"># set the path fail</span></span><br><span class="line">multipathd&gt; fail path sdc</span><br><span class="line"><span class="comment"># reinstate the path</span></span><br><span class="line">multipathd&gt; reinstate path sdc</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>device_mapper</category>
      </categories>
      <tags>
        <tag>multipath</tag>
        <tag>dm</tag>
      </tags>
  </entry>
  <entry>
    <title>ext4 info</title>
    <url>/2018/11/13/ext4/</url>
    <content><![CDATA[<p>Oh, shit , data loss, here is lustre ldiskfs</p>
<p>backup data<br>When backing up operating system partitions, the partition must be unmounted.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/mapper/3600a098000b5ea3e000006065be52460 of=3600a098000b5ea3e000006065be52460 bs=1M conv=sparse</span><br><span class="line">or you could </span><br><span class="line">$ dump -0uf /backup-files/sda3.dump /dev/sda3</span><br><span class="line">$ dump -0u -f - /dev/sda1 | ssh root@<span class="built_in">test</span>-server dd of=/tmp/sda1.dump</span><br><span class="line"><span class="comment"># recovery </span></span><br><span class="line">$ mount -t ext3 /dev/sda3 /mnt/sda3</span><br><span class="line">$ <span class="built_in">cd</span> /mnt/sda3</span><br><span class="line">restore -rf /backup-files/sda3.dump</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ debugfs &#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006065be52460</span><br><span class="line">debugfs 1.42.13.wc5 (15-Apr-2016)</span><br><span class="line">&#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006065be52460: Can&#39;t read an inode bitmap while reading inode bitmap</span><br><span class="line">debugfs:  stats</span><br><span class="line">stats: Filesystem not open</span><br><span class="line"></span><br><span class="line">$ dumpe2fs &#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006065be52460</span><br><span class="line">dumpe2fs 1.42.13.wc5 (15-Apr-2016)</span><br><span class="line">Filesystem volume name:   ldfsqd1-OST0004</span><br><span class="line">Last mounted on:          &#x2F;</span><br><span class="line">Filesystem UUID:          8ffd7826-f9f5-4e92-99ef-75cca46c281a</span><br><span class="line">Filesystem magic number:  0xEF53</span><br><span class="line">Filesystem revision #:    1 (dynamic)</span><br><span class="line">Filesystem features:      has_journal ext_attr dir_index filetype needs_recovery extent 64bit mmp flex_bg sparse_super large_file huge_file uninit_bg dir_nlink quota</span><br><span class="line">Filesystem flags:         signed_directory_hash</span><br><span class="line">Default mount options:    user_xattr acl</span><br><span class="line">Filesystem state:         clean</span><br><span class="line">Errors behavior:          Continue</span><br><span class="line">Filesystem OS type:       Linux</span><br><span class="line">Inode count:              62914560</span><br><span class="line">Block count:              16106127360</span><br><span class="line">Reserved block count:     805306368</span><br><span class="line">Free blocks:              3782402091</span><br><span class="line">Free inodes:              60311208</span><br><span class="line">First block:              0</span><br><span class="line">Block size:               4096</span><br><span class="line">Fragment size:            4096</span><br><span class="line">Group descriptor size:    64</span><br><span class="line">Blocks per group:         32768</span><br><span class="line">Fragments per group:      32768</span><br><span class="line">Inodes per group:         128</span><br><span class="line">Inode blocks per group:   8</span><br><span class="line">Flex block group size:    256</span><br><span class="line">Filesystem created:       Tue Oct 10 13:54:45 2017</span><br><span class="line">Last mount time:          Sat Oct 20 16:15:57 2018</span><br><span class="line">Last write time:          Sat Oct 20 16:15:57 2018</span><br><span class="line">Mount count:              8</span><br><span class="line">Maximum mount count:      -1</span><br><span class="line">Last checked:             Tue Oct 10 13:54:45 2017</span><br><span class="line">Check interval:           0 (&lt;none&gt;)</span><br><span class="line">Lifetime writes:          5132 GB</span><br><span class="line">Reserved blocks uid:      0 (user root)</span><br><span class="line">Reserved blocks gid:      0 (group root)</span><br><span class="line">First inode:              11</span><br><span class="line">Inode size:	          256</span><br><span class="line">Required extra isize:     28</span><br><span class="line">Desired extra isize:      28</span><br><span class="line">Journal inode:            8</span><br><span class="line">Default directory hash:   half_md4</span><br><span class="line">Directory Hash Seed:      7d46aee3-a481-456b-b9ff-6c7ab59a6e1c</span><br><span class="line">Journal backup:           inode blocks</span><br><span class="line">MMP block number:         10246</span><br><span class="line">MMP update interval:      5</span><br><span class="line">User quota inode:         3</span><br><span class="line">Group quota inode:        4</span><br><span class="line">Journal superblock magic number invalid!</span><br><span class="line">......</span><br><span class="line">$ grep  --binary-files&#x3D;text 0000430 3600a09800099437b000008f05be52536</span><br><span class="line">0000430: 9aba e45b 0c00 ffff 53ef 0100 0100 0000  ...[....S.......</span><br><span class="line">The magic number is little-endian 0xef53</span><br><span class="line"></span><br><span class="line">$ dumpe2fs &#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006055be52442 | grep superblock -i</span><br><span class="line">dumpe2fs 1.42.13.wc5 (15-Apr-2016)</span><br><span class="line">  Primary superblock at 0, Group descriptors at 1-7680</span><br><span class="line">  Backup superblock at 32768, Group descriptors at 32769-40448</span><br><span class="line">  Backup superblock at 98304, Group descriptors at 98305-105984</span><br><span class="line">  Backup superblock at 163840, Group descriptors at 163841-171520</span><br><span class="line">  Backup superblock at 229376, Group descriptors at 229377-237056</span><br><span class="line">  Backup superblock at 294912, Group descriptors at 294913-302592</span><br><span class="line">  Backup superblock at 819200, Group descriptors at 819201-826880</span><br><span class="line">  Backup superblock at 884736, Group descriptors at 884737-892416</span><br><span class="line">  Backup superblock at 1605632, Group descriptors at 1605633-1613312</span><br><span class="line">  Backup superblock at 2654208, Group descriptors at 2654209-2661888</span><br><span class="line">  Backup superblock at 4096000, Group descriptors at 4096001-4103680</span><br><span class="line">  Backup superblock at 7962624, Group descriptors at 7962625-7970304</span><br><span class="line">  Backup superblock at 11239424, Group descriptors at 11239425-11247104</span><br><span class="line">  Backup superblock at 20480000, Group descriptors at 20480001-20487680</span><br><span class="line">  Backup superblock at 23887872, Group descriptors at 23887873-23895552</span><br><span class="line">  Backup superblock at 71663616, Group descriptors at 71663617-71671296</span><br><span class="line">  Backup superblock at 78675968, Group descriptors at 78675969-78683648</span><br><span class="line">  Backup superblock at 102400000, Group descriptors at 102400001-102407680</span><br><span class="line">  Backup superblock at 214990848, Group descriptors at 214990849-214998528</span><br><span class="line">  Backup superblock at 512000000, Group descriptors at 512000001-512007680</span><br><span class="line">  Backup superblock at 550731776, Group descriptors at 550731777-550739456</span><br><span class="line">  Backup superblock at 644972544, Group descriptors at 644972545-644980224</span><br><span class="line">  Backup superblock at 1934917632, Group descriptors at 1934917633-1934925312</span><br><span class="line">  Backup superblock at 2560000000, Group descriptors at 2560000001-2560007680</span><br><span class="line">  Backup superblock at 3855122432, Group descriptors at 3855122433-3855130112</span><br><span class="line">  Backup superblock at 5804752896, Group descriptors at 5804752897-5804760576</span><br><span class="line">  Backup superblock at 12800000000, Group descriptors at 12800000001-12800007680</span><br></pre></td></tr></table></figure>
<p>Why some parition all superblock were missing ?</p>
<h3 id="For-huge-parttion-there-is-no-enough-memory-it-will-cause-slow"><a href="#For-huge-parttion-there-is-no-enough-memory-it-will-cause-slow" class="headerlink" title="For huge parttion, there is no enough memory, it will cause slow"></a>For huge parttion, there is no enough memory, it will cause slow</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;etc&#x2F;e2fsck.conf</span><br><span class="line">[scratch_files]</span><br><span class="line">directory &#x3D; &#x2F;var&#x2F;cache&#x2F;e2fsck</span><br></pre></td></tr></table></figure>

<h3 id="guess-partion-start"><a href="#guess-partion-start" class="headerlink" title="guess partion start"></a><a href="https://unix.stackexchange.com/questions/103919/how-do-i-find-the-offset-of-an-ext4-filesystem" target="_blank" rel="noopener">guess partion start</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bsz=512 <span class="comment"># or 1024, 2048, 4096 higher = faster</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;2..10000000&#125;; <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"---&gt;<span class="variable">$i</span>&lt;---"</span></span><br><span class="line">    mount -o offset=$((<span class="variable">$bsz</span>*<span class="variable">$i</span>)) -t ext4 /dev/whatever /mnt/foo</span><br><span class="line">    <span class="keyword">if</span> [ $? == 0 ]; <span class="keyword">then</span> <span class="comment"># whahoo!</span></span><br><span class="line">        <span class="built_in">echo</span> Eureka</span><br><span class="line">        <span class="built_in">break</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="Found-ext-magic-number"><a href="#Found-ext-magic-number" class="headerlink" title="Found ext magic number"></a>Found ext magic number</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">0000400: 0000 c003 0000 00c0 0000 0030 60c4 98a4  ...........0`...</span><br><span class="line">0000410: e243 9703 0000 0000 0200 0000 0200 0000  .C..............</span><br><span class="line">0000420: 0080 0000 0080 0000 8000 0000 9aba e45b  ...............[</span><br><span class="line">0000430: 9aba e45b 0c00 ffff 53ef 0100 0100 0000  ...[....S.......</span><br><span class="line">0000440: 8760 dc59 0000 0000 0000 0000 0100 0000  .`.Y............</span><br><span class="line">0000450: 0000 0000 0b00 0000 0001 0000 2c00 0000  ............,...</span><br><span class="line">0000460: c603 0000 3b01 0000 1c09 a61a 05f9 42cb  ....;.........B.</span><br><span class="line">0000470: a3a9 e929 ee10 20be 6c64 6673 7164 312d  ...).. .ldfsqd1-</span><br><span class="line">0000480: 4f53 5430 3030 3000 2f00 6d70 2f6d 6e74  OST0000./.mp/mnt</span><br><span class="line">0000490: 584e 4435 3733 0000 0000 0000 0000 0000  XND573..........</span><br><span class="line">00004a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00004b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00004c0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br></pre></td></tr></table></figure>

<h3 id="because-there-is-no-any-parttion-in-this-block-dev"><a href="#because-there-is-no-any-parttion-in-this-block-dev" class="headerlink" title="because there is no any parttion in this block dev"></a>because there is no any parttion in this block dev</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ parted /dev/mapper/3600a098000b5ea3e000006065be52460 p</span><br><span class="line">Model: Linux device-mapper (multipath) (dm)</span><br><span class="line">Disk /dev/mapper/3600a098000b5ea3e000006065be52460: 66.0TB</span><br><span class="line">Sector size (logical/physical): 512B/4096B</span><br><span class="line">Partition Table: loop <span class="comment">#######that means there is no parttion </span></span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start  End     Size    File system  Flags</span><br><span class="line"> 1      0.00B  66.0TB  66.0TB  ext4</span><br></pre></td></tr></table></figure>

<h3 id="You-can’t-fsck-by-default"><a href="#You-can’t-fsck-by-default" class="headerlink" title="You can’t fsck by default"></a>You can’t fsck by default</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ e2fsck -n -b 32768 &#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006065be52460</span><br><span class="line">e2fsck 1.42.13.wc5 (15-Apr-2016)</span><br><span class="line">MMP interval is 10 seconds and total wait time is 42 seconds. Please wait...</span><br><span class="line">Superblock has an invalid journal (inode 8).</span><br><span class="line">Clear? no</span><br><span class="line"></span><br><span class="line">e2fsck: Illegal inode number while checking ext3 journal for ldfsqd1-OST0004</span><br><span class="line"></span><br><span class="line">ldfsqd1-OST0004: ********** WARNING: Filesystem still has errors **********</span><br><span class="line"></span><br><span class="line">$ e2fsck -n -b 819200 &#x2F;dev&#x2F;mapper&#x2F;3600a098000b5ea3e000006065be52460</span><br><span class="line">e2fsck 1.42.13.wc5 (15-Apr-2016)</span><br><span class="line">MMP interval is 10 seconds and total wait time is 42 seconds. Please wait...</span><br><span class="line">Superblock has an invalid journal (inode 8).</span><br><span class="line">Clear? no</span><br><span class="line"></span><br><span class="line">e2fsck: Illegal inode number while checking ext3 journal for ldfsqd1-OST0004</span><br><span class="line"></span><br><span class="line">ldfsqd1-OST0004: ********** WARNING: Filesystem still has errors **********</span><br></pre></td></tr></table></figure>

<h3 id="Found-some-info-form-superblock"><a href="#Found-some-info-form-superblock" class="headerlink" title="Found some info form superblock"></a>Found some info form superblock</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-n     Causes mke2fs to not actually create a filesystem, but display what it would <span class="keyword">do</span> <span class="keyword">if</span> it were to create  a  filesystem.   </span><br><span class="line">       This can  be used to determine the location of the backup superblocks <span class="keyword">for</span> a particular filesystem, </span><br><span class="line">       so long as the mke2fs parame‐ters that were passed when the filesystem was originally created are used again.  </span><br><span class="line">       (With the -n option added, of course!)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Maybe you could rewrite the superblock</span></span><br><span class="line">mke2fs -S</span><br><span class="line">    -S    Write  superblock and group descriptors only.  This is useful <span class="keyword">if</span> all</span><br><span class="line">          of the superblock and backup superblocks are corrupted, and a  last-</span><br><span class="line">          ditch  recovery method is desired.  It causes mke2fs to reinitialize</span><br><span class="line">          the superblock and group descriptors, <span class="keyword">while</span> not touching  the  inode</span><br><span class="line">          table and the block and inode bitmaps.  The e2fsck program should be</span><br><span class="line">          run immediately after this option is used, and there is no guarantee</span><br><span class="line">          that  any  data  will be salvageable.  It is critical to specify the</span><br><span class="line">          correct filesystem blocksize when using this option, or there is  no</span><br><span class="line">          chance of recovery.</span><br></pre></td></tr></table></figure>

<h3 id="You-have-to-disable-ext-journal-make-it-like-ext2-all-blocks-full-scan"><a href="#You-have-to-disable-ext-journal-make-it-like-ext2-all-blocks-full-scan" class="headerlink" title="You have to disable ext journal, make it like ext2, all blocks full scan"></a>You have to disable ext journal, make it like ext2, all blocks full scan</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tune2fs -O ^has_journal,needs_rwcovery -f /dev/mapper/3600a098000b5ea3e000006065be52460</span><br><span class="line"><span class="comment">### you can fsck again, but it 's too slow..............maybe 60TB will wait 10 years</span></span><br><span class="line"></span><br><span class="line">$ e2fsck -y /dev/mapper/3600a098000b5ea3e000006065be52460</span><br><span class="line"><span class="comment">### it 's worked</span></span><br></pre></td></tr></table></figure>

<h3 id="Looks-like-the-block-dev-has-be-destroy-by-hardware-raid-controller-all-data-has-messed"><a href="#Looks-like-the-block-dev-has-be-destroy-by-hardware-raid-controller-all-data-has-messed" class="headerlink" title="Looks like the block dev has be destroy by hardware raid controller, all data has messed"></a>Looks like the block dev has be destroy by hardware raid controller, all data has messed</h3><p>In 60TiB raw dev, Lustre first 10MB is ext meta data and data map, it will not show any file part<br>But I found the horrible things, reserve space has been overrided as 128KiB, 128KiB is the raid controller segment size.<br>I don ‘t think we can recovery all data. when fsck finish, I ‘m not sure the data is it integrity. maybe some block has corruptted in single file</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">00fffa0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00fffb0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00fffc0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00fffd0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00fffe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">00ffff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">0100000: dcfa 60ef 039f c372 0793 3a8d 4c6a 642d  ..`....r..:.Ljd-</span><br><span class="line">0100010: 50c7 fddf b3f5 fbed f03f 71b5 ccb3 811c  P........?q.....</span><br><span class="line">0100020: 1495 a0c2 ab24 be48 ac46 1109 a19e aadc  .....$.H.F......</span><br><span class="line">0100030: 16d9 0488 2e26 d238 fe48 e492 27c5 dd02  .....&amp;.8.H..<span class="string">'...</span></span><br><span class="line"><span class="string">.....</span></span><br><span class="line"><span class="string">011ffd0: baf1 35e8 54e1 e97c dab5 e762 e74c 83cc  ..5.T..|...b.L..</span></span><br><span class="line"><span class="string">011ffe0: 7d02 7db3 53f2 613d 6ef4 775b a0e2 767f  &#125;.&#125;.S.a=n.w[..v.</span></span><br><span class="line"><span class="string">011fff0: ff8b 65fe 9c39 1018 3b76 586e 3893 2095  ..e..9..;vXn8. .</span></span><br><span class="line"><span class="string">0120000: c000 8023 c001 8023 0008 8023 0000 0100  ...#...#...#....</span></span><br><span class="line"><span class="string">0120010: 0000 0400 0000 0000 0000 0000 0000 3896  ..............8.</span></span><br><span class="line"><span class="string">0120020: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">##### After 128KiB</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">##### Another parttion</span></span><br><span class="line"><span class="string">01bffe0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span></span><br><span class="line"><span class="string">01bfff0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span></span><br><span class="line"><span class="string">01c0000: 9fac ef20 dd9d fd1c 18ed a503 a19e 5f7a  ... .........._z</span></span><br><span class="line"><span class="string">01c0010: ea7d b6c7 850d 7a52 a0a9 61a6 a912 f489  .&#125;....zR..a.....</span></span><br><span class="line"><span class="string">01c0020: da1f 4941 4fe4 f612 1b25 ff91 9b61 928a  ..IAO....%...a..</span></span><br><span class="line"><span class="string">01c0030: 405d 5026 e2a7 a59e d08a d376 8c57 0d1f  @]P&amp;.......v.W..</span></span><br><span class="line"><span class="string">....</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">01fff90: 5080 aaaa 0ef0 3c5c 0020 0d5c 6408 2400  P.....&lt;\. .\d.$.</span></span><br><span class="line"><span class="string">01fffa0: ef15 08e8 7d00 2399 d60f 1b1e 0000 0000  ....&#125;.#.........</span></span><br><span class="line"><span class="string">01fffb0: 966c 6516 0000 0000 ff37 fb12 0000 0000  .le......7......</span></span><br><span class="line"><span class="string">01fffc0: 954a c013 0000 0000 ea47 c570 2708 0000  .J.......G.p'</span>...</span><br><span class="line">01fffd0: 45a6 5fd7 6c03 8ca1 be55 4c44 2a6a 1500  E._.l....ULD*j..</span><br><span class="line">01fffe0: aa06 3ba9 aaba e2a6 fc0f 1b1e 0000 0000  ..;.............</span><br><span class="line">01ffff0: b46c 6516 0000 0000 2838 fb12 0000 0000  .le.....(8......</span><br><span class="line">0200000: 6c65 0975 6e6b 6e6f 776e 0930 0930 0969  le.unknown.0.0.i</span><br><span class="line">0200010: 6e74 726f 6e09 6578 6163 7409 3109 0931  ntron.exact.1..1</span><br><span class="line">0200020: 3709 3130 3030 4745 4e4f 4d45 532c 4142  7.1000GENOMES,AB</span><br><span class="line">0200030: 492c 4146 4659 2c42 4749 2c43 4f4d 504c  I,AFFY,BGI,COMPL</span><br><span class="line">0200040: 4554 455f 4745 4e4f 4d49 4353 2c43 5348  ETE_GENOMICS,CSH</span><br><span class="line">0200050: 4c2d 4841 504d 4150 2c45 4e53 454d 424c  L-HAPMAP,ENSEMBL</span><br><span class="line">0200060: 2c45 5641 2d47 4f4e 4c2c 474d 492c 4855  ,EVA-GONL,GMI,HU</span><br><span class="line">0200070: 4d41 4e47 454e 4f4d 455f 4a43 5649 2c4a  MANGENOME_JCVI,J</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">021ffb0: 454e 4f4d 4553 2c09 3209 2d2c 4154 4141  ENOMES,.2.-,ATAA</span><br><span class="line">021ffc0: 542c 0933 2e30 3030 3030 302c 3530 3035  T,.3.000000,5005</span><br><span class="line">021ffd0: 2e30 3030 3030 302c 0930 2e30 3030 3539  .000000,.0.00059</span><br><span class="line">021ffe0: 392c 302e 3939 3934 3031 2c09 0a37 3032  9,0.999401,..702</span><br><span class="line">021fff0: 0963 6872 3130 0931 3533 3338 3336 3909  .chr10.15338369.</span><br><span class="line">0220000: c000 8043 c001 8043 0008 8043 0000 8000  ...C...C...C....</span><br><span class="line">0220010: 0000 0500 0000 0000 0000 0000 8000 e4f0  ................</span><br><span class="line">0220020: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">0220030: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">0220040: c100 8043 c101 8043 0808 8043 0000 8000  ...C...C...C....</span><br><span class="line">0220050: 0000 0500 0000 0000 0000 0000 8000 9be0  ................</span><br><span class="line">0220060: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">0220070: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">0220080: c200 8043 c201 8043 1008 8043 0000 8000  ...C...C...C....</span><br><span class="line">0220090: 0000 0500 0000 0000 0000 0000 8000 1ad0  ................</span><br><span class="line">02200a0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">02200b0: 0000 0000 0000 0000 0000 0000 0000 0000  ................</span><br><span class="line">02200c0: c300 8043 c301 8043 1808 8043 0000 8000  ...C...C...C....</span><br><span class="line"></span><br><span class="line"><span class="comment">## Why there is file part as 128KiB size override file system reserve space ????</span></span><br><span class="line"><span class="comment">## Here is file system log , looks like no problem</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>ext4</tag>
        <tag>fsck</tag>
      </tags>
  </entry>
  <entry>
    <title>Benchmark</title>
    <url>/2018/07/12/benchmark/</url>
    <content><![CDATA[<p>Here is my benchmark info</p>
<a id="more"></a>

<h3 id="Block-benchmark-trap"><a href="#Block-benchmark-trap" class="headerlink" title="Block benchmark trap"></a>Block benchmark trap</h3><p>fio sequential write in the same device block cause the test result too high</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[rw]</span><br><span class="line">rw=write</span><br><span class="line">ioengine=posixaio</span><br><span class="line">iodepth=4</span><br><span class="line">nrfiles=8</span><br><span class="line">bssplit=1M/100</span><br><span class="line">direct=1</span><br><span class="line">filename=/dev/sdb1</span><br><span class="line">numjobs=4</span><br><span class="line">size=2000G</span><br></pre></td></tr></table></figure>

<p>For avoid this issue, and I don ‘t know why I have to wait after 10% test process, the speed will from 4GB/s down to 1.6GB/s<br>If you use fio test raw device, you ‘d better set numjobs=1, single core is enough</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">rw=write</span><br><span class="line">ioengine=posixaio</span><br><span class="line">iodepth=4</span><br><span class="line">nrfiles=8</span><br><span class="line">bssplit=1M/100</span><br><span class="line">direct=1</span><br><span class="line">filename=/dev/sdb1</span><br><span class="line">numjobs=1</span><br><span class="line">size=2000G</span><br><span class="line">zonemode=strided</span><br><span class="line">zonesize=100G</span><br><span class="line"></span><br><span class="line">[job1]</span><br><span class="line">zoneskip=1000G</span><br><span class="line">[job1]</span><br><span class="line">zoneskip=2000G</span><br><span class="line">[job3]</span><br><span class="line">zoneskip=3000G</span><br><span class="line">[job4]</span><br><span class="line">zoneskip=4000G</span><br><span class="line"></span><br><span class="line">job1: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=4</span><br><span class="line">job1: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=4</span><br><span class="line">job3: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=4</span><br><span class="line">job4: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=4</span><br><span class="line">fio-3.16</span><br><span class="line">Starting 4 processes</span><br><span class="line">Jobs: 4 (f=4): [W(4)][10.2%][w=1648MiB/s][w=1647 IOPS][eta 49m:02s]</span><br><span class="line"></span><br><span class="line"><span class="comment"># dd version to reproduce</span></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;=3200000; i=i+100000)); <span class="keyword">do</span> dd <span class="keyword">if</span>=/dev/zero of=/dev/sdb1 bs=1M seek=<span class="variable">$i</span> oflag=direct &amp; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>The two test function could help you get diff information</p>
<h3 id="Install-sysbench-0-5"><a href="#Install-sysbench-0-5" class="headerlink" title="Install sysbench 0.5"></a>Install sysbench 0.5</h3><p>OS version: CentOS 7</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /var/lib/mysql &amp;&amp; mount /dev/sdb1 /var/lib/mysql</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"deadline"</span> &gt; /sys/class/block/sdb/queue/scheduler</span><br><span class="line">yum install git automake libtool</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/hgxl64/sysbench-mariadb.git</span><br><span class="line"><span class="built_in">cd</span> sysbench-mariadb</span><br><span class="line">./autogen.sh</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line"><span class="built_in">cd</span> sysbech/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:$(<span class="built_in">pwd</span>)/</span><br><span class="line">sysbench --version</span><br><span class="line">sysbench 0.5</span><br></pre></td></tr></table></figure>

<h3 id="Install-mariadb"><a href="#Install-mariadb" class="headerlink" title="Install mariadb"></a>Install mariadb</h3><p><a href="https://downloads.mariadb.org/mariadb/repositories/#mirror=syringa&distro=CentOS&distro_release=centos7-amd64--centos7&version=10.1" target="_blank" rel="noopener">MariaDB.repo</a></p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment"># MariaDB 10.1 CentOS repository list - created 2016-03-02 02:05 UTC</span></span><br><span class="line"><span class="comment"># http://mariadb.org/mariadb/repositories/</span></span><br><span class="line">[mariadb]</span><br><span class="line">name = MariaDB</span><br><span class="line">baseurl = <span class="symbol">http:</span>/<span class="regexp">/yum.mariadb.org/</span><span class="number">10.1</span>/centos7-amd64</span><br><span class="line">gpgkey=<span class="symbol">https:</span>/<span class="regexp">/yum.mariadb.org/</span>RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">yum install mariadb-server mariadb-client mariadb-devel</span><br></pre></td></tr></table></figure>

<h4 id="limit-conf"><a href="#limit-conf" class="headerlink" title="limit.conf"></a>limit.conf</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">*               soft    nofile           <span class="number">20000</span></span><br><span class="line">*               hard    nofile           <span class="number">20000</span></span><br><span class="line">*               soft    nproc            <span class="number">20000</span></span><br><span class="line">*               hard    nproc            <span class="number">20000</span></span><br></pre></td></tr></table></figure>

<h4 id="my-cnf-ubuntu-16-04-mariadb-10-1"><a href="#my-cnf-ubuntu-16-04-mariadb-10-1" class="headerlink" title="my.cnf (ubuntu 16.04, mariadb 10.1)"></a>my.cnf (ubuntu 16.04, mariadb 10.1)</h4><figure class="highlight apacheconf"><table><tr><td class="code"><pre><span class="line"> <span class="attribute">cat</span> /etc/mysql/my.cnf | grep -v \#</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line"><span class="attribute">port</span>		= 3306</span><br><span class="line"><span class="attribute">socket</span>		= /var/run/mysqld/mysqld.sock</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line"><span class="attribute">socket</span>		= /var/run/mysqld/mysqld.sock</span><br><span class="line"><span class="attribute">nice</span>		= 0</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line"><span class="attribute">user</span>		= mysql</span><br><span class="line"><span class="attribute">pid</span>-file	= /var/run/mysqld/mysqld.pid</span><br><span class="line"><span class="attribute">socket</span>		= /var/run/mysqld/mysqld.sock</span><br><span class="line"><span class="attribute">port</span>		= 3306</span><br><span class="line"><span class="attribute">basedir</span>		= /usr</span><br><span class="line"><span class="attribute">datadir</span>		= /var/lib/mysql</span><br><span class="line"><span class="attribute">tmpdir</span>		= /tmp</span><br><span class="line"><span class="attribute">lc_messages_dir</span>	= /usr/share/mysql</span><br><span class="line"><span class="attribute">lc_messages</span>	= en_US</span><br><span class="line"><span class="attribute">skip</span>-external-locking</span><br><span class="line"><span class="attribute">bind</span>-address		= 127.0.0.1</span><br><span class="line"><span class="attribute">max_connections</span>		= 1024</span><br><span class="line"><span class="attribute">connect_timeout</span>		= 60</span><br><span class="line"><span class="attribute">wait_timeout</span>		= 600</span><br><span class="line"><span class="attribute">max_allowed_packet</span>	= 16M</span><br><span class="line"><span class="attribute">thread_cache_size</span>       = 128</span><br><span class="line"><span class="attribute">sort_buffer_size</span>	= 4M</span><br><span class="line"><span class="attribute">bulk_insert_buffer_size</span>	= 16M</span><br><span class="line"><span class="attribute">tmp_table_size</span>		= 32M</span><br><span class="line"><span class="attribute">max_heap_table_size</span>	= 32M</span><br><span class="line"><span class="attribute">myisam_recover_options</span> = BACKUP</span><br><span class="line"><span class="attribute">key_buffer_size</span>		= 8M</span><br><span class="line"><span class="attribute">table_open_cache</span>	= 800</span><br><span class="line"><span class="attribute">myisam_sort_buffer_size</span>	= 512M</span><br><span class="line"><span class="attribute">concurrent_insert</span>	= 2</span><br><span class="line"><span class="attribute">general_log</span>             = 0</span><br><span class="line"><span class="attribute">log_warnings</span>		= 2</span><br><span class="line"><span class="attribute">slow_query_log_file</span>	= /var/log/mysql/mariadb-slow.log</span><br><span class="line"><span class="attribute">long_query_time</span> = 60</span><br><span class="line"><span class="attribute">log_slow_verbosity</span>	= query_plan</span><br><span class="line"></span><br><span class="line"><span class="comment">#log_bin			= /var/log/mysql/mariadb-bin</span></span><br><span class="line"><span class="comment">#log_bin_index		= /var/log/mysql/mariadb-bin.index</span></span><br><span class="line"><span class="comment">#expire_logs_days	= 10</span></span><br><span class="line"><span class="comment">#max_binlog_size         = 100M</span></span><br><span class="line"><span class="comment">#diable /var/log/mysql bin-logs</span></span><br><span class="line"><span class="attribute">skip</span>-log-bin</span><br><span class="line"><span class="attribute">default_storage_engine</span>	= InnoDB</span><br><span class="line"><span class="attribute">innodb_log_file_size</span>	= 50M</span><br><span class="line"><span class="attribute">innodb_buffer_pool_size</span>	= 256M</span><br><span class="line"><span class="attribute">innodb_log_buffer_size</span>	= 16M</span><br><span class="line"><span class="attribute">innodb_file_per_table</span>	= true</span><br><span class="line"><span class="attribute">innodb_open_files</span>	= 4000</span><br><span class="line"><span class="attribute">innodb_io_capacity</span>	= 4000</span><br><span class="line"><span class="attribute">innodb_lru_scan_depth</span>   = 4000</span><br><span class="line"><span class="attribute">innodb_flush_method</span>	= O_DIRECT</span><br><span class="line"><span class="attribute">innodb_use_trim</span>=<span class="literal">ON</span></span><br><span class="line"><span class="attribute">innodb_use_fallocate</span>=<span class="literal">ON</span></span><br><span class="line"><span class="attribute">innodb_page_size</span>=16k</span><br><span class="line"><span class="attribute">innodb_thread_concurrency</span>=0</span><br><span class="line"><span class="attribute">innodb_write_io_threads</span>=24</span><br><span class="line"><span class="attribute">innodb_read_io_threads</span>=24</span><br><span class="line"><span class="attribute">innodb_buffer_pool_instances</span>=16</span><br><span class="line"><span class="attribute">innodb_mtflush_threads</span>=16</span><br><span class="line"><span class="attribute">innodb_doublewrite</span>=0</span><br><span class="line"></span><br><span class="line">[galera]</span><br><span class="line"></span><br><span class="line">[mysqldump]</span><br><span class="line"><span class="attribute">quick</span></span><br><span class="line"><span class="attribute">quote</span>-names</span><br><span class="line"><span class="attribute">max_allowed_packet</span>	= 16M</span><br><span class="line"></span><br><span class="line">[mysql]</span><br><span class="line"></span><br><span class="line">[isamchk]</span><br><span class="line"><span class="attribute">key_buffer</span>		= 16M</span><br><span class="line"></span><br><span class="line">!includedir /etc/mysql/conf.d/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attribute">ulimit</span> -n 20000</span><br><span class="line"><span class="attribute">mysqld_safe</span> --defaults-file=/etc/my.cnf</span><br><span class="line"><span class="attribute">mysql</span> -u root -e <span class="string">"create database sbtest"</span></span><br></pre></td></tr></table></figure>
<h4 id="Run-benchmark"><a href="#Run-benchmark" class="headerlink" title="Run benchmark"></a>Run benchmark</h4><p>If you have password, please add “–mysql-password=yourpass”</p>
<h5 id="prepare"><a href="#prepare" class="headerlink" title="prepare"></a>prepare</h5><p>Could not use prepare, run means parallel.</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">sysbench --test=tests/db/parallel_prepare.lua --mysql-host=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --mysql-port=<span class="number">3306</span> --mysql-user=root --oltp-tables-count=<span class="number">128</span> --oltp-table-<span class="built_in">size</span>=<span class="number">500000</span> --num-threads=<span class="number">128</span> --<span class="built_in">max</span>-time=<span class="number">300</span> <span class="built_in">run</span></span><br></pre></td></tr></table></figure>

<h5 id="read"><a href="#read" class="headerlink" title="read"></a>read</h5><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">sysbench</span> <span class="string">--test=tests/db/oltp.lua</span> <span class="string">--mysql-host=127.0.0.1</span> <span class="string">--mysql-port=3306</span> <span class="string">--mysql-user=root</span> <span class="string">--mysql-db=sbtest</span> <span class="string">--oltp_tables_count=128</span> <span class="string">--oltp-table-size=500000</span>  <span class="string">--rand-type=uniform</span> <span class="string">--oltp-read-only=on</span> <span class="string">--num-threads=128</span> <span class="string">--max-requests=0</span> <span class="string">--max-time=300</span> <span class="string">--report-interval=3</span> <span class="string">run</span></span><br></pre></td></tr></table></figure>

<h5 id="rw"><a href="#rw" class="headerlink" title="rw"></a>rw</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">sysbench --test=tests/db/oltp.lua --mysql-host=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --mysql-port=<span class="number">3306</span> --mysql-user=root --mysql-db=sbtest --oltp_tables_count=<span class="number">128</span> --oltp-table-<span class="built_in">size</span>=<span class="number">500000</span>  --rand-type=uniform --oltp-<span class="built_in">read</span>-only=off --num-threads=<span class="number">128</span> --<span class="built_in">max</span>-requests=<span class="number">0</span> --<span class="built_in">max</span>-time=<span class="number">300</span> --report-interval=<span class="number">3</span> <span class="built_in">run</span></span><br></pre></td></tr></table></figure>

<h5 id="update"><a href="#update" class="headerlink" title="update"></a>update</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">sysbench --test=tests/db/update_non_index.lua --mysql-host=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --mysql-port=<span class="number">3306</span> --mysql-user=root --mysql-db=sbtest --oltp_tables_count=<span class="number">128</span> --oltp-table-<span class="built_in">size</span>=<span class="number">500000</span>  --rand-type=uniform --oltp-<span class="built_in">read</span>-only=off --num-threads=<span class="number">128</span> --<span class="built_in">max</span>-requests=<span class="number">0</span> --<span class="built_in">max</span>-time=<span class="number">300</span> --report-interval=<span class="number">3</span> <span class="built_in">run</span></span><br></pre></td></tr></table></figure>

<h5 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">sysbench --test=tests/db/<span class="keyword">delete</span>.lua --mysql-host=<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> --mysql-port=<span class="number">3306</span> --mysql-user=root --mysql-db=sbtest --oltp_tables_count=<span class="number">128</span> --oltp-table-<span class="built_in">size</span>=<span class="number">500000</span>  --rand-type=uniform --oltp-<span class="built_in">read</span>-only=off --num-threads=<span class="number">128</span> --<span class="built_in">max</span>-requests=<span class="number">0</span> --<span class="built_in">max</span>-time=<span class="number">300</span> --report-interval=<span class="number">3</span> <span class="built_in">run</span></span><br></pre></td></tr></table></figure>
<h5 id="result"><a href="#result" class="headerlink" title="result"></a>result</h5><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">OLTP test statistics:</span></span><br><span class="line">    <span class="attr">queries performed:</span></span><br><span class="line">        <span class="attr">read:</span>                            <span class="number">492604</span></span><br><span class="line">        <span class="attr">write:</span>                           <span class="number">140744</span></span><br><span class="line">        <span class="attr">other:</span>                           <span class="number">70372</span></span><br><span class="line">        <span class="attr">total:</span>                           <span class="number">703720</span></span><br><span class="line">    <span class="attr">transactions:</span>                        <span class="number">35186</span>  <span class="string">(117.12</span> <span class="string">per</span> <span class="string">sec.)</span></span><br><span class="line">    <span class="attr">read/write requests:</span>                 <span class="number">633348</span> <span class="string">(2108.17</span> <span class="string">per</span> <span class="string">sec.)</span></span><br><span class="line">    <span class="attr">other operations:</span>                    <span class="number">70372</span>  <span class="string">(234.24</span> <span class="string">per</span> <span class="string">sec.)</span></span><br><span class="line">    <span class="attr">ignored errors:</span>                      <span class="number">0</span>      <span class="string">(0.00</span> <span class="string">per</span> <span class="string">sec.)</span></span><br><span class="line">    <span class="attr">reconnects:</span>                          <span class="number">0</span>      <span class="string">(0.00</span> <span class="string">per</span> <span class="string">sec.)</span></span><br><span class="line"></span><br><span class="line"><span class="attr">General statistics:</span></span><br><span class="line">    <span class="attr">total time:</span>                          <span class="number">300.</span><span class="string">4258s</span></span><br><span class="line">    <span class="attr">total number of events:</span>              <span class="number">35186</span></span><br><span class="line">    <span class="attr">total time taken by event execution:</span> <span class="number">9603.</span><span class="string">6924s</span></span><br><span class="line">    <span class="attr">response time:</span></span><br><span class="line">         <span class="attr">min:</span>                                 <span class="number">53.</span><span class="string">39ms</span></span><br><span class="line">         <span class="attr">avg:</span>                                <span class="number">272.</span><span class="string">94ms</span></span><br><span class="line">         <span class="attr">max:</span>                               <span class="number">1713.</span><span class="string">77ms</span></span><br><span class="line">         <span class="attr">approx.  95 percentile:</span>             <span class="number">496.</span><span class="string">65ms</span></span><br><span class="line"></span><br><span class="line"><span class="attr">Threads fairness:</span></span><br><span class="line">    <span class="string">events</span> <span class="string">(avg/stddev):</span>           <span class="number">1099.5625</span><span class="string">/11.17</span></span><br><span class="line">    <span class="string">execution</span> <span class="string">time</span> <span class="string">(avg/stddev):</span>   <span class="number">300.1154</span><span class="string">/0.12</span></span><br></pre></td></tr></table></figure>

<h5 id="About-performance-in-openzfs"><a href="#About-performance-in-openzfs" class="headerlink" title="About performance in openzfs"></a>About performance in openzfs</h5><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">MariaDB</span> <span class="string">[(none)]&gt;</span> <span class="string">SHOW</span> <span class="string">VARIABLES</span> <span class="string">LIKE</span> <span class="string">"innodb_page_size"</span><span class="string">;</span></span><br><span class="line"><span class="string">+------------------+-------+</span></span><br><span class="line"><span class="string">|</span> <span class="string">Variable_name</span>    <span class="string">|</span> <span class="string">Value</span> <span class="string">|</span></span><br><span class="line"><span class="string">+------------------+-------+</span></span><br><span class="line"><span class="string">|</span> <span class="string">innodb_page_size</span> <span class="string">|</span> <span class="number">16384</span> <span class="string">|</span></span><br><span class="line"><span class="string">+------------------+-------+</span></span><br><span class="line"><span class="number">1</span> <span class="string">row</span> <span class="string">in</span> <span class="string">set</span> <span class="string">(0.00</span> <span class="string">sec)</span></span><br></pre></td></tr></table></figure>

<p><a href="https://wiki.archlinux.org/index.php/ZFS#Database" target="_blank" rel="noopener">zfs db</a><br><a href="https://www-304.ibm.com/partnerworld/wps/servlet/download/DownloadServlet?id=YxooL$U5GZkiPCA$cnt&attachmentName=primer_to_run_sysbench_and_mariadb_benchmarks_on_lop.pdf&token=MTQ1NjgyMjIzMTcyNA==&locale=en_ALL_ZZ" target="_blank" rel="noopener">reference</a><br><a href="https://mariadb.org/using-lua-enabled-sysbench/" target="_blank" rel="noopener">reference2</a></p>
]]></content>
      <categories>
        <category>benchmark</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title>NIC receive package from linux</title>
    <url>/2018/07/12/nic/</url>
    <content><![CDATA[<h3 id="First-question"><a href="#First-question" class="headerlink" title="First question"></a>First question</h3><p>Does the ring buffer in ethernet NIC or it ‘s just server memory ?<br>Yes, NIC has the microprocess and system memory, but it’s not ring buffer in linux</p>
<p>High end ethernet cards(eg: Mellanox) will almost certainly have their own memory and microprocessors to offload work from the rest of the computer.<br>BTW: eg: if tcp stack offload by NIC and bypass kernel that means the ring buffer in NIC.</p>
<p>Low end ethernet cards may not have their own onboard memory or microprocessors, and will use the host system’s resources to handle network traffic.<br>BTW: I think if you NIC adapter could not offload anything, that means all packages will process by linux, some low end ethernet cards could bypass kernel too, could use direct memory access for some of IO</p>
<p>Infiniband cards on the other hand will tend to have their own onboard processors but no onboard memory, and will use direct memory access for all IO.</p>
<p><a href="https://stackoverflow.com/questions/17154759/how-much-memory-does-a-common-nic-have" target="_blank" rel="noopener">reference</a></p>
<a id="more"></a>

<h4 id="What-‘s-the-ring-buffer"><a href="#What-‘s-the-ring-buffer" class="headerlink" title="What ‘s the ring buffer"></a>What ‘s the ring buffer</h4><p><a href="https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/nicringbuffers.html" target="_blank" rel="noopener">Ring buffers on the NIC are important to handle bursts of incoming packets especially if there is some delay when the hardware interrupt handler schedules the packet receiving software interrupt (softirq). NIC ring buffer sizes vary per NIC vendor and NIC grade (that is, server or desktop). By increasing the Rx/Tx ring buffer size as shown below, you can decrease the probability of discarding packets in the NIC during a scheduling delay. The tool used to change ring buffer settings is the Linux utility, ethtool.</a></p>
<h4 id="Linux-network-receive-data-work-flow"><a href="#Linux-network-receive-data-work-flow" class="headerlink" title="Linux network receive data work flow"></a><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">Linux network receive data work flow</a></h4><p><a href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/" target="_blank" rel="noopener">reference2</a><br><img src="/img/ring-buffer.png" alt=""><br><img src="/img/2012110119582618-tcp-reception.png" alt=""><br><img src="/img/2012110119593557-tcp-transmission.png" alt=""></p>
<ol>
<li><p>Driver is loaded and initialized.</p>
<ul>
<li>Kernel malloc a special kernel memory regions for NIC to receive/transmit the network packets<ul>
<li>struct sk_buff is the memory interface for netowrk data<ul>
<li><a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/skbuff.h#L706" target="_blank" rel="noopener">sk_buff has the point to these memory</a>, Ring buffer just store packet descriptor one by one</li>
<li>There are two status for ring buffer: “used” or “ready” <ul>
<li>ready means (the null descriptor) and it point a free sk_buff”<ul>
<li>Linux malloc system memory —mapping—&gt; ring buffer queue ( interface:struct sk_buff, netdev_alloc_skb - allocate an skbuff for rx on a specific device ) <a href="https://elixir.bootlin.com/linux/v4.4/source/include/linux/skbuff.h#L706" target="_blank" rel="noopener">source code</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Packet arrives at the NIC from the network.</p>
</li>
<li><p>Packet is copied (via DMA)the data to a ring buffer in kernel memory</p>
<ul>
<li>DMA just get data from NIC<ul>
<li>Got the the next ready descriptor, and save the data to the descriptor,the descriptor point the memory regions(sk_buff), and modify the ready descriptor to “used”</li>
<li>It ‘s a FIFO queue</li>
</ul>
</li>
<li>NIC data —-DMA—-&gt; next ready packet descriptor (The default queuing discipline to use for network devices.)</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p>NIC generate the hardware interrupt to let the system know(trigger another software interrupt) a packet is in memory.</p>
<ul>
<li>If single packet trigger once IRQ to CPU, it must be exhaust a lot of CPU resources and it ‘s inefficiency</li>
<li>the kernel begin to run interrupt handler(driver register)</li>
</ul>
</li>
<li><p>Driver calls into NAPI to start a poll loop if one was not running already.</p>
<ul>
<li>Create New API(NAPI) to merge IRQs, to reduce IRQ times</li>
<li>The driver ‘s interrupt handeler use the napi_schedule trigger softirq(<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3204" target="_blank" rel="noopener">NET_RX_SOFTIRQ</a> to wake up NAPI subsystem</li>
</ul>
</li>
<li><p>ksoftirqd processes run on each CPU on the system. They are registered at boot time. The ksoftirqd processes pull packets off the ring buffer by calling the NAPI poll function that the device driver registered during initialization.</p>
</li>
<li><p>Memory regions in the ring buffer that have had network data written to them are unmapped.</p>
</li>
<li><p>Data that was DMA’d into memory is passed up the networking layer as an ‘skb’ for more processing.</p>
<ul>
<li>DMA get finish —-&gt; trigger the hardware interrupt (IRQ) let CPU receive the data —-&gt; if not support NAPI, one irq ,got one frame with single irq ?</li>
<li>DMA get finish —-&gt; trigger the hardware interrupt (IRQ) let CPU receive the data —-&gt; support NAPI (driver support), merge IRQ to reduce CPU loading    </li>
</ul>
</li>
<li><p>The softirq handler function net_rx_action will call the NAPI poll function to harvest packets (sk_buff).</p>
<ul>
<li><a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6361" target="_blank" rel="noopener">What ‘s the poll doing</a> ?<ul>
<li>Read out sk_buff from ring buffer </li>
<li>Check sk_buff, if single frame in multiple sk_buff, merge them</li>
<li>Deliver these network data frames to the upper-layer protocal from the queues<ul>
<li>Clear sk_buff, reset ring buff to ready</li>
<li>Update kernel/driver statistics , receive how many packages , bytes</li>
</ul>
</li>
<li>If no data or if trigger net.core.netdev_budget(packages numbers) or netdev_budget_usecs(timeout), exit the poll</li>
</ul>
</li>
</ul>
</li>
<li><p>Driver will disable the NIC IRQ, make sure will not received the new IRQ before poll finish all data</p>
</li>
<li><p>After poll receive all packegs, the NIC driver will re-enable NIC interrupts again and disable NAPI subsystem(napi_complete_done), exit pollng, NAPI subsystem will be disable, re-enable  NIC IRQ</p>
</li>
<li><p>Go to 3</p>
</li>
</ol>
<p>I guess, NIC transfer data by frame, and the frame has be splitted in sk_buff</p>
<h4 id="The-kernel-parameters"><a href="#The-kernel-parameters" class="headerlink" title="The kernel parameters"></a><a href="https://github.com/leandromoreira/linux-network-performance-parameters/blob/master/README.md" target="_blank" rel="noopener">The kernel parameters</a></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sysctl net.core.default_qdisc</span><br><span class="line">net.core.default_qdisc &#x3D; pfifo_fast #default fq</span><br></pre></td></tr></table></figure>
<p>default_qdisc is the default queuing discipline to use for network devices<br>tc -s qdisc ls dev ethX</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sysctl net.core.netdev_budget_usecs # default 2000</span><br></pre></td></tr></table></figure>
<p>netdev_budget_usecs maximum number of microseconds in one NAPI polling cycle. Polling will exit when either netdev_budget_usecs have elapsed during the poll cycle or the number of packets processed reaches netdev_budget</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sysctl net.core.netdev_budget # default 300</span><br></pre></td></tr></table></figure>
<p>netdev_budget is the maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed netdev_budget_usecs microseconds, even if netdev_budget has not been exhausted.</p>
<p>$ cat /proc/net/sockstat<br>sockets: used 911<br>TCP: inuse 7 orphan 0 tw 0 alloc 11 mem 2<br>UDP: inuse 6 mem 1<br>UDPLITE: inuse 0<br>RAW: inuse 0<br>FRAG: inuse 0 memory 0</p>
<h4 id="Socket-buffer"><a href="#Socket-buffer" class="headerlink" title="Socket buffer"></a>Socket buffer</h4><p>Each network socket is allocated a send buffer for outbound packets and a receive socket for inbound packets. These buffers are assigned a default size that depends on parameters of the operating system<br>In linux<br>receive<br>net.core.rmem_max<br>net.core.rmem_default </p>
<p>Sender<br>net.core.wmem_max<br>net.core.wmem_default </p>
<h5 id="sysctl-w-net-core-netdev-budget-2000"><a href="#sysctl-w-net-core-netdev-budget-2000" class="headerlink" title="sysctl -w net.core.netdev_budget=2000"></a>sysctl -w net.core.netdev_budget=2000</h5><p>Maximum number of packets taken from all interfaces in one polling cycle (NAPI poll).<br>In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed netdev_budget_usecs microseconds, even if netdev_budget has not been exhausted.</p>
<h6 id="netdev-budget-usecs"><a href="#netdev-budget-usecs" class="headerlink" title="netdev_budget_usecs"></a>netdev_budget_usecs</h6><p>Maximum number of microseconds in one NAPI polling cycle. Polling will exit when either netdev_budget_usecs have elapsed during the poll cycle or the number of packets processed reaches netdev_budget.</p>
<h5 id="netdev-max-backlog"><a href="#netdev-max-backlog" class="headerlink" title="netdev_max_backlog"></a>netdev_max_backlog</h5><p>Maximum number of  packets,  queued  on  the  INPUT  side, when the interface receives packets faster than kernel can process them.<br>Sets the maximum number of packets allowed to queue when a particular interface receives packets faster than the kernel can process them.</p>
<h5 id="Enabling-flow-limits-and-tuning-flow-limit-hash-table-size"><a href="#Enabling-flow-limits-and-tuning-flow-limit-hash-table-size" class="headerlink" title="Enabling flow limits and tuning flow limit hash table size"></a>Enabling flow limits and tuning flow limit hash table size</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sysctl -w net.core.flow_limit_table_len=8192</span><br><span class="line">The value is only consulted when a new table is allocated. Modifying</span><br><span class="line">it does not update active tables.</span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/net/core/flow_limit_cpu_bitmap</span><br><span class="line">00000</span><br><span class="line"></span><br><span class="line"><span class="comment"># set a bitmask </span></span><br><span class="line"><span class="built_in">echo</span> f &gt; /proc/sys/net/core/flow_limit_cpu_bitmap</span><br><span class="line"></span><br><span class="line">Per-flow rate is calculated by hashing each packet into a hashtable</span><br><span class="line">bucket and incrementing a per-bucket counter. The <span class="built_in">hash</span> <span class="keyword">function</span> is</span><br><span class="line">the same that selects a CPU <span class="keyword">in</span> RPS, but as the number of buckets can</span><br><span class="line">be much larger than the number of CPUs, flow <span class="built_in">limit</span> has finer-grained</span><br><span class="line">identification of large flows and fewer <span class="literal">false</span> positives. The default</span><br><span class="line">table has 4096 buckets. This value can be modified through sysctl</span><br><span class="line"></span><br><span class="line">Flow <span class="built_in">limit</span> is useful on systems with many concurrent connections,</span><br><span class="line"><span class="built_in">where</span> a single connection taking up 50% of a CPU indicates a problem.</span><br><span class="line">In such environments, <span class="built_in">enable</span> the feature on all CPUs that handle</span><br><span class="line">network rx interrupts (as <span class="built_in">set</span> <span class="keyword">in</span> /proc/irq/N/smp_affinity).</span><br><span class="line"></span><br><span class="line">The feature depends on the input packet queue length to exceed</span><br><span class="line">the flow <span class="built_in">limit</span> threshold (50%) + the flow <span class="built_in">history</span> length (256).</span><br><span class="line">Setting net.core.netdev_max_backlog to either 1000 or 10000</span><br><span class="line">performed well <span class="keyword">in</span> experiments.</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ethtool -l enp1s0f1<span class="string">"</span></span><br><span class="line"><span class="string">Channel parameters for enp1s0f1:</span></span><br><span class="line"><span class="string">Pre-set maximums:</span></span><br><span class="line"><span class="string">RX:		0</span></span><br><span class="line"><span class="string">TX:		0</span></span><br><span class="line"><span class="string">Other:		1</span></span><br><span class="line"><span class="string">Combined:	63</span></span><br><span class="line"><span class="string">Current hardware settings:</span></span><br><span class="line"><span class="string">RX:		0</span></span><br><span class="line"><span class="string">TX:		0</span></span><br><span class="line"><span class="string">Other:		1</span></span><br><span class="line"><span class="string">Combined:	4 # 4 cores Xeon E3 CPU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ ethtool -L enp1s0f1 combined 8  #combined means tx and rx</span></span><br><span class="line"><span class="string">$ ethtool -L enp1s0f1 rx 8</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#Does NIC support ring buffer multi queues</span></span><br><span class="line"><span class="string">$ ethtool -g em1</span></span><br><span class="line"><span class="string">Ring parameters for em1:</span></span><br><span class="line"><span class="string">Pre-set maximums:</span></span><br><span class="line"><span class="string">RX:		4078</span></span><br><span class="line"><span class="string">RX Mini:	0</span></span><br><span class="line"><span class="string">RX Jumbo:	0</span></span><br><span class="line"><span class="string">TX:		4078</span></span><br><span class="line"><span class="string">$ Current hardware settings:</span></span><br><span class="line"><span class="string">RX:		1024</span></span><br><span class="line"><span class="string">RX Mini:	0</span></span><br><span class="line"><span class="string">RX Jumbo:	0</span></span><br><span class="line"><span class="string">TX:		4078</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ethtool -G em1 rx 4078 tx 4078</span></span><br></pre></td></tr></table></figure>
<h3 id="NIC-Offload"><a href="#NIC-Offload" class="headerlink" title="NIC Offload"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/network-nic-offloads" target="_blank" rel="noopener">NIC Offload</a></h3><h4 id="Generic-Receive-Offloading-replace-Large-receive-offload-https-lwn-net-Articles-358910"><a href="#Generic-Receive-Offloading-replace-Large-receive-offload-https-lwn-net-Articles-358910" class="headerlink" title="[Generic Receive Offloading replace Large receive offload] (https://lwn.net/Articles/358910/)"></a>[Generic Receive Offloading replace Large receive offload] (<a href="https://lwn.net/Articles/358910/" target="_blank" rel="noopener">https://lwn.net/Articles/358910/</a>)</h4><p><a href="https://en.wikipedia.org/wiki/Large_receive_offload" target="_blank" rel="noopener">Large receive offload(LRO)</a><br> It works by aggregating multiple incoming packets from a single stream into a larger buffer before they are passed higher up the networking stack, thus reducing the number of packets that have to be processed. Linux implementations generally use LRO in conjunction with the New API (NAPI) to also reduce the number of interrupts.</p>
<p>LRO Uses the TCP protocol. All incoming packets are re-segmented as they are received, reducing the number of segments the system has to process. They can be merged either in the driver or using the NIC. A problem with LRO is that it tends to resegment all incoming packets, often ignoring differences in headers and other information which can cause errors. It is generally not possible to use LRO when IP forwarding is enabled.</p>
<p>GRO Uses either the TCP or UDP protocols. GRO is more rigorous than LRO when resegmenting packets. For example it checks the MAC headers of each packet, which must match, only a limited number of TCP or IP headers can be different, and the TCP timestamps must match. Resegmenting can be handled by either the NIC or the GSO code.</p>
<h4 id="TCP-Segmentation-Offload-TSO"><a href="#TCP-Segmentation-Offload-TSO" class="headerlink" title="TCP Segmentation Offload (TSO)"></a>TCP Segmentation Offload (TSO)</h4><p>Uses the TCP protocol to send large packets. Uses the NIC to handle segmentation, and then adds the TCP, IP and data link layer protocol headers to each segment.</p>
<h4 id="UDP-Fragmentation-Offload-UFO"><a href="#UDP-Fragmentation-Offload-UFO" class="headerlink" title="UDP Fragmentation Offload (UFO)"></a>UDP Fragmentation Offload (UFO)</h4><p>Uses the UDP protocol to send large packets. Uses the NIC to handle IP fragmentation into MTU sized packets for large UDP datagrams.</p>
<h4 id="Generic-Segmentation-Offload-GSO"><a href="#Generic-Segmentation-Offload-GSO" class="headerlink" title="Generic Segmentation Offload (GSO)"></a>Generic Segmentation Offload (GSO)</h4><p>Uses the TCP or UDP protocol to send large packets. If the NIC cannot handle segmentation/fragmentation, GSO performs the same operations, bypassing the NIC hardware. This is achieved by delaying segmentation until as late as possible, for example, when the packet is processed by the device driver.</p>
<h4 id="RECEIVE-SIDE-SCALING-RSS"><a href="#RECEIVE-SIDE-SCALING-RSS" class="headerlink" title="RECEIVE-SIDE SCALING (RSS)"></a>RECEIVE-SIDE SCALING (RSS)</h4><p>Receive-Side Scaling (RSS), also known as multi-queue receive, distributes network receive processing across several hardware-based receive queues, allowing inbound network traffic to be processed by multiple CPUs. RSS can be used to relieve bottlenecks in receive interrupt processing caused by overloading a single CPU, and to reduce network latency.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ethtool --show-rxfh-indir p4p1</span><br><span class="line">$ ethtool -x p4p1 </span><br><span class="line"><span class="comment">## You can set priority for each queue.</span></span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> p4p1 with 48 RX ring(s):</span><br><span class="line">    0:      0     1     2     3     4     5     6     7 </span><br><span class="line">    8:      8     9    10    11    12    13    14    15</span><br><span class="line">   16:     16    17    18    19    20    21    22    23</span><br><span class="line">   24:     24    25    26    27    28    29    30    31</span><br><span class="line">   32:     32    33    34    35    36    37    38    39</span><br><span class="line">   40:     40    41    42    43    44    45    46    47</span><br><span class="line">   48:      0     1     2     3     4     5     6     7</span><br><span class="line">   56:      8     9    10    11    12    13    14    15</span><br><span class="line">   64:     16    17    18    19    20    21    22    23</span><br><span class="line">   72:     24    25    26    27    28    29    30    31</span><br><span class="line">   80:     32    33    34    35    36    37    38    39</span><br><span class="line">   88:     40    41    42    43    44    45    46    47</span><br><span class="line">   96:      0     1     2     3     4     5     6     7</span><br><span class="line">  104:      8     9    10    11    12    13    14    15</span><br><span class="line">  112:     16    17    18    19    20    21    22    23</span><br><span class="line">  120:     24    25    26    27    28    29    30    31</span><br><span class="line"><span class="comment">### 16 x 8 = 128, total 128 value, that means indirection table = 128, total 48 RX rings</span></span><br><span class="line"><span class="comment">### eg: line "96:" second field(1), means 98th hash data equal 2, the data will go to second queue</span></span><br><span class="line"><span class="comment">### ethtool -X eth0 weight 6 5 4 3 2 1 .... , you can set 48 x weights, 48 data sum will not large than 128</span></span><br><span class="line"></span><br><span class="line">$ ethtool --show-rxfh-indir p4p1 rx-flow-hash tcp4</span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> p4p1 with 48 RX ring(s):</span><br><span class="line">    0:      0     1     2     3     4     5     6     7</span><br><span class="line">    8:      8     9    10    11    12    13    14    15</span><br><span class="line">   16:     16    17    18    19    20    21    22    23</span><br><span class="line">   24:     24    25    26    27    28    29    30    31</span><br><span class="line">   32:     32    33    34    35    36    37    38    39</span><br><span class="line">   40:     40    41    42    43    44    45    46    47</span><br><span class="line">   48:      0     1     2     3     4     5     6     7</span><br><span class="line">   56:      8     9    10    11    12    13    14    15</span><br><span class="line">   64:     16    17    18    19    20    21    22    23</span><br><span class="line">   72:     24    25    26    27    28    29    30    31</span><br><span class="line">   80:     32    33    34    35    36    37    38    39</span><br><span class="line">   88:     40    41    42    43    44    45    46    47</span><br><span class="line">   96:      0     1     2     3     4     5     6     7</span><br><span class="line">  104:      8     9    10    11    12    13    14    15</span><br><span class="line">  112:     16    17    18    19    20    21    22    23</span><br><span class="line">  120:     24    25    26    27    28    29    30    31</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#modify hash behavior</span></span><br><span class="line"><span class="comment">#src IP, dst IP, src Port, dst Port (sdfn)</span></span><br><span class="line">To include UDP port numbers <span class="keyword">in</span> RSS hashing run(4-tuple):</span><br><span class="line">$ ethtool -N eth1 rx-flow-hash udp4 sdfn</span><br><span class="line"></span><br><span class="line">To exclude UDP port numbers from RSS hashing run(2-tuple):</span><br><span class="line">$ ethtool -N eth1 rx-flow-hash udp4 sd</span><br><span class="line"></span><br><span class="line">To display UDP hashing current configuration run:</span><br><span class="line">$ ethtool -n eth1 rx-flow-hash udp4</span><br><span class="line"></span><br><span class="line"><span class="comment"># use --set-rxfh-indir modify the parameters</span></span><br><span class="line">`</span><br></pre></td></tr></table></figure>
<p>Receive Packet Steering (RPS) is similar to RSS in that it is used to direct packets to specific CPUs for processing. However, RPS is implemented at the software level, and helps to prevent the hardware queue of a single network interface card from becoming a bottleneck in network traffic.<br>Does the all cpu cores share a  hardware-based receive queues ?</p>
<h4 id="Receive-Flow-Steering-ntuple-RFS"><a href="#Receive-Flow-Steering-ntuple-RFS" class="headerlink" title="Receive Flow Steering, (ntuple,RFS)"></a><a href="https://wiki.freebsd.org/201305DevSummit/NetworkReceivePerformance/ComparingMutiqueueSupportLinuxvsFreeBSD" target="_blank" rel="noopener">Receive Flow Steering, (ntuple,RFS)</a></h4><p>RSS transmit diff the network frame to diff cpu core, the userspace application not in this cpu core, move data to cache for share, and avoid cache bouncing<br>RPS/ntuple will to optimze the process</p>
<p>Receive Flow Steering (RFS) extends RPS/RSS behavior to increase the CPU cache hit rate and thereby reduce network latency. Where RPS forwards packets based solely on queue length, RFS uses the RPS backend to calculate the most appropriate CPU, then forwards packets based on the location of the application consuming the packet. This increases CPU cache efficiency.</p>
<ul>
<li>RFS software<ul>
<li>RPS does not increase the hardware interrupt rate of the network device.</li>
</ul>
</li>
<li>ntuple hardware<ul>
<li>EP, Externally Programed<ul>
<li>Custom policy<ul>
<li>Match the policy, move the frame to the transmit CPU core queue </li>
<li>or match the policy/action, move to special queue for route/firewall</li>
</ul>
</li>
</ul>
</li>
<li>ATR, Automated Application Targeting Routing</li>
</ul>
</li>
</ul>
<p>There’s generic flow steering rule configuration interface on ethtool, called RX NFC.<br>Accelerated RFS configures Flow Steering automatically, RX NFC offers manual configuration feature to user.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ lshw -c network -businfo</span><br><span class="line">Bus info          Device      Class          Description</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">pci@0000:04:00.0  em1         network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe</span><br><span class="line">pci@0000:04:00.1  em2         network        NetXtreme BCM5720 2-port Gigabit Ethernet PCIe</span><br><span class="line">pci@0000:65:00.0  p2p1        network        Ethernet 10G 2P X520 Adapter</span><br><span class="line">pci@0000:65:00.1  p2p2        network        Ethernet 10G 2P X520 Adapter</span><br><span class="line">                  bond0       network        Ethernet interface</span><br><span class="line">#To specify that all traffic from 10.23.4.6 to 10.23.4.18 be placed in queue 4, issue this command:</span><br><span class="line">$ ethtool --config-ntuple flow-type tcp4 src-ip 10.23.4.6 dst-ip 10.23.4.18 action 4</span><br><span class="line"></span><br><span class="line">#Forwards to queue 2 all IPv4 TCP traffic from 192.168.10.1:2000 that is going to 192.168.10.2:2001, placing the filter at position 33 of the Perfect-Match filter table (and overwriting any rule currently in that position):</span><br><span class="line">$ ethtool --config-ntuple &lt;interface name&gt; flow-type tcp4 src-ip 192.168.10.1 dst-ip 192.168.10.2 src-port 2000 dst-port 2001 action 2 loc 33</span><br><span class="line"></span><br><span class="line">#Drops all UDP packets from 10.4.83.2:</span><br><span class="line">$ ethtool --config-ntuple flow-type udp4 src-ip 10.4.82.2 action -1</span><br><span class="line">#Note: The VLAN field is not a supported filter with the i40e driver (Intel Ethernet Controller XL710 and Intel Ethernet Controller X710 NICs).</span><br><span class="line">#For more information and options, see the ethtool man page documentation on the -U, -N, or --config-ntuple option.</span><br><span class="line"></span><br><span class="line"># List</span><br><span class="line">$ ethtool --show-ntuple p2p1</span><br><span class="line">10 RX rings available</span><br><span class="line">Total 0 rules</span><br><span class="line"></span><br><span class="line"># Remove </span><br><span class="line">$ ethtool --config-ntuple &lt;interface name&gt; delete N</span><br><span class="line"></span><br><span class="line">$ ethtool -K eth0 ntuple on</span><br><span class="line">$ ethtool -k eth0|grep ntuple</span><br><span class="line">ntuple-filters: on</span><br><span class="line">$ ethtool --config-nfc ix00 flow-type tcp4 src-ip 10.0.0.1 dst-ip 10.0.0.2 src-port 10000 dst-port 10001 action 6</span><br><span class="line">Added rule with ID 2045</span><br><span class="line">$ ethtool --show-nfc ix00</span><br><span class="line">12 RX rings available</span><br><span class="line">Total 1 rules</span><br><span class="line"></span><br><span class="line">Filter: 2045</span><br><span class="line">     Rule Type: TCP over IPv4</span><br><span class="line">     Src IP addr: 10.0.0.1 mask: 0.0.0.0</span><br><span class="line">     Dest IP addr: 10.0.0.2 mask: 0.0.0.0</span><br><span class="line">     TOS: 0x0 mask: 0xff</span><br><span class="line">     Src port: 10000 mask: 0x0</span><br><span class="line">     Dest port: 10001 mask: 0x0</span><br><span class="line">     VLAN EtherType: 0x0 mask: 0xffff</span><br><span class="line">     VLAN: 0x0 mask: 0xffff</span><br><span class="line">     User-defined: 0x0 mask: 0xffffffffffffffff</span><br><span class="line">     Action: Direct to queue 6</span><br><span class="line">$ ethtool --config-ntuple eth0  rx-flow-hash tcp4 sdfn</span><br><span class="line">$ ethtool --show-ntuple eth0 rx-flow-hash tcp4</span><br></pre></td></tr></table></figure>

<h5 id="intel-82599"><a href="#intel-82599" class="headerlink" title="intel 82599"></a>intel 82599</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ethtool -k enp131s0f1</span><br><span class="line">Features <span class="keyword">for</span> enp131s0f1:</span><br><span class="line">rx-checksumming: on</span><br><span class="line">tx-checksumming: on</span><br><span class="line">	tx-checksum-ipv4: off [fixed]</span><br><span class="line">	tx-checksum-ip-generic: on</span><br><span class="line">	tx-checksum-ipv6: off [fixed]</span><br><span class="line">	tx-checksum-fcoe-crc: on [fixed]</span><br><span class="line">	tx-checksum-sctp: on</span><br><span class="line">scatter-gather: on</span><br><span class="line">	tx-scatter-gather: on</span><br><span class="line">	tx-scatter-gather-fraglist: off [fixed]</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">	tx-tcp-segmentation: on</span><br><span class="line">	tx-tcp-ecn-segmentation: off [fixed]</span><br><span class="line">	tx-tcp6-segmentation: on</span><br><span class="line">	tx-tcp-mangleid-segmentation: off</span><br><span class="line">udp-fragmentation-offload: off [fixed]</span><br><span class="line">generic-segmentation-offload: on</span><br><span class="line">generic-receive-offload: on</span><br><span class="line">large-receive-offload: off</span><br><span class="line">rx-vlan-offload: on</span><br><span class="line">tx-vlan-offload: on</span><br><span class="line">ntuple-filters: off</span><br><span class="line">receive-hashing: on</span><br><span class="line">highdma: on [fixed]</span><br><span class="line">rx-vlan-filter: on</span><br><span class="line">vlan-challenged: off [fixed]</span><br><span class="line">tx-lockless: off [fixed]</span><br><span class="line">netns-local: off [fixed]</span><br><span class="line">tx-gso-robust: off [fixed]</span><br><span class="line">tx-fcoe-segmentation: on [fixed]</span><br><span class="line">tx-gre-segmentation: on</span><br><span class="line">tx-ipip-segmentation: on</span><br><span class="line">tx-sit-segmentation: on</span><br><span class="line">tx-udp_tnl-segmentation: on</span><br><span class="line">tx-mpls-segmentation: off [fixed]</span><br><span class="line">fcoe-mtu: off [fixed]</span><br><span class="line">tx-nocache-copy: off</span><br><span class="line">loopback: off [fixed]</span><br><span class="line">rx-fcs: off [fixed]</span><br><span class="line">rx-all: off</span><br><span class="line">tx-vlan-stag-hw-insert: off [fixed]</span><br><span class="line">rx-vlan-stag-hw-parse: off [fixed]</span><br><span class="line">rx-vlan-stag-filter: off [fixed]</span><br><span class="line">busy-poll: on [fixed]</span><br><span class="line">tx-gre-csum-segmentation: on</span><br><span class="line">tx-udp_tnl-csum-segmentation: on</span><br><span class="line">tx-gso-partial: on</span><br><span class="line">tx-sctp-segmentation: off [fixed]</span><br><span class="line">l2-fwd-offload: off</span><br><span class="line">hw-tc-offload: off [fixed]</span><br></pre></td></tr></table></figure>

<h5 id="Mellanox-ConnectX-4-Lx"><a href="#Mellanox-ConnectX-4-Lx" class="headerlink" title="Mellanox ConnectX-4 Lx"></a>Mellanox ConnectX-4 Lx</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ethtool -k enp131s0f1</span><br><span class="line">Features <span class="keyword">for</span> enp131s0f1:</span><br><span class="line">rx-checksumming: on</span><br><span class="line">tx-checksumming: on</span><br><span class="line">	tx-checksum-ipv4: on</span><br><span class="line">	tx-checksum-ip-generic: off [fixed]</span><br><span class="line">	tx-checksum-ipv6: on</span><br><span class="line">	tx-checksum-fcoe-crc: off [fixed]</span><br><span class="line">	tx-checksum-sctp: off [fixed]</span><br><span class="line">scatter-gather: on</span><br><span class="line">	tx-scatter-gather: on</span><br><span class="line">	tx-scatter-gather-fraglist: off [fixed]</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">	tx-tcp-segmentation: on</span><br><span class="line">	tx-tcp-ecn-segmentation: off [fixed]</span><br><span class="line">	tx-tcp6-segmentation: on</span><br><span class="line">	tx-tcp-mangleid-segmentation: off</span><br><span class="line">udp-fragmentation-offload: off [fixed]</span><br><span class="line">generic-segmentation-offload: on</span><br><span class="line">generic-receive-offload: on</span><br><span class="line">large-receive-offload: on</span><br><span class="line">rx-vlan-offload: on</span><br><span class="line">tx-vlan-offload: on</span><br><span class="line">ntuple-filters: off</span><br><span class="line">receive-hashing: on</span><br><span class="line">highdma: on [fixed]</span><br><span class="line">rx-vlan-filter: on</span><br><span class="line">vlan-challenged: off [fixed]</span><br><span class="line">tx-lockless: off [fixed]</span><br><span class="line">netns-local: off [fixed]</span><br><span class="line">tx-gso-robust: off [fixed]</span><br><span class="line">tx-fcoe-segmentation: off [fixed]</span><br><span class="line">tx-gre-segmentation: off [fixed]</span><br><span class="line">tx-ipip-segmentation: off [fixed]</span><br><span class="line">tx-sit-segmentation: off [fixed]</span><br><span class="line">tx-udp_tnl-segmentation: on</span><br><span class="line">tx-mpls-segmentation: off [fixed]</span><br><span class="line">fcoe-mtu: off [fixed]</span><br><span class="line">tx-nocache-copy: off</span><br><span class="line">loopback: off [fixed]</span><br><span class="line">rx-fcs: off</span><br><span class="line">rx-all: off</span><br><span class="line">tx-vlan-stag-hw-insert: off [fixed]</span><br><span class="line">rx-vlan-stag-hw-parse: off [fixed]</span><br><span class="line">rx-vlan-stag-filter: off [fixed]</span><br><span class="line">busy-poll: off [fixed]</span><br><span class="line">tx-gre-csum-segmentation: off [fixed]</span><br><span class="line">tx-udp_tnl-csum-segmentation: on</span><br><span class="line">tx-gso-partial: on</span><br><span class="line">tx-sctp-segmentation: off [fixed]</span><br><span class="line">l2-fwd-offload: off [fixed]</span><br><span class="line">hw-tc-offload: off</span><br></pre></td></tr></table></figure>

<h4 id="softnet-stat"><a href="#softnet-stat" class="headerlink" title="softnet_stat"></a><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">softnet_stat</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/net/softnet_stat;</span><br><span class="line">00000797 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">1e391700 00000000 00000039 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">e46ce530 00000000 00000010 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">046aed8d 00000000 0000003f 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">-----    -----    --------                                              -------  -------</span><br><span class="line">   \      \_ drop count \                                                  \        \-flow <span class="built_in">limit</span> cont</span><br><span class="line">    \_ packet count      \                                                  \-- cpu collision          </span><br><span class="line">                          \- time sequeeze</span><br><span class="line"></span><br><span class="line">Important details about /proc/net/softnet_stat:</span><br><span class="line"></span><br><span class="line">Each line of /proc/net/softnet_stat corresponds to a struct softnet_data structure, of <span class="built_in">which</span> there is 1 per CPU.</span><br><span class="line"></span><br><span class="line">The values are separated by a single space and are displayed <span class="keyword">in</span> hexadecimal</span><br><span class="line">The first value, sd-&gt;processed, is the number of network frames processed. This can be more than the total number of network frames received <span class="keyword">if</span> you are using ethernet bonding. There are cases <span class="built_in">where</span> the ethernet bonding driver will trigger network data to be re-processed, <span class="built_in">which</span> would increment the sd-&gt;processed count more than once <span class="keyword">for</span> the same packet.</span><br><span class="line"></span><br><span class="line">The second value, sd-&gt;dropped, is the number of network frames dropped because there was no room on the processing queue. More on this later.</span><br><span class="line">If second values are gorwing, improve sysctl -w net.core.netdev_max_backlog = 2000, A value over 10000 is unlikely to be very helpful.</span><br><span class="line"></span><br><span class="line">The third value, sd-&gt;time_squeeze, is (as we saw) the number of <span class="built_in">times</span> the net_rx_action loop terminated because the budget was consumed or the time <span class="built_in">limit</span> was reached, but more work could have been. Increasing the budget as explained earlier can <span class="built_in">help</span> reduce this.</span><br><span class="line">If 3rd column is growing,improve sysctl -w net.core.netdev_budget=600</span><br><span class="line">Be careful of increasing this value unless there is a very good reason. A value exceeding 1000 is unlikely to be very helpful. In fact increasing this value too much can have detrimental effect and <span class="keyword">in</span> the worse <span class="keyword">case</span> scenario lead to softirq hangs or performance problems, as the softirqs can run <span class="keyword">for</span> too long and starve other processes of CPU</span><br><span class="line"></span><br><span class="line">The next 5 values are always 0.</span><br><span class="line"></span><br><span class="line">The ninth value, sd-&gt;cpu_collision, is a count of the number of <span class="built_in">times</span> a collision occurred when trying to obtain a device lock when transmitting packets. This article is about receive, so this statistic will not be seen below.</span><br><span class="line"></span><br><span class="line">The tenth value, sd-&gt;received_rps, is a count of the number of <span class="built_in">times</span> this CPU has been woken up to process packets via an Inter-processor Interrupt</span><br><span class="line"></span><br><span class="line">The last value, flow_limit_count, is a count of the number of <span class="built_in">times</span> the flow <span class="built_in">limit</span> has been reached. Flow limiting is an optional Receive Packet Steering feature that will be examined shortly.</span><br><span class="line"></span><br><span class="line">If you decide to monitor this file and graph the results, you must be extremely careful that the ordering of these fields hasn’t changed and that the meaning of each field has been preserved. You will need to <span class="built_in">read</span> the kernel <span class="built_in">source</span> to verify this.</span><br><span class="line"></span><br><span class="line">seq_printf(seq,</span><br><span class="line">       <span class="string">"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n"</span>,</span><br><span class="line">       sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,</span><br><span class="line">       0, 0, 0, 0, /* was fastroute */</span><br><span class="line">       sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</span><br><span class="line"></span><br><span class="line"><span class="comment">#convert these data by bash</span></span><br><span class="line"></span><br><span class="line">cat ./softnet_stat.sh</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">awk <span class="string">'BEGIN &#123;</span></span><br><span class="line"><span class="string">     t[1]="sd-&gt;processed"</span></span><br><span class="line"><span class="string">     t[2]="sd-&gt;dropped"</span></span><br><span class="line"><span class="string">     t[3]="sd-&gt;time_squeeze"</span></span><br><span class="line"><span class="string">     t[9]="sd-&gt;cpu_collision"</span></span><br><span class="line"><span class="string">     t[10]="sd-&gt;received_rps"</span></span><br><span class="line"><span class="string">     printf "%s %s %s %s %s\n",t[1],t[2],t[3],t[9],t[10];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">   printf "%d %d %d %d %d\n", strtonum( "0x"$1 ),strtonum( "0x"$2 ),strtonum( "0x"$3 ),strtonum( "0x"$9 ),strtonum( "0x"$10 )</span></span><br><span class="line"><span class="string">&#125;'</span>  /proc/net/softnet_stat | column -t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ watch -d ./softnet_stat.sh</span><br><span class="line">Every 2.0s: ./softnet_stat.sh                                                                                          Wed Jul 18 11:32:10 2018</span><br><span class="line"></span><br><span class="line">sd-&gt;processed  sd-&gt;dropped  sd-&gt;time_squeeze  sd-&gt;cpu_collision  sd-&gt;received_rps</span><br><span class="line">511952849      0            3                 0                  0</span><br><span class="line">172076174      0            0                 0                  0</span><br><span class="line">424094332      0            0                 0                  0</span><br><span class="line">202775884      0            0                 0                  0</span><br><span class="line">207782085      0            0                 0                  0</span><br><span class="line">269964983      0            1                 0                  0</span><br><span class="line">409462655      0            0                 0                  0</span><br><span class="line">253556517      0            0                 0                  0</span><br><span class="line">215071078      0            0                 0                  0</span><br><span class="line">321583328      0            0                 0                  0</span><br><span class="line">4854           0            0                 0                  0</span><br><span class="line">5438           0            0                 0                  0</span><br><span class="line">4832           0            0                 0                  0</span><br><span class="line">5124           0            0                 0                  0</span><br><span class="line">5409           0            0                 0                  0</span><br><span class="line">5512           0            0                 0                  0</span><br><span class="line">5765           0            0                 0                  0</span><br><span class="line">4915           0            0                 0                  0</span><br><span class="line">4208           0            0                 0                  0</span><br><span class="line">3617           0            0                 0                  0</span><br></pre></td></tr></table></figure>

<h3 id="why-telent-return-after-127s"><a href="#why-telent-return-after-127s" class="headerlink" title="why telent return after 127s"></a><a href="https://testerhome.com/topics/8859" target="_blank" rel="noopener">why telent return after 127s</a></h3><p>net.ipv4.tcp_syn_retries = 6</p>
<p>tcp_syn_retries - INTEGER</p>
<p>Number of times initial SYNs for an active TCP connection attempt will be retransmitted. Should not be higher than 127. Default value<br>is 6, which corresponds to 63seconds till the last retransmission with the current initial RTO of 1second. With this the final timeout for an active TCP connection attempt will happen after 127seconds.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">time telnet <span class="variable">$ip</span> <span class="variable">$port</span></span><br><span class="line"></span><br><span class="line">Trying <span class="variable">$ip</span>...</span><br><span class="line">telnet: connect to address <span class="variable">$ip</span>: Connection timed out</span><br><span class="line">telnet <span class="variable">$ip</span> <span class="variable">$port</span>  0.00s user 0.00s system 0% cpu 2:07.29 total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">18:04:23.765507 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13801993 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:04:24.768182 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13802996 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:04:26.772188 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13805000 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:04:30.780189 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13809008 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:04:38.796205 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13817024 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:04:54.828196 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13833056 ecr 0,nop,wscale 7], length 0</span><br><span class="line">18:05:26.860210 IP <span class="variable">$ip1</span>.<span class="variable">$port1</span> &gt; <span class="variable">$ip2</span>.<span class="variable">$port2</span>: Flags [S], seq 922947731, win 29200, options [mss 1460,sackOK,TS val 13865088 ecr 0,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">18:04:23.765507</span><br><span class="line">18:04:24.768182 1s</span><br><span class="line">18:04:26.772188 2s</span><br><span class="line">18:04:30.780189 4s</span><br><span class="line">18:04:38.796205 8s</span><br><span class="line">18:04:54.828196 16s</span><br><span class="line">18:05:26.860210 32s</span><br><span class="line">06:30.98        64s</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line"><span class="comment">#net.ipv4.tcp_syn_retries = 6</span></span><br></pre></td></tr></table></figure>

<h3 id="Kernel-optimze"><a href="#Kernel-optimze" class="headerlink" title="Kernel optimze"></a>Kernel optimze</h3><p>&lt; 4.4 kernel each listener has a request queue, receive the syn package, the listener create a request for insert, &gt;=4.4 kernel the new requestfunction, use tcp ehash table, reduce the listener lock to compete</p>
<h3 id="Enable-VT-D-and-VT-in-BIOS"><a href="#Enable-VT-D-and-VT-in-BIOS" class="headerlink" title="Enable VT-D and VT in BIOS"></a>Enable VT-D and VT in BIOS</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grubby --update-kernel=ALL --args=<span class="string">'intel_iommu=on'</span></span><br></pre></td></tr></table></figure>

<p><code>Please enable sr-iov in BIOS setting with the NIC</code></p>
<h4 id="sriov-doc"><a href="#sriov-doc" class="headerlink" title="sriov doc"></a><a href="https://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/xl710-sr-iov-config-guide-gbe-linux-brief.pdf" target="_blank" rel="noopener">sriov doc</a></h4><p>On Linux Kernel version 3.8.x and above, the maximum number of VFs supported by the adapter can be queried by reading the sriov_totalvfs parameter via sysfs interface. </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/class/net/ens5f0/device/sriov_totalvfs</span><br><span class="line">64</span><br><span class="line">$ cat /sys/class/net/ens5f0/device/sriov_numvfs</span><br><span class="line">4</span><br><span class="line">$ lspci | grep Virtual</span><br><span class="line">03:02.0 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:02.1 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:02.2 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:02.3 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:0a.0 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:0a.1 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:0a.2 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line">03:0a.3 Ethernet controller: Intel Corporation XL710/X710 Virtual Function (rev 02)</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 4 &gt; /sys/class/net/ens5f0/device/sriov_numvfs</span><br><span class="line"><span class="built_in">echo</span> 4 &gt; /sys/class/net/ens5f1/device/sriov_numvfs</span><br><span class="line">modprobe -r i40evf</span><br><span class="line">ip link <span class="built_in">set</span> ens5f0 vf 0 mac 52:54:00:e8:a5:78</span><br><span class="line">ip link <span class="built_in">set</span> ens5f0 vf 1 mac 52:54:00:e8:a5:79</span><br><span class="line">ip link <span class="built_in">set</span> ens5f0 vf 2 mac 52:54:00:e8:a5:80</span><br><span class="line">ip link <span class="built_in">set</span> ens5f0 vf 3 mac 52:54:00:e8:a5:81</span><br><span class="line">ip link <span class="built_in">set</span> ens5f1 vf 0 mac 52:54:00:e8:a5:82</span><br><span class="line">ip link <span class="built_in">set</span> ens5f1 vf 1 mac 52:54:00:e8:a5:83</span><br><span class="line">ip link <span class="built_in">set</span> ens5f1 vf 2 mac 52:54:00:e8:a5:84</span><br><span class="line">ip link <span class="built_in">set</span> ens5f1 vf 3 mac 52:54:00:e8:a5:85</span><br><span class="line"></span><br><span class="line">$ ip link show ens5f0</span><br><span class="line">19: ens5f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9000 qdisc mq state UP mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 0c:c4:7a:88:0c:aa brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    vf 0 MAC 52:54:00:e8:a5:78, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 1 MAC 52:54:00:e8:a5:79, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 2 MAC 52:54:00:e8:a5:80, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 3 MAC 52:54:00:e8:a5:81, spoof checking on, link-state auto, trust off</span><br><span class="line">$ ip link show ens5f1</span><br><span class="line">20: ens5f1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 0c:c4:7a:88:0c:ab brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    vf 0 MAC 52:54:00:e8:a5:82, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 1 MAC 52:54:00:e8:a5:83, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 2 MAC 52:54:00:e8:a5:84, spoof checking on, link-state auto, trust off</span><br><span class="line">    vf 3 MAC 52:54:00:e8:a5:85, spoof checking on, link-state auto, trust off</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>nic</category>
      </categories>
      <tags>
        <tag>hardware</tag>
        <tag>nic</tag>
      </tags>
  </entry>
  <entry>
    <title>scsi device maintain</title>
    <url>/2018/07/08/scsi_dev/</url>
    <content><![CDATA[<h3 id="How-many-devices-can-you-connect"><a href="#How-many-devices-can-you-connect" class="headerlink" title="How many devices can you connect?"></a><a href="https://www.microsemi.com/document-portal/doc_download/136098-university-training-what-is-sas" target="_blank" rel="noopener">How many devices can you connect?</a></h3><p>Individual SAS RAID adapters can typically support up to 128 end devices with the use of SAS expanders, and can communicate with both SAS and SATA devices.</p>
<p>Note: Although you can use both SAS and SATA disk drives in the same SAS domain (see page 93), we recommend that you not combine SAS and SATA disk drives within the same array or logical<br>drive. The difference in performance between the two types of disk drives may adversely affect the performance of the array.</p>
<p>However, as you read in the previous section, with the use of fanout and edge expanders, a single SAS domain can comprise up to 16,384 SAS ports.</p>
<a id="more"></a>

<h3 id="Enable-scsi-device-led-from-JBOD"><a href="#Enable-scsi-device-led-from-JBOD" class="headerlink" title="Enable scsi device led from JBOD"></a>Enable scsi device led from JBOD</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### Active sas/sata dev led</span></span><br><span class="line"><span class="comment">#### LSI</span></span><br><span class="line">```bash</span><br><span class="line">sas3ircu 0 display</span><br><span class="line"></span><br><span class="line">Device is a Hard disk</span><br><span class="line">  Enclosure <span class="comment">#                             : 3</span></span><br><span class="line">  Slot <span class="comment">#                                  : 14</span></span><br><span class="line">  SAS Address                             : 0000000-0-0000-0000</span><br><span class="line">  State                                   : Ready (RDY)</span><br><span class="line">  Size (<span class="keyword">in</span> MB)/(<span class="keyword">in</span> sectors)               : 3815447/976754645</span><br><span class="line">  Manufacturer                            : HGST</span><br><span class="line">  Model Number                            : HUS726040AL4210</span><br><span class="line">  Firmware Revision                       : AD05</span><br><span class="line">  Serial No                               : 00000000</span><br><span class="line">  Unit Serial No(VPD)                     : 00000000</span><br><span class="line">  GUID                                    : 0000000000000000</span><br><span class="line">  Protocol                                : SAS</span><br><span class="line">  Drive Type                              : SAS_HDD</span><br><span class="line"></span><br><span class="line">sas3ircu 0 locate 3:14 on</span><br><span class="line">sas3ircu 0 locate 3:14 off</span><br></pre></td></tr></table></figure>

<h4 id="Dell"><a href="#Dell" class="headerlink" title="Dell"></a>Dell</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lsscsi -gi | grep 35000cca2735b4104</span><br><span class="line">[17:0:132:0] disk    HGST     HUH721010AL5200  LS14  /dev/sddw  35000cca2735b4104  /dev/sg134</span><br><span class="line"></span><br><span class="line">$ shmcli list drives | grep /dev/sg134</span><br><span class="line">     5000cca2735b4107 /dev/sg134                   HGST                          HUH721010AL5200            2YHM5T6D     LS14      8.91TB          50050cc11ac013c6 (EN-8435A-E6EBD)  TH007FPRHGT0087762ZGA02</span><br><span class="line"></span><br><span class="line">$ shmcli blink drive -d = 5000cca2735b4107</span><br><span class="line">blink drive - Executing <span class="built_in">command</span>..</span><br><span class="line"></span><br><span class="line">Blink drive successful.</span><br><span class="line"></span><br><span class="line">blink drive - Command execution complete.</span><br><span class="line"></span><br><span class="line">$ shmcli list drive slots -a = 0 -enc = 0 -verbose | grep 2000000D -i </span><br><span class="line"><span class="comment"># -a = 0 was SAS HBA index</span></span><br><span class="line"><span class="comment"># -enc = 0 was enclosure index</span></span><br><span class="line"></span><br><span class="line">Enc Slot Drwr/Slot Status Vendor  ProductId       Serial   Size   Lnk   Rev Blnk? Logical Vols       SAS Connections</span><br><span class="line">65    1 / 23         OK    HGST   HUH721010AL5200 2000000D 8.91TB 6G,6G LS14 YES,  5000cca27330a695,      n/a</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable led</span></span><br><span class="line">$ shmcli blink drive -d = 5000cca2736726c7 -off</span><br></pre></td></tr></table></figure>

<h4 id="SCSI-command"><a href="#SCSI-command" class="headerlink" title="SCSI command"></a>SCSI command</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># looks like solt number could be map</span></span><br><span class="line"></span><br><span class="line">$ sas3ircu 0 display | grep c3b1 -A 5 -B 3</span><br><span class="line"></span><br><span class="line">Device is a Hard disk</span><br><span class="line">  Enclosure <span class="comment">#                             : 2</span></span><br><span class="line">  Slot <span class="comment">#                                  : 49</span></span><br><span class="line">  SAS Address                             : 5000cca-2-6b90-c3b1</span><br><span class="line">  State                                   : Ready (RDY)</span><br><span class="line">  Size (<span class="keyword">in</span> MB)/(<span class="keyword">in</span> sectors)               : 9537535/19532873727</span><br><span class="line">  Manufacturer                            : HGST</span><br><span class="line">  Model Number                            : HUH721010AL5200</span><br><span class="line">  Firmware Revision                       : A384</span><br><span class="line"></span><br><span class="line">$ sg_ses /dev/sg52 -p0x2 -j | grep c3b1  -i -A 8</span><br><span class="line">join_work: oi=0, ei=9, eiioe=1 not <span class="keyword">in</span> join_arr</span><br><span class="line">Drive Slot <span class="comment">#50_5000CCA26B90C3B1 [0,49]  Element type: Array device slot</span></span><br><span class="line">  Enclosure Status:</span><br><span class="line">    Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">    OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">    In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">    App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">    Ready to insert=0, RMV=0, Ident=0, Report=0                           <span class="comment"># here is the device status</span></span><br><span class="line">    App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">    Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line"></span><br><span class="line">$ $ lsscsi -git</span><br><span class="line">[0:0:0:0]    disk    sas:0x500056b3787358c0          /dev/sda   -  /dev/sg0</span><br><span class="line">[0:0:1:0]    disk    sas:0x500056b3787358c1          /dev/sdb   -  /dev/sg1</span><br><span class="line">[0:0:2:0]    disk    sas:0x500056b3787358c2          /dev/sdc   -  /dev/sg2</span><br><span class="line">[0:0:3:0]    disk    sas:0x500056b3787358c3          /dev/sdd   -  /dev/sg3</span><br><span class="line">[0:0:4:0]    disk    sas:0x500056b3787358c4          /dev/sde   -  /dev/sg4</span><br><span class="line">[0:0:5:0]    disk    sas:0x500056b3787358c5          /dev/sdf   -  /dev/sg5</span><br><span class="line">[0:0:6:0]    disk    sas:0x500056b3787358c6          /dev/sdg   -  /dev/sg6</span><br><span class="line">[0:0:7:0]    disk    sas:0x500056b3787358c7          /dev/sdh   -  /dev/sg7</span><br><span class="line">[0:0:8:0]    disk    sas:0x500056b3787358c8          /dev/sdi   -  /dev/sg8</span><br><span class="line">[0:0:9:0]    disk    sas:0x500056b3787358c9          /dev/sdj   -  /dev/sg9</span><br><span class="line">[0:0:10:0]   disk    sas:0x500056b3787358ca          /dev/sdk   -  /dev/sg10</span><br><span class="line">[0:0:11:0]   disk    sas:0x500056b3787358cb          /dev/sdl   -  /dev/sg11</span><br><span class="line">[0:0:12:0]   enclosu sas:0x500056b3787358fd          -          -  /dev/sg12</span><br><span class="line">[15:0:0:0]   disk    sata:                           /dev/sdm   -  /dev/sg13</span><br><span class="line">[17:0:0:0]   process sata:                           -          -  /dev/sg14</span><br><span class="line"></span><br><span class="line">$ sg_ses -p 0xa /dev/sg12 |grep -E <span class="string">'Element|slot'</span></span><br><span class="line">    &lt;&lt;&lt;additional: response too short&gt;&gt;&gt;</span><br><span class="line">    Element <span class="built_in">type</span>: Array device slot, subenclosure id: 0 [ti=0]</span><br><span class="line">      Element index: 0  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 0</span><br><span class="line">      Element index: 1  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 1</span><br><span class="line">      Element index: 2  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 2</span><br><span class="line">      Element index: 3  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 3</span><br><span class="line">      Element index: 4  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 4</span><br><span class="line">      Element index: 5  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 5</span><br><span class="line">      Element index: 6  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 6</span><br><span class="line">      Element index: 7  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 7</span><br><span class="line">      Element index: 8  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 8</span><br><span class="line">      Element index: 9  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 9</span><br><span class="line">      Element index: 10  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 10</span><br><span class="line">      Element index: 11  eiioe=0</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 11</span><br><span class="line"></span><br><span class="line"><span class="comment"># the light color depends hardware vendor</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--clear, --get, --<span class="built_in">set</span> acronyms <span class="keyword">for</span> Enclosure Status/Control [<span class="string">'es'</span> or <span class="string">'ec'</span>] page:</span><br><span class="line">    active  [Device slot] [2:7:1]</span><br><span class="line">    active  [Array device slot] [2:7:1]</span><br><span class="line">    <span class="built_in">disable</span>  [*] [0:5:1]</span><br><span class="line">    devoff  [Device slot] [3:4:1]</span><br><span class="line">    devoff  [Array device slot] [3:4:1]</span><br><span class="line">    dnr  [Device slot] [2:6:1]</span><br><span class="line">    dnr  [Array device slot] [2:6:1]</span><br><span class="line">    fault  [Device slot] [3:5:1]</span><br><span class="line">    fault  [Array device slot] [3:5:1]</span><br><span class="line">    ident  [Device slot] [2:1:1]</span><br><span class="line">    ident  [Array device slot] [2:1:1]</span><br><span class="line">    ident  [Power supply] [1:7:1]</span><br><span class="line">    ident  [Cooling] [1:7:1]</span><br><span class="line">    ident  [Enclosure] [1:7:1]</span><br><span class="line">    insert  [Device slot] [2:3:1]</span><br><span class="line">    insert  [Array device slot] [2:3:1]</span><br><span class="line">    locate  [Device slot] [2:1:1]</span><br><span class="line">    locate  [Array device slot] [2:1:1]</span><br><span class="line">    locate  [Power supply] [1:7:1]</span><br><span class="line">    locate  [Cooling] [1:7:1]</span><br><span class="line">    locate  [Enclosure] [1:7:1]</span><br><span class="line">    missing  [Device slot] [2:4:1]</span><br><span class="line">    missing  [Array device slot] [2:4:1]</span><br><span class="line">    locate  [Device slot] [2:1:1]</span><br><span class="line">    locate  [Array device slot] [2:1:1]</span><br><span class="line">    prdfail  [*] [0:6:1]</span><br><span class="line">    remove  [Device slot] [2:2:1]</span><br><span class="line">    remove  [Array device slot] [2:2:1]</span><br><span class="line">    speed_act  [Cooling] [2:7:8]</span><br><span class="line">    speed_code  [Cooling] [3:2:3]</span><br><span class="line">    swap  [*] [0:4:1]</span><br><span class="line"></span><br><span class="line">--clear, --get, --<span class="built_in">set</span> acronyms <span class="keyword">for</span> Threshold In/Out [<span class="string">'th'</span>] page:</span><br><span class="line">    high_crit  [*] [0:7:8]</span><br><span class="line">    high_warn  [*] [1:7:8]</span><br><span class="line">    low_crit  [*] [2:7:8]</span><br><span class="line">    low_warn  [*] [3:7:8]</span><br><span class="line"></span><br><span class="line">--get acronyms <span class="keyword">for</span> Additional Element Status [<span class="string">'aes'</span>] page (SAS EIP=1):</span><br><span class="line">    at_sas_addr  [*] [20:7:64]</span><br><span class="line">    dev_type  [*] [8:6:3]</span><br><span class="line">    phy_id  [*] [28:7:8]</span><br><span class="line">    sas_addr  [*] [12:7:64]</span><br><span class="line">    sata_dev  [*] [11:0:1]</span><br><span class="line">    sata_port_sel  [*] [11:7:1]</span><br><span class="line">    smp_init  [*] [10:1:1]</span><br><span class="line">    smp_targ  [*] [11:1:1]</span><br><span class="line">    ssp_init  [*] [10:3:1]</span><br><span class="line">    ssp_targ  [*] [11:3:1]</span><br><span class="line">    stp_init  [*] [10:2:1]</span><br><span class="line">    stp_targ  [*] [11:2:1]</span><br><span class="line"></span><br><span class="line">$ sg_ses --index=7 --setl=ident /dev/sg12</span><br><span class="line"><span class="comment">### rebuild , green led blink</span></span><br><span class="line">$ sg_ses --index=7 --<span class="built_in">set</span>=1:1:1 /dev/sg12</span><br><span class="line">$ sg_ses --index=7 --<span class="built_in">set</span>=1:1:1 /dev/sg12</span><br><span class="line"><span class="comment">### fault , red led blink</span></span><br><span class="line">$ sg_ses --index=7 --<span class="built_in">set</span>=fault /dev/sg12</span><br><span class="line">$ sg_ses --index=7 --clear=fault /dev/sg12</span><br><span class="line"><span class="comment">### prdfail, orange and green blink by turn</span></span><br><span class="line">$ sg_ses --index=7 --<span class="built_in">set</span>=prdfail /dev/sg12</span><br><span class="line">$ sg_ses --index=7 --clear=prdfail /dev/sg12</span><br><span class="line"><span class="comment">### locate not work</span></span><br><span class="line">$ sg_ses --index=7 --<span class="built_in">set</span>=2:1:1 /dev/sg12</span><br><span class="line">$ sg_ses --index=7 --clear=2:1:1 /dev/sg12</span><br></pre></td></tr></table></figure>
<p>Element Customization<br>Array device slot control element</p>
<table>
<thead>
<tr>
<th>Byte Bit</th>
<th align="center">7</th>
<th align="right">6</th>
<th align="right">5</th>
<th align="right">4</th>
<th align="right">3</th>
<th align="right">2</th>
<th align="right">1</th>
<th align="right">0</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td align="center">select</td>
<td align="right">PRDFAIL</td>
<td align="right">DISABLED</td>
<td align="right">RST SWAP</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
</tr>
<tr>
<td>1</td>
<td align="center">RQST OK</td>
<td align="right">RQSD RSVE DEV</td>
<td align="right">RQST HOT SPA</td>
<td align="right">RQST CONS CHK</td>
<td align="right">RQST IN CRIT ARR</td>
<td align="right">RQST IN FAILED ARR</td>
<td align="right">RQST REBUILD/REMAP</td>
<td align="right">RQST R/R ABORT</td>
</tr>
<tr>
<td>2</td>
<td align="center">RQST active</td>
<td align="right">DO NOT REMOVE</td>
<td align="right">RESERVED</td>
<td align="right">RQST MISSING</td>
<td align="right">RQST REMOVE</td>
<td align="right">RQST REMOVE</td>
<td align="right">RQST IDENT</td>
<td align="right">RESERVED</td>
</tr>
<tr>
<td>3</td>
<td align="center">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RQST FAULT</td>
<td align="right">DEVICE OFF</td>
<td align="right">ENABLE BYP B</td>
<td align="right">ENABLE BYP B</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
</tr>
</tbody></table>
<h4 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### view pages from the enclosure</span></span><br><span class="line">$ sg_ses -p 0 /dev/sg103</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lsscsi -git</span><br><span class="line">[15:0:50:0]  enclosu sas:0x5000ccab0202777d          -          -  /dev/sg103</span><br><span class="line"></span><br><span class="line">$ sg_ses -p 2 /dev/sg103</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">        Actual speed=0 rpm, Fan stopped</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line">        Actual speed=4100 rpm, Fan at lowest speed</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line">    Element <span class="built_in">type</span>: Temperature sensor, subenclosure id: 0 [ti=3]</span><br><span class="line">        Temperature: &lt;reserved&gt;</span><br><span class="line">        Temperature=32 C</span><br><span class="line">        Temperature=30 C</span><br><span class="line">        Temperature=37 C</span><br><span class="line">        Temperature=59 C</span><br><span class="line">        Temperature=31 C</span><br><span class="line">        Temperature=32 C</span><br><span class="line">        Temperature=32 C</span><br><span class="line">        Temperature=57 C</span><br><span class="line">        Temperature=21 C</span><br><span class="line">        Temperature=21 C</span><br><span class="line">        Temperature=45 C</span><br><span class="line">        Temperature=29 C</span><br><span class="line">        Temperature=35 C</span><br><span class="line">        Temperature=47 C</span><br><span class="line">        Temperature=31 C</span><br><span class="line">        Temperature=36 C</span><br><span class="line"></span><br><span class="line">$  sg_ses -p 2 /dev/sg103 -r</span><br><span class="line">        00 00 00 00 05 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        05 00 00 00 05 00 00 00  05 00 00 00 05 00 00 00</span><br><span class="line">        05 00 00 00 05 00 00 00  05 00 00 00 05 00 00 00</span><br><span class="line">        05 00 00 00 05 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 00 00 00  01 01 9a 01 01 01 95 01</span><br><span class="line">        01 01 9a 01 01 01 9a 01  01 01 9a 01 01 01 9a 01</span><br><span class="line">        01 00 00 00 01 00 34 00  01 00 32 00 01 00 39 00</span><br><span class="line">        01 00 50 00 01 00 33 00  01 00 34 00 01 00 34 00</span><br><span class="line">        01 00 4e 00 01 00 29 00  01 00 29 00 01 00 41 00</span><br><span class="line">        01 00 31 00 01 00 37 00  01 00 43 00 01 00 33 00</span><br><span class="line">        01 00 38 00 01 00 00 00  01 00 01 80 01 00 01 80</span><br><span class="line">        01 00 00 00 01 00 00 00  01 00 00 00 01 00 00 5f</span><br><span class="line">        01 00 00 64 01 00 00 b6  01 00 01 4c 01 00 00 5c</span><br><span class="line">        01 00 00 63 01 00 00 b4  01 00 01 46 01 00 04 bf</span><br><span class="line">        01 00 04 bf 01 00 00 00  01 00 00 00 01 00 00 00</span><br><span class="line">        01 00 00 00 01 03 ff 00  01 03 ff 00 01 03 ff 00</span><br><span class="line">        01 03 ff 00</span><br><span class="line"></span><br><span class="line">$ sg_ses /dev/sg129 -p0x2 -I2,1 -j</span><br><span class="line">first 2 menas Fan,1 is power,3 is temperature</span><br><span class="line">-I2,0 means the first Fan, -I 2,1 means second fan</span><br><span class="line"></span><br><span class="line">$ sg_ses -p 0 /dev/sg343</span><br><span class="line">  SMC946    R39                 1C</span><br><span class="line">Supported diagnostic pages:</span><br><span class="line">  Supported Diagnostic Pages [sdp] [0x0]</span><br><span class="line">  Configuration (SES) [cf] [0x1]</span><br><span class="line">  Enclosure Status/Control (SES) [ec,es] [0x2]</span><br><span class="line">  String In/Out (SES) [str] [0x4]</span><br><span class="line">  Threshold In/Out (SES) [th] [0x5]</span><br><span class="line">  Element Descriptor (SES) [ed] [0x7]</span><br><span class="line">  Additional Element Status (SES-2) [aes] [0xa]</span><br><span class="line">  Supported SES Diagnostic Pages (SES-2) [ssp] [0xd]</span><br><span class="line">  Download Microcode (SES-2) [dm] [0xe]</span><br><span class="line">  Subenclosure Nickname (SES-2) [snic] [0xf]</span><br><span class="line"></span><br><span class="line">$ sg_ses -p 0 /dev/sg103</span><br><span class="line">  HGST      4U60_STOR_ENCL    0210</span><br><span class="line">Supported diagnostic pages:</span><br><span class="line">  Supported Diagnostic Pages [sdp] [0x0]</span><br><span class="line">  Configuration (SES) [cf] [0x1]</span><br><span class="line">  Enclosure Status/Control (SES) [ec,es] [0x2]</span><br><span class="line">  String In/Out (SES) [str] [0x4]</span><br><span class="line">  Threshold In/Out (SES) [th] [0x5]</span><br><span class="line">  Element Descriptor (SES) [ed] [0x7]</span><br><span class="line">  Additional Element Status (SES-2) [aes] [0xa]</span><br><span class="line">  Download Microcode (SES-2) [dm] [0xe]</span><br><span class="line">  &lt;unknown&gt; [0x10]</span><br><span class="line">  &lt;unknown&gt; [0x11]</span><br><span class="line">  &lt;unknown&gt; [0x12]</span><br><span class="line">  &lt;unknown&gt; [0x13]</span><br><span class="line">  &lt;unknown&gt; [0x14]</span><br><span class="line">  &lt;unknown&gt; [0x15]</span><br><span class="line">  &lt;unknown&gt; [0x16]</span><br><span class="line">$ sg_ses -p 0 /dev/sg91</span><br><span class="line">  DELL      EN-8435A-E6EBD    3535</span><br><span class="line">Supported diagnostic pages:</span><br><span class="line">  Supported Diagnostic Pages [sdp] [0x0]</span><br><span class="line">  Configuration (SES) [cf] [0x1]</span><br><span class="line">  Enclosure Status/Control (SES) [ec,es] [0x2]</span><br><span class="line">  Threshold In/Out (SES) [th] [0x5]</span><br><span class="line">  Element Descriptor (SES) [ed] [0x7]</span><br><span class="line">  Additional Element Status (SES-2) [aes] [0xa]</span><br><span class="line">  Download Microcode (SES-2) [dm] [0xe]</span><br><span class="line">  &lt;unknown&gt; [0x84]</span><br><span class="line">  &lt;unknown&gt; [0x85]</span><br><span class="line">  &lt;unknown&gt; [0x90]</span><br><span class="line">  &lt;unknown&gt; [0x91]</span><br><span class="line">  &lt;unknown&gt; [0x92]</span><br><span class="line">  &lt;unknown&gt; [0x93]</span><br><span class="line">$ sg_ses -p 0 /dev/sg335</span><br><span class="line">  HPE       D6020             1.63</span><br><span class="line">Supported diagnostic pages:</span><br><span class="line">  Supported Diagnostic Pages [sdp] [0x0]</span><br><span class="line">  Configuration (SES) [cf] [0x1]</span><br><span class="line">  Enclosure Status/Control (SES) [ec,es] [0x2]</span><br><span class="line">  Help Text (SES) [ht] [0x3]</span><br><span class="line">  Element Descriptor (SES) [ed] [0x7]</span><br><span class="line">  Additional Element Status (SES-2) [aes] [0xa]</span><br><span class="line">  &lt;unknown&gt; [0x10]</span><br><span class="line">  &lt;unknown&gt; [0x14]</span><br><span class="line"></span><br><span class="line">sg_ses -ee /dev/sg291</span><br><span class="line">&gt;&gt;&gt; DEVICE /dev/sg291 ignored when --enumerate option given.</span><br><span class="line">--clear, --get, --<span class="built_in">set</span> acronyms <span class="keyword">for</span> Enclosure Status/Control [<span class="string">'es'</span> or <span class="string">'ec'</span>] page:</span><br><span class="line">    active  [Device slot] [2:7:1]</span><br><span class="line">    active  [Array device slot] [2:7:1]</span><br><span class="line">    <span class="built_in">disable</span>  [*] [0:5:1]</span><br><span class="line">    devoff  [Device slot] [3:4:1]</span><br><span class="line">    devoff  [Array device slot] [3:4:1]</span><br><span class="line">    dnr  [Device slot] [2:6:1]</span><br><span class="line">    dnr  [Array device slot] [2:6:1]</span><br><span class="line">    fault  [Device slot] [3:5:1]</span><br><span class="line">    fault  [Array device slot] [3:5:1]</span><br><span class="line">    ident  [Device slot] [2:1:1]</span><br><span class="line">    ident  [Array device slot] [2:1:1]</span><br><span class="line">    ident  [Power supply] [1:7:1]</span><br><span class="line">    ident  [Cooling] [1:7:1]</span><br><span class="line">    ident  [Enclosure] [1:7:1]</span><br><span class="line">    insert  [Device slot] [2:3:1]</span><br><span class="line">    insert  [Array device slot] [2:3:1]</span><br><span class="line">    locate  [Device slot] [2:1:1]</span><br><span class="line">    locate  [Array device slot] [2:1:1]</span><br><span class="line">    locate  [Power supply] [1:7:1]</span><br><span class="line">    locate  [Cooling] [1:7:1]</span><br><span class="line">    locate  [Enclosure] [1:7:1]</span><br><span class="line">    missing  [Device slot] [2:4:1]</span><br><span class="line">    missing  [Array device slot] [2:4:1]</span><br><span class="line">    locate  [Device slot] [2:1:1]</span><br><span class="line">    locate  [Array device slot] [2:1:1]</span><br><span class="line">    prdfail  [*] [0:6:1]</span><br><span class="line">    remove  [Device slot] [2:2:1]</span><br><span class="line">    remove  [Array device slot] [2:2:1]</span><br><span class="line">    speed_act  [Cooling] [2:7:8]</span><br><span class="line">    speed_code  [Cooling] [3:2:3]</span><br><span class="line">    swap  [*] [0:4:1]</span><br><span class="line"></span><br><span class="line">--clear, --get, --<span class="built_in">set</span> acronyms <span class="keyword">for</span> Threshold In/Out [<span class="string">'th'</span>] page:</span><br><span class="line">    high_crit  [*] [0:7:8]</span><br><span class="line">    high_warn  [*] [1:7:8]</span><br><span class="line">    low_crit  [*] [2:7:8]</span><br><span class="line">    low_warn  [*] [3:7:8]</span><br><span class="line"></span><br><span class="line">--get acronyms <span class="keyword">for</span> Additional Element Status [<span class="string">'aes'</span>] page (SAS EIP=1):</span><br><span class="line">    at_sas_addr  [*] [20:7:64]</span><br><span class="line">    dev_type  [*] [8:6:3]</span><br><span class="line">    phy_id  [*] [28:7:8]</span><br><span class="line">    sas_addr  [*] [12:7:64]</span><br><span class="line">    sata_dev  [*] [11:0:1]</span><br><span class="line">    sata_port_sel  [*] [11:7:1]</span><br><span class="line">    smp_init  [*] [10:1:1]</span><br><span class="line">    smp_targ  [*] [11:1:1]</span><br><span class="line">    ssp_init  [*] [10:3:1]</span><br><span class="line">    ssp_targ  [*] [11:3:1]</span><br><span class="line">    stp_init  [*] [10:2:1]</span><br><span class="line">    stp_targ  [*] [11:2:1]</span><br><span class="line"></span><br><span class="line">$ sg_ses -es /dev/sg291</span><br><span class="line">&gt;&gt;&gt; DEVICE /dev/sg291 ignored when --enumerate option given.</span><br><span class="line">Diagnostic pages, followed by abbreviation(s) <span class="keyword">then</span> page code:</span><br><span class="line">    Supported Diagnostic Pages  [sdp] [0x0]</span><br><span class="line">    Configuration (SES)  [cf] [0x1]</span><br><span class="line">    Enclosure Status/Control (SES)  [ec,es] [0x2]</span><br><span class="line">    Help Text (SES)  [ht] [0x3]</span><br><span class="line">    String In/Out (SES)  [str] [0x4]</span><br><span class="line">    Threshold In/Out (SES)  [th] [0x5]</span><br><span class="line">    Array Status/Control (SES, obsolete)  [ac,as] [0x6]</span><br><span class="line">    Element Descriptor (SES)  [ed] [0x7]</span><br><span class="line">    Short Enclosure Status (SES)  [ses] [0x8]</span><br><span class="line">    Enclosure Busy (SES-2)  [eb] [0x9]</span><br><span class="line">    Additional Element Status (SES-2)  [aes] [0xa]</span><br><span class="line">    Subenclosure Help Text (SES-2)  [sht] [0xb]</span><br><span class="line">    Subenclosure String In/Out (SES-2)  [sstr] [0xc]</span><br><span class="line">    Supported SES Diagnostic Pages (SES-2)  [ssp] [0xd]</span><br><span class="line">    Download Microcode (SES-2)  [dm] [0xe]</span><br><span class="line">    Subenclosure Nickname (SES-2)  [snic] [0xf]</span><br><span class="line">    Protocol Specific (SAS transport)  [] [0x3f]</span><br><span class="line">    Translate Address (SBC)  [] [0x40]</span><br><span class="line">    Device Status (SBC)  [] [0x41]</span><br><span class="line">    Rebuild Assist (SBC)  [] [0x42]</span><br><span class="line"></span><br><span class="line">SES element <span class="built_in">type</span> names, followed by abbreviation and element <span class="built_in">type</span> code:</span><br><span class="line">    Unspecified  [un] [0x0]</span><br><span class="line">    Device slot  [dev] [0x1]</span><br><span class="line">    Power supply  [ps] [0x2]</span><br><span class="line">    Cooling  [coo] [0x3]</span><br><span class="line">    Temperature sensor  [ts] [0x4]</span><br><span class="line">    Door  [<span class="keyword">do</span>] [0x5]</span><br><span class="line">    Audible alarm  [aa] [0x6]</span><br><span class="line">    Enclosure services controller electronics  [esc] [0x7]</span><br><span class="line">    SCC controller electronics  [sce] [0x8]</span><br><span class="line">    Nonvolatile cache  [nc] [0x9]</span><br><span class="line">    Invalid operation reason  [ior] [0xa]</span><br><span class="line">    Uninterruptible power supply  [ups] [0xb]</span><br><span class="line">    Display  [dis] [0xc]</span><br><span class="line">    Key pad entry  [kpe] [0xd]</span><br><span class="line">    Enclosure  [enc] [0xe]</span><br><span class="line">    SCSI port/transceiver  [sp] [0xf]</span><br><span class="line">    Language  [lan] [0x10]</span><br><span class="line">    Communication port  [cp] [0x11]</span><br><span class="line">    Voltage sensor  [vs] [0x12]</span><br><span class="line">    Current sensor  [cs] [0x13]</span><br><span class="line">    SCSI target port  [stp] [0x14]</span><br><span class="line">    SCSI initiator port  [sip] [0x15]</span><br><span class="line">    Simple subenclosure  [ss] [0x16]</span><br><span class="line">    Array device slot  [arr] [0x17]</span><br><span class="line">    SAS expander  [sse] [0x18]</span><br><span class="line">    SAS connector  [ssc] [0x19]</span><br><span class="line"></span><br><span class="line">[13:0:90:0]  disk    HGST     HUH721010AL5200  A21D  /dev/sdji  35000cca251ae9e94  /dev/sg278  10.0TB</span><br><span class="line">[13:0:91:0]  disk    HGST     HUH721010AL5200  A21D  /dev/sdmv  35000cca251ac584c  /dev/sg372  10.0TB</span><br><span class="line">[13:0:92:0]  disk    HGST     HUH721010AL5200  A21D  /dev/sdmw  35000cca251aca1b4  /dev/sg373  10.0TB</span><br><span class="line">[14:0:0:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjj  35000cca251b137a0  /dev/sg279  10.0TB</span><br><span class="line">[14:0:1:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjk  35000cca251b29994  /dev/sg280  10.0TB</span><br><span class="line">[14:0:2:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjl  35000cca251ab4d30  /dev/sg281  10.0TB</span><br><span class="line">[14:0:3:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjm  35000cca25198c678  /dev/sg282  10.0TB</span><br><span class="line">[14:0:4:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjn  35000cca251b12a14  /dev/sg283  10.0TB</span><br><span class="line">[14:0:5:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjo  35000cca251ac5a14  /dev/sg284  10.0TB</span><br><span class="line">[14:0:6:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjp  35000cca251b21390  /dev/sg285  10.0TB</span><br><span class="line">[14:0:7:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjq  35000cca251b25234  /dev/sg286  10.0TB</span><br><span class="line">[14:0:8:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjr  35000cca251b2f228  /dev/sg287  10.0TB</span><br><span class="line">[14:0:9:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdjs  35000cca251b380d8  /dev/sg288  10.0TB</span><br><span class="line">[14:0:10:0]  disk    HGST     HUH721010AL5200  A21D  /dev/sdjt  35000cca251b272ec  /dev/sg289  10.0TB</span><br><span class="line">[14:0:11:0]  disk    HGST     HUH721010AL5200  A21D  /dev/sdju  35000cca251b13860  /dev/sg290  10.0TB</span><br><span class="line">[14:0:12:0]  enclosu SMC946   12                 1C  -          -  /dev/sg291</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get all descriptor</span></span><br><span class="line">$ sg_ses -p 0x7 /dev/sg291</span><br><span class="line">  SMC946    12                  1C</span><br><span class="line">  Primary enclosure logical identifier (hex): 5003048009281a3f</span><br><span class="line">Element Descriptor In diagnostic page:</span><br><span class="line">  generation code: 0x0</span><br><span class="line">  element descriptor by <span class="built_in">type</span> list</span><br><span class="line">    Element <span class="built_in">type</span>: Array device slot, subenclosure id: 0 [ti=0]</span><br><span class="line">      Overall descriptor: ArrayDevicesInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: Slot00</span><br><span class="line">      Element 1 descriptor: Slot01</span><br><span class="line">      Element 2 descriptor: Slot02</span><br><span class="line">      Element 3 descriptor: Slot03</span><br><span class="line">      Element 4 descriptor: Slot04</span><br><span class="line">      Element 5 descriptor: Slot05</span><br><span class="line">      Element 6 descriptor: Slot06</span><br><span class="line">      Element 7 descriptor: Slot07</span><br><span class="line">      Element 8 descriptor: Slot08</span><br><span class="line">      Element 9 descriptor: Slot09</span><br><span class="line">      Element 10 descriptor: Slot10</span><br><span class="line">      Element 11 descriptor: Slot11</span><br><span class="line">    Element <span class="built_in">type</span>: SAS expander, subenclosure id: 0 [ti=1]</span><br><span class="line">      Overall descriptor: SAS Expander</span><br><span class="line">      Element 0 descriptor: Expander0</span><br><span class="line">    Element <span class="built_in">type</span>: SAS connector, subenclosure id: 0 [ti=2]</span><br><span class="line">      Overall descriptor: SAS Connectors</span><br><span class="line">      Element 0 descriptor: ConnectorA</span><br><span class="line">      Element 1 descriptor: ConnectorB</span><br><span class="line">      Element 2 descriptor: ConnectorC</span><br><span class="line">      Element 3 descriptor: ConnectorD</span><br><span class="line">      Element 4 descriptor: Drive Connector 00</span><br><span class="line">      Element 5 descriptor: Drive Connector 01</span><br><span class="line">      Element 6 descriptor: Drive Connector 02</span><br><span class="line">      Element 7 descriptor: Drive Connector 03</span><br><span class="line">      Element 8 descriptor: Drive Connector 04</span><br><span class="line">      Element 9 descriptor: Drive Connector 05</span><br><span class="line">      Element 10 descriptor: Drive Connector 06</span><br><span class="line">      Element 11 descriptor: Drive Connector 07</span><br><span class="line">      Element 12 descriptor: Drive Connector 08</span><br><span class="line">      Element 13 descriptor: Drive Connector 09</span><br><span class="line">      Element 14 descriptor: Drive Connector 10</span><br><span class="line">      Element 15 descriptor: Drive Connector 11</span><br><span class="line">    Element <span class="built_in">type</span>: Enclosure, subenclosure id: 0 [ti=3]</span><br><span class="line">      Overall descriptor: EnclosureElementInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: RD-BHE-000-02343</span><br><span class="line">    Element <span class="built_in">type</span>: Temperature sensor, subenclosure id: 0 [ti=4]</span><br><span class="line">      Overall descriptor: TempSensorsInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: ChipDie</span><br><span class="line">      Element 1 descriptor: VRM1</span><br><span class="line">      Element 2 descriptor: VRM2</span><br><span class="line">      Element 3 descriptor: SYSTemp</span><br><span class="line">      Element 4 descriptor: HDDBPN</span><br><span class="line">    Element <span class="built_in">type</span>: Voltage sensor, subenclosure id: 0 [ti=5]</span><br><span class="line">      Overall descriptor: VoltageSensorsInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: 12VSB</span><br><span class="line">      Element 1 descriptor: 3P3VSB</span><br><span class="line">      Element 2 descriptor: 1P2BMC</span><br><span class="line">      Element 3 descriptor: 1P5BMC</span><br><span class="line">      Element 4 descriptor: VBAT</span><br><span class="line">      Element 5 descriptor: 12VCC</span><br><span class="line">      Element 6 descriptor: 3P3VCC</span><br><span class="line">      Element 7 descriptor: 1P8VEXP1</span><br><span class="line">      Element 8 descriptor: 0P9VEXP1</span><br><span class="line">      Element 9 descriptor: 0P9VREXP1</span><br><span class="line">      Element 10 descriptor: 1P8VEXP2</span><br><span class="line">      Element 11 descriptor: 0P9VEXP2</span><br><span class="line">      Element 12 descriptor: 0P9VREXP2</span><br><span class="line">      Element 13 descriptor: 1P8VEXP3</span><br><span class="line">      Element 14 descriptor: 0P9VEXP3</span><br><span class="line">      Element 15 descriptor: 0P9VREXP3</span><br><span class="line">    Element <span class="built_in">type</span>: Cooling, subenclosure id: 0 [ti=6]</span><br><span class="line">      Overall descriptor: CoolingElementInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: Fan1</span><br><span class="line">      Element 1 descriptor: Fan2</span><br><span class="line">      Element 2 descriptor: Fan3</span><br><span class="line">      Element 3 descriptor: Fan4</span><br><span class="line">      Element 4 descriptor: Fan5</span><br><span class="line">      Element 5 descriptor: Fan1C</span><br><span class="line">      Element 6 descriptor: Fan2C</span><br><span class="line">      Element 7 descriptor: Fan3C</span><br><span class="line">      Element 8 descriptor: Fan4C</span><br><span class="line">      Element 9 descriptor: Fan5C</span><br><span class="line">    Element <span class="built_in">type</span>: Power supply, subenclosure id: 0 [ti=7]</span><br><span class="line">      Overall descriptor: PowerSupplyStatus</span><br><span class="line">      Element 0 descriptor: PWS1</span><br><span class="line">      Element 1 descriptor: PWS2</span><br><span class="line">      Element 2 descriptor: PWS3</span><br><span class="line">      Element 3 descriptor: PWS4</span><br><span class="line"></span><br><span class="line">$  sg_ses -p 0x2 /dev/sg291</span><br><span class="line">  SMC946    12                  1C</span><br><span class="line">  Primary enclosure logical identifier (hex): 5003048009281a3f</span><br><span class="line">Enclosure Status diagnostic page:</span><br><span class="line">  INVOP=0, INFO=0, NON-CRIT=0, CRIT=1, UNRECOV=0</span><br><span class="line">  generation code: 0x0</span><br><span class="line">  status descriptor list</span><br><span class="line">    Element <span class="built_in">type</span>: Array device slot, subenclosure id: 0 [ti=0]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 4 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 5 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 6 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 7 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 8 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 9 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 10 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">      Element 11 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        OK=0, Reserved device=0, Hot spare=0, Cons check=0</span><br><span class="line">        In crit array=0, In failed array=0, Rebuild/remap=0, R/R abort=0</span><br><span class="line">        App client bypass A=0, Do not remove=0, Enc bypass A=0, Enc bypass B=0</span><br><span class="line">        Ready to insert=0, RMV=0, Ident=0, Report=0</span><br><span class="line">        App client bypass B=0, Fault sensed=0, Fault reqstd=0, Device off=0</span><br><span class="line">        Bypassed A=0, Bypassed B=0, Dev bypassed A=0, Dev bypassed B=0</span><br><span class="line">    Element <span class="built_in">type</span>: SAS expander, subenclosure id: 0 [ti=1]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, Fail=0</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0</span><br><span class="line">    Element <span class="built_in">type</span>: SAS connector, subenclosure id: 0 [ti=2]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, No information</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Mini SAS HD 4i receptacle (SFF-8643) [max 4 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Mini SAS HD 4i receptacle (SFF-8643) [max 4 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Mini SAS HD 4i receptacle (SFF-8643) [max 4 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Mini SAS HD 4i receptacle (SFF-8643) [max 4 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 4 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 5 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 6 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 7 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 8 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 9 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 10 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 11 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 12 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 13 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 14 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">      Element 15 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, SAS Drive backplane receptacle (SFF-8482) [max 2 phys]</span><br><span class="line">        Connector physical link=0x0, Fail=0</span><br><span class="line">    Element <span class="built_in">type</span>: Enclosure, subenclosure id: 0 [ti=3]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, Time until power cycle=0, Failure indication=0</span><br><span class="line">        Warning indication=0, Requested power off duration=0</span><br><span class="line">        Failure requested=0, Warning requested=0</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Time until power cycle=0, Failure indication=0</span><br><span class="line">        Warning indication=0, Requested power off duration=0</span><br><span class="line">        Failure requested=0, Warning requested=0</span><br><span class="line">    Element <span class="built_in">type</span>: Temperature sensor, subenclosure id: 0 [ti=4]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature: &lt;reserved&gt;</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature=79 C</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature=46 C</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature=43 C</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature=78 C</span><br><span class="line">      Element 4 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0, OT failure=0, OT warning=0, UT failure=0</span><br><span class="line">        UT warning=0</span><br><span class="line">        Temperature=29 C</span><br><span class="line">    Element <span class="built_in">type</span>: Voltage sensor, subenclosure id: 0 [ti=5]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.00 volts</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 12.20 volts</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 3.20 volts</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 1.20 volts</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 1.50 volts</span><br><span class="line">      Element 4 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 3.20 volts</span><br><span class="line">      Element 5 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 11.90 volts</span><br><span class="line">      Element 6 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 3.20 volts</span><br><span class="line">      Element 7 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 1.80 volts</span><br><span class="line">      Element 8 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.90 volts</span><br><span class="line">      Element 9 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.90 volts</span><br><span class="line">      Element 10 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 1.80 volts</span><br><span class="line">      Element 11 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.80 volts</span><br><span class="line">      Element 12 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.90 volts</span><br><span class="line">      Element 13 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 1.80 volts</span><br><span class="line">      Element 14 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.90 volts</span><br><span class="line">      Element 15 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Fail=0,  Warn Over=0, Warn Under=0, Crit Over=0</span><br><span class="line">        Crit Under=0</span><br><span class="line">        Voltage: 0.90 volts</span><br><span class="line">    Element <span class="built_in">type</span>: Cooling, subenclosure id: 0 [ti=6]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, Hot swap=0, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=0 rpm, Fan stopped</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=12000 rpm, Fan stopped</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11700 rpm, Fan stopped</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11100 rpm, Fan stopped</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11700 rpm, Fan stopped</span><br><span class="line">      Element 4 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11800 rpm, Fan stopped</span><br><span class="line">      Element 5 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11000 rpm, Fan stopped</span><br><span class="line">      Element 6 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=10900 rpm, Fan stopped</span><br><span class="line">      Element 7 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11900 rpm, Fan stopped</span><br><span class="line">      Element 8 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=10800 rpm, Fan stopped</span><br><span class="line">      Element 9 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=0, Hot swap=1, Fail=0, Requested on=0, Off=0</span><br><span class="line">        Actual speed=11000 rpm, Fan stopped</span><br><span class="line">    Element <span class="built_in">type</span>: Power supply, subenclosure id: 0 [ti=7]</span><br><span class="line">      Overall descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: Unsupported</span><br><span class="line">        Ident=0, DC overvoltage=0, DC undervoltage=0, DC overcurrent=0</span><br><span class="line">        Hot swap=0, Fail=0, Requested on=0, Off=0, Overtmp fail=0</span><br><span class="line">        Temperature warn=0, AC fail=0, DC fail=0</span><br><span class="line">      Element 0 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=1, DC overvoltage=0, DC undervoltage=0, DC overcurrent=0</span><br><span class="line">        Hot swap=1, Fail=0, Requested on=1, Off=0, Overtmp fail=0</span><br><span class="line">        Temperature warn=0, AC fail=0, DC fail=0</span><br><span class="line">      Element 1 descriptor:</span><br><span class="line">        Predicted failure=1, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=1, DC overvoltage=0, DC undervoltage=0, DC overcurrent=0</span><br><span class="line">        Hot swap=1, Fail=0, Requested on=1, Off=0, Overtmp fail=0</span><br><span class="line">        Temperature warn=0, AC fail=0, DC fail=0</span><br><span class="line">      Element 2 descriptor:</span><br><span class="line">        Predicted failure=1, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=1, DC overvoltage=0, DC undervoltage=0, DC overcurrent=0</span><br><span class="line">        Hot swap=1, Fail=0, Requested on=1, Off=0, Overtmp fail=0</span><br><span class="line">        Temperature warn=0, AC fail=0, DC fail=0</span><br><span class="line">      Element 3 descriptor:</span><br><span class="line">        Predicted failure=0, Disabled=0, Swap=0, status: OK</span><br><span class="line">        Ident=1, DC overvoltage=0, DC undervoltage=0, DC overcurrent=0</span><br><span class="line">        Hot swap=1, Fail=0, Requested on=1, Off=0, Overtmp fail=0</span><br><span class="line">        Temperature warn=0, AC fail=0, DC fail=0 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------------------------------</span><br><span class="line">[0:0:9:0]    disk    HGST     HUH721010AL5200  LS14  /dev/sdj   35000cca26c1b8630  /dev/sg9   10.0TB</span><br><span class="line"></span><br><span class="line">Element 9 descriptor: Drive Slot 9</span><br><span class="line"></span><br><span class="line">      Element index: 9  eiioe=0</span><br><span class="line">        Transport protocol: SAS</span><br><span class="line">        number of phys: 2, not all phys: 0, device slot number: 9</span><br><span class="line">        phy index: 0</span><br><span class="line">          SAS device <span class="built_in">type</span>: end device</span><br><span class="line">          initiator port <span class="keyword">for</span>:</span><br><span class="line">          target port <span class="keyword">for</span>: SSP</span><br><span class="line">          attached SAS address: 0x500056b3787358ff</span><br><span class="line">          SAS address: 0x5000cca26c1b8631</span><br><span class="line">          phy identifier: 0x0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn off the device slot power</span></span><br><span class="line">$ sg_ses --descriptor=<span class="string">"Drive Slot 9"</span> --<span class="built_in">set</span>=3:4:1 /dev/sg12</span><br><span class="line"></span><br><span class="line">$ sg_ses --descriptor=<span class="string">"Drive Slot 9"</span> --get=3:4:1 /dev/sg12</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn on the device slot power</span></span><br><span class="line">$ sg_ses --descriptor=<span class="string">"Drive Slot 9"</span> --clear=3:4:1 /dev/sg12</span><br><span class="line"></span><br><span class="line"><span class="comment">########## Power</span></span><br><span class="line">    Element <span class="built_in">type</span>: Power supply, subenclosure id: 0 [ti=7]</span><br><span class="line">      Overall descriptor: PowerSupplyStatus</span><br><span class="line">      Element 0 descriptor: PWS1</span><br><span class="line">      Element 1 descriptor: PWS2</span><br><span class="line">      Element 2 descriptor: PWS3</span><br><span class="line">      Element 3 descriptor: PWS4</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn off PWS1</span></span><br><span class="line">$ sg_ses --discriptor=<span class="string">"PWS1"</span> --clear=3:5:1 /dev/sg291</span><br><span class="line"></span><br><span class="line"><span class="comment"># Power on </span></span><br><span class="line">$ sg_ses --discriptor=<span class="string">"PWS1"</span> --<span class="built_in">set</span>=3:5:1 /dev/sg291</span><br><span class="line"></span><br><span class="line"><span class="comment">############ Fan</span></span><br><span class="line">    Element <span class="built_in">type</span>: Cooling, subenclosure id: 0 [ti=6]</span><br><span class="line">      Overall descriptor: CoolingElementInSubEnclsr0</span><br><span class="line">      Element 0 descriptor: Fan1</span><br><span class="line">      Element 1 descriptor: Fan2</span><br><span class="line">      Element 2 descriptor: Fan3</span><br><span class="line">      Element 3 descriptor: Fan4</span><br><span class="line">      Element 4 descriptor: Fan5</span><br><span class="line">      Element 5 descriptor: Fan1C</span><br><span class="line">      Element 6 descriptor: Fan2C</span><br><span class="line">      Element 7 descriptor: Fan3C</span><br><span class="line">      Element 8 descriptor: Fan4C</span><br><span class="line">      Element 9 descriptor: Fan5C</span><br><span class="line"></span><br><span class="line"><span class="comment">#Light the FAN led</span></span><br><span class="line">$ sg_ses --descriptor=Fan1C --<span class="built_in">set</span>=1:7:1 /dev/sg291</span><br><span class="line"></span><br><span class="line"><span class="comment">#REQUESTED SPEED MODE  PWM</span></span><br><span class="line">7 Full speed</span><br><span class="line">6 Second highest </span><br><span class="line">5 third highest</span><br><span class="line">4 Intermediate</span><br><span class="line">3 Third Loweest</span><br><span class="line">2 Second lowest</span><br><span class="line">1 Lowest speed</span><br><span class="line">0 Leave at current speed</span><br><span class="line"></span><br><span class="line">$ sg_ses --descriptor=Fan1C --<span class="built_in">set</span>=3:2:3=7 /dev/sg291</span><br><span class="line">$ sg_ses --descriptor=Fan1C --<span class="built_in">set</span>=3:2:3=5 /dev/sg291</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>Byte Bit</th>
<th align="center">7</th>
<th align="right">6</th>
<th align="right">5</th>
<th align="right">4</th>
<th align="right">3</th>
<th align="right">2</th>
<th align="right">1</th>
<th align="right">0</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td align="center">select</td>
<td align="right">PRDFAIL</td>
<td align="right">DISABLED</td>
<td align="right">RST SWAP</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
</tr>
<tr>
<td>1</td>
<td align="center">RQST IDENT</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
</tr>
<tr>
<td>2</td>
<td align="center">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
</tr>
<tr>
<td>3</td>
<td align="center">RESERVED</td>
<td align="right">RQST FAIL</td>
<td align="right">RQST ON</td>
<td align="right">RESERVED</td>
<td align="right">RESERVED</td>
<td align="right">REQUESTED SPEED CODE</td>
<td align="right">REQUESTED SPEED CODE</td>
<td align="right">REQUESTED SPEED CODE</td>
</tr>
</tbody></table>
<h4 id="Avoid-multiple-JBOD-devs-across-in-single-raidz"><a href="#Avoid-multiple-JBOD-devs-across-in-single-raidz" class="headerlink" title="Avoid multiple JBOD devs across in single raidz"></a>Avoid multiple JBOD devs across in single raidz</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tempdir=/tmp;</span><br><span class="line">lsscsi -tiv  | sed -z <span class="string">'s/\n  dir:/ /g'</span> | awk -v tempdir=<span class="string">"<span class="variable">$tempdir</span>"</span> -F <span class="string">'[:\\[/ ]+'</span> <span class="string">'$10!~/-/ &amp;&amp; $0~/disk/ &amp;&amp; $7~/sas/ &#123;print "/dev/disk/by-id/scsi-"$11,$2 &gt; tempdir"/"$30"_"$31"_"$32"_"$33"_"$34&#125;'</span></span><br></pre></td></tr></table></figure>

<h4 id="map-element-index-and-sas-addr"><a href="#map-element-index-and-sas-addr" class="headerlink" title="map element index and sas addr"></a>map element index and sas addr</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[0:0:12:0]   enclosu DP       BP14G+EXP        2.25  -          -  /dev/sg12</span><br><span class="line"></span><br><span class="line">$ sg_ses -p 0xa /dev/sg12 | awk <span class="string">'&#123;if($0!~/0x0000000000000000/ &amp;&amp; $0!~/attached SAS address/) &#123;if ($0~/Element index/) &#123;printf "Element_index "$3" "&#125;; if ($0~/number of phys/) &#123;printf "slot "$NF" "&#125;; if ($0~/SAS address/) &#123;print "sasaddr "$NF&#125; &#125;&#125;'</span></span><br><span class="line">Element_index 0 slot 0 sasaddr 0x500056b3787358c0</span><br><span class="line">Element_index 1 slot 1 sasaddr 0x500056b3787358c1</span><br><span class="line">Element_index 2 slot 2 sasaddr 0x500056b3787358c2</span><br><span class="line">Element_index 3 slot 3 sasaddr 0x500056b3787358c3</span><br><span class="line">Element_index 4 slot 4 sasaddr 0x500056b3787358c4</span><br><span class="line">Element_index 5 slot 5 sasaddr 0x500056b3787358c5</span><br><span class="line">Element_index 6 slot 6 sasaddr 0x500056b3787358c6</span><br><span class="line">Element_index 7 slot 7 sasaddr 0x500056b3787358c7</span><br><span class="line">Element_index 8 slot 8 sasaddr 0x500056b3787358c8</span><br><span class="line">Element_index 9 slot 9 sasaddr 0x500056b3787358c9</span><br><span class="line">Element_index 10 slot 10 sasaddr 0x500056b3787358ca</span><br></pre></td></tr></table></figure>


<h3 id="SATA"><a href="#SATA" class="headerlink" title="SATA"></a>SATA</h3><h4 id="Enable-SATA-TLER-ERC-CCTL"><a href="#Enable-SATA-TLER-ERC-CCTL" class="headerlink" title="Enable SATA TLER/ERC/CCTL"></a>Enable SATA TLER/ERC/CCTL</h4><p>Western Digital/HGST TLER - Time-Limited Error Recovery<br>Seagate ERC- Error Recovery Control<br>Hitachi/Samsung CCTL - Command Completion Time Limit</p>
<p>Streaming Commands<br>When the device is in standby mode, Streaming Commands can’t be completed<br>while waiting for the spindle to reach operating speed even if execution time<br>exceeds specified CCTL(Command Completion Time Limit). The minimum CCTL<br>is 50ms.CCTL is set to 50ms when the specified value is shorter than 50ms.</p>
<p>SCT<br>When the device is in standby mode, any command where error recovery time<br>limit is specified can’t be completed while waiting for the spindle to reach operating<br>speed even if execution time exceeds specified recovery time limit. The minimum<br>time limit is 6.5 second. When the specified time limit is shorter than 6.5 second,<br>the issued command is aborted.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lsscsi -gi | grep sdy</span><br><span class="line">[1:0:18:0]   disk    ATA      HGST HUH721212AL T3D0  /dev/sdy   -  /dev/sg25</span><br><span class="line"></span><br><span class="line">$ smartctl -l scterc,70,70 /dev/sdy</span><br><span class="line">smartctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-957.el7_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">SCT Error Recovery Control <span class="built_in">set</span> to:</span><br><span class="line">           Read:     70 (7.0 seconds)</span><br><span class="line">          Write:     70 (7.0 seconds)</span><br><span class="line"></span><br><span class="line">$ smartctl -l scterc /dev/sdy</span><br><span class="line">martctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-957.el7_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">SCT Error Recovery Control:</span><br><span class="line">           Read:     70 (7.0 seconds)</span><br><span class="line">          Write:     70 (7.0 seconds)</span><br></pre></td></tr></table></figure>

<h3 id="Get-hide-area-from-SATA-SSD"><a href="#Get-hide-area-from-SATA-SSD" class="headerlink" title="Get hide area from SATA SSD"></a><a href="https://tinyapps.org/docs/wipe_drives_hdparm.html" target="_blank" rel="noopener">Get hide area from SATA SSD</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hdparm -I &#x2F;dev&#x2F;sdx</span><br><span class="line">....</span><br><span class="line">Security:</span><br><span class="line">        Master password revision code &#x3D; 65534</span><br><span class="line">                supported</span><br><span class="line">        not     enabled</span><br><span class="line">        not     locked</span><br><span class="line">        not     frozen</span><br><span class="line">        not     expired: security count</span><br><span class="line">                supported: enhanced erase</span><br><span class="line">        4min for SECURITY ERASE UNIT. 4min for ENHANCED SECURITY ERASE UNIT.</span><br><span class="line"></span><br><span class="line"># set password</span><br><span class="line">$ hdparm --user-master u --security-set-pass p &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Erase the dev</span><br><span class="line">$ hdparm --user-master u --security-erase p &#x2F;dev&#x2F;sdx</span><br><span class="line"># if you dev support security-erase-enhanced, you could use security-erase-enhanced</span><br><span class="line">Secure erase overwrites all user data areas with binary zeroes. Enhanced secure erase writes predetermined data patterns (set by the manufacturer) to all user data areas, including sectors that are no longer in use due to reallocation. ***NOTE: the enhanced secure erase option is not supported by all ATA drives.</span><br><span class="line"></span><br><span class="line">$ after security-erase</span><br><span class="line">$ dd if&#x3D;&#x2F;dev&#x2F;sdx bs&#x3D;8192 status&#x3D;progress | hexdump</span><br><span class="line">0000000 0000 0000 0000 0000 0000 0000 0000 0000</span><br><span class="line"></span><br><span class="line"># if your dev has locked</span><br><span class="line">    locked</span><br><span class="line"></span><br><span class="line">#unlock</span><br><span class="line">$ hdparm --user-master u --security-unlock p &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">#disable security</span><br><span class="line">$ hdparm --user-master u --security-disable p &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">$ hdparm -I &#x2F;dev&#x2F;sdx</span><br><span class="line">not     enabled</span><br><span class="line">not     locked</span><br><span class="line"></span><br><span class="line">## hide Areas</span><br><span class="line">$ hdparm -N &#x2F;dev&#x2F;sda</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sda:</span><br><span class="line"> READ_NATIVE_MAX_ADDRESS_EXT failed: Input&#x2F;output error</span><br><span class="line"></span><br><span class="line">$ hdparm -N &#x2F;dev&#x2F;sdd</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdd:</span><br><span class="line"> max sectors   &#x3D; 3907029168&#x2F;3907029168, HPA is disabled</span><br><span class="line"></span><br><span class="line">##</span><br><span class="line"></span><br><span class="line">$ hdparm -N &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdx:</span><br><span class="line">max sectors   &#x3D; 78125000&#x2F;78165360, HPA is enabled</span><br><span class="line"></span><br><span class="line">$ hdparm -N p78165360 &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdx:</span><br><span class="line">setting max visible sectors to 78165360 (permanent)</span><br><span class="line">max sectors   &#x3D; 78165360&#x2F;78165360, HPA is disabled</span><br><span class="line"></span><br><span class="line">$ hdparm --dco-identify &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdx:</span><br><span class="line">DCO Revision: 0x0001</span><br><span class="line">The following features can be selectively disabled via DCO:</span><br><span class="line">       Transfer modes:</span><br><span class="line">                udma0 udma1 udma2 udma3 udma4 udma5</span><br><span class="line">       Real max sectors: 78165360</span><br><span class="line">       ATA command&#x2F;feature sets:</span><br><span class="line">                AAM HPA</span><br><span class="line"></span><br><span class="line">$ hdparm --dco-restore &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdx:</span><br><span class="line">Use of --dco-restore is VERY DANGEROUS.</span><br><span class="line">You are trying to deliberately reset your drive configuration back to</span><br><span class="line">the factory defaults.</span><br><span class="line">This may change the apparent capacity and feature set of the drive,</span><br><span class="line">making all data on it inaccessible.</span><br><span class="line">You could lose *everything*.</span><br><span class="line">Please supply the --yes-i-know-what-i-am-doing flag if you really want this.</span><br><span class="line">Program aborted.</span><br><span class="line"></span><br><span class="line">$ hdparm --yes-i-know-what-i-am-doing --dco-restore &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line">&#x2F;dev&#x2F;sdx:</span><br><span class="line">issuing DCO restore command</span><br></pre></td></tr></table></figure>

<h3 id="Enable-SAS-SATA-dev-cache"><a href="#Enable-SAS-SATA-dev-cache" class="headerlink" title="Enable SAS/SATA dev cache"></a>Enable SAS/SATA dev cache</h3><h4 id="SAS-cache"><a href="#SAS-cache" class="headerlink" title="SAS cache"></a>SAS cache</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#simple way</span><br><span class="line">$ sdparm -s WCE&#x3D;1,RCD&#x3D;0 &#x2F;dev&#x2F;sdX</span><br><span class="line"></span><br><span class="line">Byte  7   6    5   4    3   2   1   0               Default</span><br><span class="line">0    ps 0  page code&#x3D;08h                             88h</span><br><span class="line">1    page length &#x3D; 12h                               12h</span><br><span class="line">2    ic ABPF CAP DISC SIZE WCE MF RCD                04h</span><br><span class="line"></span><br><span class="line">#show detail</span><br><span class="line">$ sg_wr_mode -p 0x8 &#x2F;dev&#x2F;sda</span><br><span class="line">&gt;&gt;&gt; No contents given, so show current mode page data:</span><br><span class="line">  header:</span><br><span class="line">00 22 00 10 00 00 00 08</span><br><span class="line">  block descriptor(s):</span><br><span class="line">ff ff ff ff 00 00 02 00</span><br><span class="line">  mode page:</span><br><span class="line">88 12 00 00 ff ff 00 00  ff ff ff ff 00 08 00 00</span><br><span class="line">00 00 00 00</span><br><span class="line"></span><br><span class="line">#Enable read and write cache</span><br><span class="line">$ sg_wr_mode -p 0x8 &#x2F;dev&#x2F;sda</span><br><span class="line">&gt;&gt;&gt; No contents given, so show current mode page data:</span><br><span class="line">  header:</span><br><span class="line">00 22 00 10 00 00 00 08</span><br><span class="line">  block descriptor(s):</span><br><span class="line">ff ff ff ff 00 00 02 00</span><br><span class="line">  mode page:</span><br><span class="line">88 12 04 00 ff ff 00 00  ff ff ff ff 00 08 00 00</span><br><span class="line">00 00 00 00</span><br><span class="line"></span><br><span class="line">#Disable all</span><br><span class="line">$ sg_wr_mode -p 0x8 &#x2F;dev&#x2F;sda</span><br><span class="line">&gt;&gt;&gt; No contents given, so show current mode page data:</span><br><span class="line">  header:</span><br><span class="line">00 22 00 10 00 00 00 08</span><br><span class="line">  block descriptor(s):</span><br><span class="line">ff ff ff ff 00 00 02 00</span><br><span class="line">  mode page:</span><br><span class="line">88 12 05 00 ff ff 00 00  ff ff ff ff 00 08 00 00</span><br><span class="line">00 00 00 00</span><br></pre></td></tr></table></figure>


<h4 id="SATA-cache"><a href="#SATA-cache" class="headerlink" title="SATA cache"></a>SATA cache</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#simple way</span><br><span class="line">$ hdparm -W1 &#x2F;dev&#x2F;sdc</span><br><span class="line"></span><br><span class="line">#show detail</span><br><span class="line">$ sg_sat_set_features --feature&#x3D;2h -v &#x2F;dev&#x2F;sdc #Enable write cache</span><br><span class="line">    ATA pass-through(16) cdb: 85 06 0c 00 02 00 00 00 00 00 00 00 00 00 ef 00</span><br><span class="line">    85      -     0x7429   Commands and feature sets supported or enabled</span><br><span class="line">   255      -     0xe0a5   Integrity word</span><br><span class="line">   255     15:8     0xe0   Checksum</span><br><span class="line">0x7429&#x3D;111,0100,0010,1001</span><br><span class="line"></span><br><span class="line">$ sg_sat_set_features --feature&#x3D;AAh -v &#x2F;dev&#x2F;sdc #after enable write cache, continue to enabel read cahe</span><br><span class="line">    ATA pass-through(16) cdb: 85 06 0c 00 aa 00 00 00 00 00 00 00 00 00 ef 00</span><br><span class="line">    85      -     0x7469   Commands and feature sets supported or enabled   </span><br><span class="line">   255      -     0xa0a5   Integrity word</span><br><span class="line">   255     15:8     0xa0   Checksum</span><br><span class="line">0x7469&#x3D;111,0100,0110,1001</span><br><span class="line">   </span><br><span class="line">$ sg_sat_set_features --feature&#x3D;55h -v &#x2F;dev&#x2F;sdc #disable read cache</span><br><span class="line">    ATA pass-through(16) cdb: 85 06 0c 00 55 00 00 00 00 00 00 00 00 00 ef 00</span><br><span class="line">    85      -     0x7429   Commands and feature sets supported or enabled</span><br><span class="line">    85      5          1   Write cache enabled  ## Here will auto add a new line</span><br><span class="line">   255      -     0xe0a5   Integrity word</span><br><span class="line">   255     15:8     0xe0   Checksum</span><br><span class="line">0x7429&#x3D;111,0100,0010,1001</span><br><span class="line"></span><br><span class="line">$ sg_sat_set_features --feature&#x3D;82h -v &#x2F;dev&#x2F;sdc #after disable read cache, continue to disable write cache</span><br><span class="line">    ATA pass-through(16) cdb: 85 06 0c 00 82 00 00 00 00 00 00 00 00 00 ef 00</span><br><span class="line">    85      -     0x7409   Commands and feature sets supported or enabled</span><br><span class="line">   255      -     0x00a5   Integrity word</span><br><span class="line">0x7409&#x3D;111,0100,0000,1001</span><br></pre></td></tr></table></figure>
<p>Set Features (EFh)</p>
<table>
<thead>
<tr>
<th align="center">FUNCTION</th>
<th align="center">FEATURES REGISTER</th>
<th align="center">SECTOR COUNTREGISTER</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Enable read cache1</td>
<td align="center">AAh</td>
<td align="center">Don’t care</td>
</tr>
<tr>
<td align="center">Disable read cache1</td>
<td align="center">55h</td>
<td align="center">Don’t care</td>
</tr>
<tr>
<td align="center">Enable write cache1</td>
<td align="center">02h</td>
<td align="center">Don’t care</td>
</tr>
<tr>
<td align="center">Disable write cache1</td>
<td align="center">82h</td>
<td align="center">Don’t care</td>
</tr>
<tr>
<td align="center">Set Transfer Mode</td>
<td align="center">03h</td>
<td align="center">Don’t care</td>
</tr>
<tr>
<td align="center">Enable use of Serial ATA Feature</td>
<td align="center">10h</td>
<td align="center">02h-DMA Setup FIS Auto-Activate optimization 06h-Software Settings</td>
</tr>
<tr>
<td align="center">Disable use of Serial ATA Feature</td>
<td align="center">90h</td>
<td align="center">02h-DMA Setup FIS Auto-Activate optimization 06h-Software Settings</td>
</tr>
</tbody></table>
<p>Note: Changes are only valid while power remains applied to the drive. After power is cycled, the drive reverts to the<br>default settings.</p>
<p>Sends an ATA SET FEATURES command via a SAT pass through.<br>Primary feature code is placed in ‘–feature=FEA’ with ‘–count=CO’ and ‘–lba=LBA’ being auxiliaries for some features.  The arguments CO, FEA and LBA are decimal unless prefixed by ‘0x’ or have a trailing ‘h’.</p>
<h3 id="sector-size"><a href="#sector-size" class="headerlink" title="sector size"></a><a href="http://people.redhat.com/msnitzer/docs/linux-advanced-storage-6.1.pdf" target="_blank" rel="noopener">sector size</a></h3><h4 id="512e-SATA"><a href="#512e-SATA" class="headerlink" title="512e SATA"></a>512e SATA</h4><p>the first logical block does begin on a boundary aligned to the physical block size</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"> 106      -     0x6003   Physical sector size / logical sector size</span><br><span class="line"> 106     15:14     0x1   Must be <span class="built_in">set</span> to 0x1</span><br><span class="line"> 106     13          1   Multiple logical sectors per physical sector</span><br><span class="line"> 106      3:0      0x3   2^X logical sectors per physical sector</span><br><span class="line">...</span><br><span class="line"> 209      -     0x4000   Alignment of logical sectors</span><br><span class="line"> 209     15:14     0x1   Must be <span class="built_in">set</span> to 0x1</span><br><span class="line"></span><br><span class="line">$ dc -e <span class="string">'16i2o0x6003p'</span></span><br><span class="line">110000000000011</span><br><span class="line">110,0000,0000,0011 &lt;- 2^3=8, 512 Bytes x 8 = 4096 Bytes (phy)</span><br><span class="line"> ^</span><br><span class="line"> |</span><br><span class="line"><span class="comment">#Word 106 bit 15 must be 0 and bit 14 must be 1. This signals that word 106 is valid. Bit 13 must be set to indicate that bits 3:0 are valid and that word 209 is specified.</span></span><br><span class="line">first 110, I guess 0 means 3:0 are valid  and that word 209 is specified</span><br><span class="line"></span><br><span class="line"><span class="comment">#Bits 3:0 indicate the physical block size exponent. I.e. 2n logical blocks/physical block. For a drive with 4096-byte physical blocks and 512-byte logical blocks, a value of 3 is reported in bits 3:0 (23 = 8).</span></span><br><span class="line"></span><br><span class="line">$ dc -e <span class="string">'16i2o0x4000p'</span></span><br><span class="line">100000000000000</span><br><span class="line">100,0000,0000,0000</span><br></pre></td></tr></table></figure>

<p>If the first logical block does <code>not</code> begin on a boundary aligned to the physical block size<br>word 209 should be reported by the device. Bit 15 must be zero, bit 14 must be 1. Bits 13:0 indicate the logical sector offset within the first physical sector where the first logical sector is placed.<br>For compatibility with legacy operating systems some vendors may align the blocks so that LBA 63 falls on a physical block boundary. In that case an alignment of 1 should be reported.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### I have not reproduce this case</span></span><br><span class="line">IDENTIFY DEVICE 106 0110 0000 0000 0011 0x6003</span><br><span class="line">IDENTIFY DEVICE 209 0100 0000 0000 0001 0x4001</span><br></pre></td></tr></table></figure>

<h4 id="4KN"><a href="#4KN" class="headerlink" title="4KN"></a>4KN</h4><p>If a drive uses 4KiB logical and physical blocks, IDENTIFY DEVICE words 106, 117, and 118 must be specified.<br>Word 106 bit 15 must be 0 and bit 14 must be 1. This signals that word 106 is valid. Bit 12 must be set to indicate that words 117 and 118 are specified.<br>Word 117 (LSB) and 118 (MSB) specify the logical block size in words. </p>
<p>I.e. for a 4KiB drive that would be 2048.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">IDENTIFY DEVICE 106 0101 0000 0000 0000 0x5000</span><br><span class="line">IDENTIFY DEVICE 117 0000 1000 0000 0000 0x0800 <span class="comment"># 2^11=2048=0x0800</span></span><br><span class="line">IDENTIFY DEVICE 118 0000 0000 0000 0000 0x0000</span><br></pre></td></tr></table></figure>

<h4 id="Nominal-media-rotation-rate"><a href="#Nominal-media-rotation-rate" class="headerlink" title="Nominal media rotation rate"></a>Nominal media rotation rate</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">217      -     0x1c20   Nominal media rotation rate</span><br><span class="line">$ dc -e <span class="string">'16i2o0x1c20p'</span></span><br><span class="line">100000</span><br><span class="line">10,0000</span><br><span class="line"><span class="comment"># why dell SATA HDD not be 0x1.</span></span><br><span class="line">IDENTIFY DEVICE 217 0000 0000 0000 0001 0x0001</span><br></pre></td></tr></table></figure>
<p>If the storage device is non-rotational (i.e. SSD or array) word 217 of IDENTIFY DEVICE should be set to one. The I/O scheduler may then be less agressive in terms of seek avoidance. Any other value reported in 217 will be ignored.</p>
<h3 id="Does-devices-support-the-scsi-Persistent-Reservation"><a href="#Does-devices-support-the-scsi-Persistent-Reservation" class="headerlink" title="Does devices support the scsi-Persistent-Reservation"></a>Does devices support the scsi-Persistent-Reservation</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_persist -c /dev/sdi</span><br><span class="line">  ATA       HGST HUH721010AL  LT14</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">PR <span class="keyword">in</span> (Report capabilities): <span class="built_in">command</span> not supported</span><br><span class="line">sg_persist failed: Illegal request, Invalid opcode</span><br><span class="line"></span><br><span class="line">$ sg_persist -c /dev/sdj</span><br><span class="line">  HGST      HUH721010AL5200   LS14</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">Report capabilities response:</span><br><span class="line">  Replace Lost Reservation Capable(RLR_C): 0</span><br><span class="line">  Compatible Reservation Handling(CRH): 0</span><br><span class="line">  Specify Initiator Ports Capable(SIP_C): 1</span><br><span class="line">  All Target Ports Capable(ATP_C): 1</span><br><span class="line">  Persist Through Power Loss Capable(PTPL_C): 1</span><br><span class="line">  Type Mask Valid(TMV): 1</span><br><span class="line">  Allow Commands: 0</span><br><span class="line">  Persist Through Power Loss Active(PTPL_A): 0</span><br><span class="line">    Support indicated <span class="keyword">in</span> Type mask:</span><br><span class="line">      Write Exclusive, all registrants: 0</span><br><span class="line">      Exclusive Access, registrants only: 1</span><br><span class="line">      Write Exclusive, registrants only: 1</span><br><span class="line">      Exclusive Access: 1</span><br><span class="line">      Write Exclusive: 1</span><br><span class="line">      Exclusive Access, all registrants: 0</span><br><span class="line"></span><br><span class="line">A: [1:0:14:0]   disk    HP       MK0800JVYPQ      HPD6  /dev/sdo   35001173d028c863c  /dev/sg16</span><br><span class="line">B: [1:0:14:0]   disk    HP       MK0800JVYPQ      HPD6  /dev/sdo   35001173d028c863c  /dev/sg16</span><br><span class="line"></span><br><span class="line">$ sg_persist --<span class="built_in">read</span>-keys /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x0, there are NO registered reservation keys</span><br><span class="line">$ sg_persist --<span class="built_in">read</span>-reservation /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x0, there is NO reservation held</span><br><span class="line">$ sg_persist /dev/sdo</span><br><span class="line">&gt;&gt; No service action given; assume Persistent Reserve In <span class="built_in">command</span></span><br><span class="line">&gt;&gt; with Read Keys service action</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x0, there are NO registered reservation keys</span><br><span class="line">$ sg_persist -n -i -r -d /dev/sdo</span><br><span class="line">  PR generation=0x0, there is NO reservation held</span><br></pre></td></tr></table></figure>

<h4 id="Add-it-to-multipath-test-in-HUS-150-worked"><a href="#Add-it-to-multipath-test-in-HUS-150-worked" class="headerlink" title="Add it to multipath, test in HUS 150, worked"></a>Add it to multipath, test in HUS 150, worked</h4><p>mulitpath just for little device, not for software raid solution</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">no_path_retry queue</span><br><span class="line">features &quot;0&quot;</span><br></pre></td></tr></table></figure>

<h3 id="Because-brain-split-zfs-will-corrupt"><a href="#Because-brain-split-zfs-will-corrupt" class="headerlink" title="Because brain-split, zfs will corrupt"></a>Because brain-split, zfs will corrupt</h3><p>process<br>register -&gt; reserve -&gt; preempt-abort -&gt; re-register another node</p>
<h4 id="Register"><a href="#Register" class="headerlink" title="Register"></a>Register</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># register</span></span><br><span class="line">A $ openssl rand -hex 4</span><br><span class="line">f089a155</span><br><span class="line">A $ sg_persist --out --register --param-sark=f089a155 -d /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">A $ sg_persist --<span class="built_in">read</span>-keys /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x7, 1 registered reservation key follows:</span><br><span class="line">    0xf089a155</span><br></pre></td></tr></table></figure>

<h4 id="Reserve"><a href="#Reserve" class="headerlink" title="Reserve"></a>Reserve</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">A $ sg_persist --out --reserve --param-rk&#x3D;f089a155 --prout-type&#x3D;1 &#x2F;dev&#x2F;sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device type: disk</span><br><span class="line"></span><br><span class="line">B $ sg_persist --read-keys &#x2F;dev&#x2F;sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device type: disk</span><br><span class="line">  PR generation&#x3D;0xd, 1 registered reservation key follows:</span><br><span class="line">    0xf089a155</span><br><span class="line"></span><br><span class="line">B $ sg_persist --read-reservation &#x2F;dev&#x2F;sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device type: disk</span><br><span class="line">  PR generation&#x3D;0xd, Reservation follows:</span><br><span class="line">    Key&#x3D;0xf089a155</span><br><span class="line">    scope: LU_SCOPE,  type: Write Exclusive</span><br><span class="line"></span><br><span class="line">A $ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;sdo bs&#x3D;1M count&#x3D;10 oflag&#x3D;sync</span><br><span class="line">10+0 records in</span><br><span class="line">10+0 records out</span><br><span class="line">10485760 bytes (10 MB) copied, 0.0191827 s, 547 MB&#x2F;s</span><br><span class="line"></span><br><span class="line">B $ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;mapper&#x2F;35000cca2515f34c4 bs&#x3D;1M count&#x3D;10 oflag&#x3D;sync</span><br><span class="line">dd: error writing ‘&#x2F;dev&#x2F;sdo’: Input&#x2F;output error</span><br><span class="line">1+0 records in</span><br><span class="line">0+0 records out</span><br><span class="line">0 bytes (0 B) copied, 0.618006 s, 0.0 kB&#x2F;s</span><br></pre></td></tr></table></figure>

<h4 id="Server-B-preempt-the-reservation"><a href="#Server-B-preempt-the-reservation" class="headerlink" title="Server B preempt the reservation"></a>Server B preempt the reservation</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">B $ openssl rand -hex 4</span><br><span class="line">f7ec3033</span><br><span class="line">B $ sg_persist --<span class="built_in">read</span>-keys /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x1, 1 registered reservation key follows:</span><br><span class="line">    0xf089a155</span><br><span class="line"></span><br><span class="line">B $ sg_persist --out --register --param-sark=f7ec3033 -d /dev/sdo</span><br><span class="line">B $ sg_persist --<span class="built_in">read</span>-keys /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0xa, 2 registered reservation keys follow:</span><br><span class="line">    0xf089a155</span><br><span class="line">    0xf7ec3033</span><br><span class="line"></span><br><span class="line">B $ sg_persist --<span class="built_in">read</span>-reservation /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0xa, Reservation follows:</span><br><span class="line">    Key=0xf089a155</span><br><span class="line">    scope: LU_SCOPE,  <span class="built_in">type</span>: Write Exclusive</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">B $ sg_persist -n -o -A -K f7ec3033 -S f089a155  -T 1 -v -d /dev/sdo</span><br><span class="line">    Persistent Reservation Out cmd: 5f 05 01 00 00 00 00 00 18 00</span><br><span class="line">PR out: <span class="built_in">command</span> (Preempt and abort) successful</span><br><span class="line"></span><br><span class="line">B $ sg_persist --<span class="built_in">read</span>-reservation -d /dev/sdo<span class="string">"</span></span><br><span class="line"><span class="string">  HP        MK0800JVYPQ       HPD6</span></span><br><span class="line"><span class="string">  Peripheral device type: disk</span></span><br><span class="line"><span class="string">  PR generation=0xf, Reservation follows:</span></span><br><span class="line"><span class="string">    Key=0xf7ec3033</span></span><br><span class="line"><span class="string">    scope: LU_SCOPE,  type: Write Exclusive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A $ dd if=/dev/zero of=/dev/sdo bs=1M count=10 oflag=sync</span></span><br><span class="line"><span class="string">dd: error writing ‘/dev/sdo’: Input/output error</span></span><br><span class="line"><span class="string">1+0 records in</span></span><br><span class="line"><span class="string">0+0 records out</span></span><br><span class="line"><span class="string">0 bytes (0 B) copied, 0.617521 s, 0.0 kB/s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">B $ dd if=/dev/zero of=/dev/sdo bs=1M count=10 oflag=sync</span></span><br><span class="line"><span class="string">10+0 records in</span></span><br><span class="line"><span class="string">10+0 records out</span></span><br><span class="line"><span class="string">10485760 bytes (10 MB) copied, 0.0212361 s, 494 MB/s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### if you input wrong -K and -S value, it will return error</span></span><br><span class="line"><span class="string">B $ sg_persist -n -o -A -d /dev/sdo -K f7ec3zzz -S f08fsd55  -T 1 -v</span></span><br><span class="line"><span class="string">    Persistent Reservation Out cmd: 5f 05 01 00 00 00 00 00 18 00</span></span><br><span class="line"><span class="string">    persistent reserve out: scsi status: Reservation Conflict</span></span><br><span class="line"><span class="string">    PR out: command failed</span></span><br></pre></td></tr></table></figure>

<h4 id="After-preempt-abort-Server-A-preempt-the-reservation"><a href="#After-preempt-abort-Server-A-preempt-the-reservation" class="headerlink" title="After preempt-abort, Server A preempt the reservation"></a>After preempt-abort, Server A preempt the reservation</h4><p>Need to be register again</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">A $ sg_persist --<span class="built_in">read</span>-keys /dev/sdo<span class="string">"</span></span><br><span class="line"><span class="string">  HP        MK0800JVYPQ       HPD6</span></span><br><span class="line"><span class="string">  Peripheral device type: disk</span></span><br><span class="line"><span class="string">  PR generation=0xf, 1 registered reservation key follows:</span></span><br><span class="line"><span class="string">    0xf7ec3033</span></span><br><span class="line"><span class="string">A $ sg_persist --out --register --param-sark=f089a155 -d /dev/sdo</span></span><br><span class="line"><span class="string">  HP        MK0800JVYPQ       HPD6</span></span><br><span class="line"><span class="string">  Peripheral device type: disk</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A $ sg_persist --read-keys /dev/sdo</span></span><br><span class="line"><span class="string">  HP        MK0800JVYPQ       HPD6</span></span><br><span class="line"><span class="string">  Peripheral device type: disk</span></span><br><span class="line"><span class="string">  PR generation=0x10, 2 registered reservation keys follow:</span></span><br><span class="line"><span class="string">    0xf089a155</span></span><br><span class="line"><span class="string">    0xf7ec3033</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A $ sg_persist -n -o -A -K f089a155 -S f7ec3033  -T 1 -v -d /dev/sdo</span></span><br><span class="line"><span class="string">    Persistent Reservation Out cmd: 5f 05 01 00 00 00 00 00 18 00</span></span><br><span class="line"><span class="string">PR out: command (Preempt and abort) successful</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A $ sg_persist --read-reservation /dev/sdo</span></span><br><span class="line"><span class="string">  HP        MK0800JVYPQ       HPD6</span></span><br><span class="line"><span class="string">  Peripheral device type: disk</span></span><br><span class="line"><span class="string">  PR generation=0x11, Reservation follows:</span></span><br><span class="line"><span class="string">    Key=0xf089a155</span></span><br><span class="line"><span class="string">    scope: LU_SCOPE,  type: Write Exclusive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A $ dd if=/dev/zero of=/dev/sdo bs=1M count=10 oflag=sync</span></span><br><span class="line"><span class="string">10+0 records in</span></span><br><span class="line"><span class="string">10+0 records out</span></span><br><span class="line"><span class="string">10485760 bytes (10 MB) copied, 0.0188563 s, 556 MB/s</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">B $ dd if=/dev/zero of=/dev/sdo bs=1M count=10 oflag=sync"</span></span><br><span class="line">dd: error writing ‘/dev/sdo’: Input/output error</span><br><span class="line">1+0 records <span class="keyword">in</span></span><br><span class="line">0+0 records out</span><br><span class="line">0 bytes (0 B) copied, 0.744081 s, 0.0 kB/s</span><br></pre></td></tr></table></figure>

<h4 id="Prout-type"><a href="#Prout-type" class="headerlink" title="Prout-type"></a>Prout-type</h4><p>1-&gt; write exclusive<br>3-&gt; exclusive access<br>5-&gt; write exclusive - registrants only<br>6-&gt; exclusive access - registrants only<br>7-&gt; write exclusive - all registrants<br>8-&gt; exclusive access - all registrants</p>
<h4 id="Release-reservation"><a href="#Release-reservation" class="headerlink" title="Release reservation"></a>Release reservation</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_persist --out --release -K f089a155 --prout-type=1 /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line"></span><br><span class="line">$ sg_persist --<span class="keyword">in</span> -r --no-inquiry -d /dev/sdo</span><br><span class="line">  PR generation=0x5f, there is NO reservation held</span><br><span class="line"></span><br><span class="line">$ sg_persist --<span class="built_in">read</span>-reservation /dev/sdo</span><br><span class="line">  HP        MK0800JVYPQ       HPD6</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">  PR generation=0x11, there is NO reservation held</span><br></pre></td></tr></table></figure>

<h4 id="Unregister-key"><a href="#Unregister-key" class="headerlink" title="Unregister key"></a>Unregister key</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">A $ sg_persist --out --register -K f089a155 -d /dev/sdo</span><br><span class="line">  HGST      HUH721010AL5200   A21D</span><br><span class="line">  Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line">A $ sg_persist --<span class="keyword">in</span> -k --no-inquiry -d /dev/sdo</span><br><span class="line">  PR generation=0x60, there are NO registered reservation keys</span><br></pre></td></tr></table></figure>

<h4 id="Force-mode"><a href="#Force-mode" class="headerlink" title="Force mode ?"></a>Force mode ?</h4><p>It releases the persistent reservation (if any) and clears all registrations from the device</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">A $ sg_persist --out --clear -K f089a155 /dev/sdo</span><br></pre></td></tr></table></figure>

<h4 id="After-A-and-B-reboot-the-reservation-worked"><a href="#After-A-and-B-reboot-the-reservation-worked" class="headerlink" title="After A and B reboot, the reservation worked"></a>After A and B reboot, the reservation worked</h4><h4 id="Plugin-it-to-another-JBOD-all-key-reservation-will-missing"><a href="#Plugin-it-to-another-JBOD-all-key-reservation-will-missing" class="headerlink" title="Plugin it to another JBOD, all key, reservation will missing"></a>Plugin it to another JBOD, all key, reservation will missing</h4><pre><code>-Z, --param-aptpl
       set  the &apos;activate persist through power loss&apos; (APTPL) flag in the parameter block of the PROUT command. Relevant for &apos;regis‐
       ter&apos;, &apos;register and ignore existing key&apos; and &apos;register and move&apos; sub-commands.</code></pre><p><a href="http://scsi3pr.blogspot.com/2010/11/scsi-3-persistent-group-reservation.html" target="_blank" rel="noopener">SCSI-3 Persistent Group Reservation</a><br><a href="https://dhelios.blogspot.com/2015/04/how-can-i-view-create-and-remove-scsi.html" target="_blank" rel="noopener">How can I view create and remove scsi</a><br><a href="http://lkml.iu.edu/hypermail/linux/kernel/0903.3/02074.html" target="_blank" rel="noopener">Target_Core_Mod</a><br><a href="https://lists.wpkg.org/pipermail/stgt/2010-October/017304.html" target="_blank" rel="noopener">Support for persistent reservations</a><br><a href="https://assets.cdn.sap.com/sapcom/docs/2016/06/84ea994f-767c-0010-82c7-eda71af511fa.pdf" target="_blank" rel="noopener">SAP HANA Fiber Channel Storage Connector Admin Guide</a><br><a href="http://scsi3pr.blogspot.com/2010/11/scsi-3-persistent-group-reservation.html" target="_blank" rel="noopener">When to use SCSI-3 Persistent Group Reservation</a></p>
<h3 id="NVME-persist-reservation"><a href="#NVME-persist-reservation" class="headerlink" title="NVME persist reservation"></a><a href="https://blogs.vmware.com/virtualblocks/2018/08/20/nvme-over-fabrics-part-two/" target="_blank" rel="noopener">NVME persist reservation</a></h3><h3 id="the-others-sg-tools"><a href="#the-others-sg-tools" class="headerlink" title="the others sg tools"></a>the others sg tools</h3><h4 id="sg-readcap"><a href="#sg-readcap" class="headerlink" title="sg_readcap"></a>sg_readcap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_readcap -l /dev/sdc</span><br><span class="line">Read Capacity results:</span><br><span class="line">   Protection: prot_en=0, p_type=0, p_i_exponent=0</span><br><span class="line">   Thin provisioning: tpe=0, tprz=0</span><br><span class="line">   Last logical block address=15628053167 (0x3a3812aaf), Number of logical blocks=15628053168</span><br><span class="line">   Logical block length=512 bytes</span><br><span class="line">   Logical blocks per physical block exponent=3</span><br><span class="line">   Lowest aligned logical block address=0</span><br><span class="line">Hence:</span><br><span class="line">   Device size: 8001563222016 bytes, 7630885.3 MiB, 8001.56 GB</span><br></pre></td></tr></table></figure>

<h4 id="sg-logs"><a href="#sg-logs" class="headerlink" title="sg_logs"></a>sg_logs</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_logs -q /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Supported <span class="built_in">log</span> pages  [0x0]:</span><br><span class="line">    0x00        Supported <span class="built_in">log</span> pages [sp]</span><br><span class="line">    0x02        Write error [we]</span><br><span class="line">    0x03        Read error [re]</span><br><span class="line">    0x05        Verify error [ve]</span><br><span class="line">    0x06        Non medium [nm]</span><br><span class="line">    0x08        Format status [fs]</span><br><span class="line">    0x0d        Temperature [temp]</span><br><span class="line">    0x0e        Start-stop cycle counter [sscc]</span><br><span class="line">    0x0f        Application client [ac]</span><br><span class="line">    0x10        Self <span class="built_in">test</span> results [str]</span><br><span class="line">    0x15        Background scan results [bsr]</span><br><span class="line">    0x18        Protocol specific port [psp]</span><br><span class="line">    0x19        General Statistics and Performance [gsp]</span><br><span class="line">    0x1a        Power condition transitions [pct]</span><br><span class="line">    0x2f        Informational exceptions [ie]</span><br><span class="line">    0x30        Performance counters (Hitachi) [pc_hi]</span><br><span class="line">    0x37        Cache (seagate) [c_se]</span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x18 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Protocol Specific port page <span class="keyword">for</span> SAS SSP  (sas-2) [0x18]</span><br><span class="line">relative target port id = 1</span><br><span class="line">  generation code = 2</span><br><span class="line">  number of phys = 1</span><br><span class="line">  phy identifier = 0</span><br><span class="line">    attached device <span class="built_in">type</span>: expander device</span><br><span class="line">    attached reason: SMP phy control <span class="keyword">function</span></span><br><span class="line">    reason: unknown</span><br><span class="line">    negotiated logical link rate: 12 Gbps</span><br><span class="line">    attached initiator port: ssp=0 stp=0 smp=0</span><br><span class="line">    attached target port: ssp=0 stp=0 smp=1</span><br><span class="line">    SAS address = 0x5000cca26c1b8631</span><br><span class="line">    attached SAS address = 0x500056b3787358ff</span><br><span class="line">    attached phy identifier = 9</span><br><span class="line">    Invalid DWORD count = 0</span><br><span class="line">    Running disparity error count = 0</span><br><span class="line">    Loss of DWORD synchronization = 0</span><br><span class="line">    Phy reset problem = 0</span><br><span class="line">    Phy event descriptors:</span><br><span class="line">     Invalid word count: 0</span><br><span class="line">     Running disparity error count: 0</span><br><span class="line">     Loss of dword synchronization count: 0</span><br><span class="line">     Phy reset problem count: 0</span><br><span class="line">relative target port id = 2</span><br><span class="line">  generation code = 2</span><br><span class="line">  number of phys = 1</span><br><span class="line">  phy identifier = 1</span><br><span class="line">    attached device <span class="built_in">type</span>: no device attached</span><br><span class="line">    attached reason: unknown</span><br><span class="line">    reason: power on</span><br><span class="line">    negotiated logical link rate: phy enabled; unknown reason</span><br><span class="line">    attached initiator port: ssp=0 stp=0 smp=0</span><br><span class="line">    attached target port: ssp=0 stp=0 smp=0</span><br><span class="line">    SAS address = 0x5000cca26c1b8632</span><br><span class="line">    attached SAS address = 0x0</span><br><span class="line">    attached phy identifier = 0</span><br><span class="line">    Invalid DWORD count = 0</span><br><span class="line">    Running disparity error count = 0</span><br><span class="line">    Loss of DWORD synchronization = 0</span><br><span class="line">    Phy reset problem = 0</span><br><span class="line">    Phy event descriptors:</span><br><span class="line">     Invalid word count: 0</span><br><span class="line">     Running disparity error count: 0</span><br><span class="line">     Loss of dword synchronization count: 0</span><br><span class="line">     Phy reset problem count: 0</span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x02 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Write error counter page  [0x2]</span><br><span class="line">  Errors corrected without substantial delay = 0</span><br><span class="line">  Errors corrected with possible delays = 0</span><br><span class="line">  Total rewrites or rereads = 0</span><br><span class="line">  Total errors corrected = 1</span><br><span class="line">  Total <span class="built_in">times</span> correction algorithm processed = 404822</span><br><span class="line">  Total bytes processed = 35127905470928</span><br><span class="line">  Total uncorrected errors = 0</span><br><span class="line">$ sg_logs -p 0x03 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Read error counter page  [0x3]</span><br><span class="line">  Errors corrected without substantial delay = 0</span><br><span class="line">  Errors corrected with possible delays = 0</span><br><span class="line">  Total rewrites or rereads = 0</span><br><span class="line">  Total errors corrected = 0</span><br><span class="line">  Total <span class="built_in">times</span> correction algorithm processed = 1350903</span><br><span class="line">  Total bytes processed = 49018892496048</span><br><span class="line">  Total uncorrected errors = 0</span><br><span class="line">$ sg_logs -p 0x05 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Verify error counter page  [0x5]</span><br><span class="line">  Errors corrected without substantial delay = 0</span><br><span class="line">  Errors corrected with possible delays = 0</span><br><span class="line">  Total rewrites or rereads = 0</span><br><span class="line">  Total errors corrected = 0</span><br><span class="line">  Total <span class="built_in">times</span> correction algorithm processed = 5154</span><br><span class="line">  Total bytes processed = 101563105280</span><br><span class="line">  Total uncorrected errors = 0</span><br><span class="line">$ sg_logs -p 0x06 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Non-medium error page  [0x6]</span><br><span class="line">  Non-medium error count = 0</span><br><span class="line"></span><br><span class="line"><span class="comment">## the "Total errors corrected" has the a lot of growth in some of issue seagate HDDs</span></span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x08 /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Format status page  [0x8]</span><br><span class="line">  Format data out:</span><br><span class="line"> 00     00 02 00 00                                         ....</span><br><span class="line">  Grown defects during certification = 0</span><br><span class="line">  Total blocks reassigned during format = 0</span><br><span class="line">  Total new blocks reassigned = 0</span><br><span class="line">  Power on minutes since format = 10647</span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x1a /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Power condition transitions page  [0x1a]</span><br><span class="line">  Accumulated transitions to active = 700283</span><br><span class="line">  Accumulated transitions to idle_a = 698441</span><br><span class="line">  Accumulated transitions to idle_b = 1855</span><br><span class="line">  Accumulated transitions to idle_c = 0</span><br><span class="line">  Accumulated transitions to standby_z = 0</span><br><span class="line">  Accumulated transitions to standby_y = 0</span><br><span class="line"></span><br><span class="line">$ sg_logs -p 0x2f /dev/sdj</span><br><span class="line">    HGST      HUH721010AL5200   LS14</span><br><span class="line">Informational Exceptions page  [0x2f]</span><br><span class="line">  IE asc = 0x0, ascq = 0x0</span><br><span class="line">    Current temperature = 29 C</span><br><span class="line">    Threshold temperature = 85 C  [common extension]</span><br><span class="line">  parameter code = 0x1, contents <span class="keyword">in</span> hex:</span><br><span class="line"> 00     00 01 03 03 64 64 19</span><br></pre></td></tr></table></figure>

<h4 id="sg-senddiag"><a href="#sg-senddiag" class="headerlink" title="sg_senddiag"></a>sg_senddiag</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sg_senddiag -l &#x2F;dev&#x2F;sg18</span><br><span class="line">Supported diagnostic pages response:</span><br><span class="line">  0x00  Supported diagnostic pages</span><br><span class="line">  0x3f  Protocol specific (SAS transport)</span><br><span class="line">  0x40  Translate address (direct access)</span><br><span class="line">  0xdf  &lt;unknown&gt;</span><br></pre></td></tr></table></figure>

<h4 id="sg-inq"><a href="#sg-inq" class="headerlink" title="sg_inq"></a>sg_inq</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_inq -d /dev/sdj</span><br><span class="line">standard INQUIRY:</span><br><span class="line">  PQual=0  Device_type=0  RMB=0  LU_CONG=0  version=0x06  [SPC-4]</span><br><span class="line">  [AERC=0]  [TrmTsk=0]  NormACA=0  HiSUP=1  Resp_data_format=2</span><br><span class="line">  SCCS=0  ACC=0  TPGS=0  3PC=0  Protect=1  [BQue=0]</span><br><span class="line">  EncServ=0  MultiP=1 (VS=0)  [MChngr=0]  [ACKREQQ=0]  Addr16=0</span><br><span class="line">  [RelAdr=0]  WBus16=0  Sync=0  [Linked=0]  [TranDis=0]  CmdQue=1</span><br><span class="line">  [SPI: Clocking=0x0  QAS=0  IUS=0]</span><br><span class="line">    length=164 (0xa4)   Peripheral device <span class="built_in">type</span>: disk</span><br><span class="line"> Vendor identification: HGST</span><br><span class="line"> Product identification: HUH721010AL5200</span><br><span class="line"> Product revision level: LS14</span><br><span class="line"> Unit serial number: 1DGH47ZZ</span><br><span class="line"></span><br><span class="line">$ sg_inq -e /dev/sdj</span><br><span class="line">VPD INQUIRY, page code=0x00:</span><br><span class="line">   Supported VPD pages:</span><br><span class="line">     0x0        Supported VPD pages</span><br><span class="line">     0x3</span><br><span class="line">     0x80       Unit serial number</span><br><span class="line">     0x83       Device identification</span><br><span class="line">     0x86       Extended INQUIRY data</span><br><span class="line">     0x87       Mode page policy</span><br><span class="line">     0x88       SCSI ports</span><br><span class="line">     0x8a       Power condition</span><br><span class="line">     0x8d       Power consumption</span><br><span class="line">     0x90       Protocol-specific logical unit information</span><br><span class="line">     0x91       Protocol-specific port information</span><br><span class="line">     0xb0       Block limits (sbc2)</span><br><span class="line">     0xb1       Block device characteristics (sbc3)</span><br><span class="line">     0xb2       Logical block provisioning (sbc3)</span><br><span class="line">     0xd1</span><br><span class="line">     0xd2</span><br><span class="line">     0xdc</span><br><span class="line"></span><br><span class="line">$ sg_vpd /dev/sdj</span><br><span class="line">Supported VPD pages VPD page:</span><br><span class="line">  Supported VPD pages [sv]</span><br><span class="line">  0x3</span><br><span class="line">  Unit serial number [sn]</span><br><span class="line">  Device identification [di]</span><br><span class="line">  Extended inquiry data [ei]</span><br><span class="line">  Mode page policy [mpp]</span><br><span class="line">  SCSI ports [sp]</span><br><span class="line">  Power condition [pc]</span><br><span class="line">  Power consumption [psm]</span><br><span class="line">  Protocol-specific logical unit information [pslu]</span><br><span class="line">  Protocol-specific port information [pspo]</span><br><span class="line">  Block limits (SBC) [bl]</span><br><span class="line">  Block device characteristics (SBC) [bdc]</span><br><span class="line">  Logical block provisioning (SBC) [lbpv]</span><br><span class="line">  0xd1</span><br><span class="line">  0xd2</span><br><span class="line">  0xdc</span><br><span class="line"></span><br><span class="line">$ sg_vpd --page=xx /dev/sdj</span><br><span class="line">abbreviation doesn<span class="string">'t match a VPD page</span></span><br><span class="line"><span class="string">Available standard VPD pages:</span></span><br><span class="line"><span class="string">  ai         0x89      ATA information (SAT)</span></span><br><span class="line"><span class="string">  aod        0x82      ASCII implemented operating definition (obsolete)</span></span><br><span class="line"><span class="string">  adsn       0xb3      Automation device serial number (SSC)</span></span><br><span class="line"><span class="string">  bl         0xb0      Block limits (SBC)</span></span><br><span class="line"><span class="string">  ble        0xb7      Block limits extension (SBC)</span></span><br><span class="line"><span class="string">  bdc        0xb1      Block device characteristics (SBC)</span></span><br><span class="line"><span class="string">  bdce       0xb5      Block device characteristics extension (SBC)</span></span><br><span class="line"><span class="string">  cfa        0x8c      CFA profile information</span></span><br><span class="line"><span class="string">  dc         0x8b      Device constituents</span></span><br><span class="line"><span class="string">  di         0x83      Device identification</span></span><br><span class="line"><span class="string">  di_asis    0x83      Like '</span>di<span class="string">' but designators ordered as found</span></span><br><span class="line"><span class="string">  di_lu      0x83      Device identification, lu only</span></span><br><span class="line"><span class="string">  di_port    0x83      Device identification, target port only</span></span><br><span class="line"><span class="string">  di_target  0x83      Device identification, target device only</span></span><br><span class="line"><span class="string">  dtde       0xb4      Data transfer device element address (SSC)</span></span><br><span class="line"><span class="string">  ei         0x86      Extended inquiry data</span></span><br><span class="line"><span class="string">  iod        0x81      Implemented operating definition (obsolete)</span></span><br><span class="line"><span class="string">  lbpro      0xb5      Logical block protection (SSC)</span></span><br><span class="line"><span class="string">  lbpv       0xb2      Logical block provisioning (SBC)</span></span><br><span class="line"><span class="string">  mas        0xb1      Manufacturer assigned serial number (SSC)</span></span><br><span class="line"><span class="string">  masa       0xb1      Manufacturer assigned serial number (ADC)</span></span><br><span class="line"><span class="string">  mna        0x85      Management network addresses</span></span><br><span class="line"><span class="string">  mpp        0x87      Mode page policy</span></span><br><span class="line"><span class="string">  oi         0xb0      OSD information</span></span><br><span class="line"><span class="string">  pc         0x8a      Power condition</span></span><br><span class="line"><span class="string">  psm        0x8d      Power consumption</span></span><br><span class="line"><span class="string">  pslu       0x90      Protocol-specific logical unit information</span></span><br><span class="line"><span class="string">  pspo       0x91      Protocol-specific port information</span></span><br><span class="line"><span class="string">  ref        0xb3      Referrals (SBC)</span></span><br><span class="line"><span class="string">  sad        0xb0      Sequential access device capabilities (SSC)</span></span><br><span class="line"><span class="string">  sbl        0xb4      Supported block lengths and protection types (SBC)</span></span><br><span class="line"><span class="string">  sfs        0x92      SCSI feature sets</span></span><br><span class="line"><span class="string">  sii        0x84      Software interface identification</span></span><br><span class="line"><span class="string">  sinq       -1        Standard inquiry response</span></span><br><span class="line"><span class="string">  sn         0x80      Unit serial number</span></span><br><span class="line"><span class="string">  sp         0x88      SCSI ports</span></span><br><span class="line"><span class="string">  st         0xb1      Security token (OSD)</span></span><br><span class="line"><span class="string">  sv         0x00      Supported VPD pages</span></span><br><span class="line"><span class="string">  tas        0xb2      TapeAlert supported flags (SSC)</span></span><br><span class="line"><span class="string">  tpc        0x8f      Third party copy</span></span><br><span class="line"><span class="string">  zbdc       0xb6      Zoned block device characteristics</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Vendor/product identifiers:</span></span><br><span class="line"><span class="string">  dds        3      DDS tape family from IBM</span></span><br><span class="line"><span class="string">  emc        2      EMC (company)</span></span><br><span class="line"><span class="string">  hit        7      WDC/Hitachi disk</span></span><br><span class="line"><span class="string">  hp3par     4      3PAR array (HP was Left Hand)</span></span><br><span class="line"><span class="string">  hp_lto     6      HP LTO tape/systems</span></span><br><span class="line"><span class="string">  ibm_lto    5      IBM LTO tape/systems</span></span><br><span class="line"><span class="string">  nvme       8      NVMe related</span></span><br><span class="line"><span class="string">  rdac       1      RDAC array (NetApp E-Series)</span></span><br><span class="line"><span class="string">  sea        0      Seagate disk</span></span><br><span class="line"><span class="string">  sg         9      sg3_utils extensions</span></span><br><span class="line"><span class="string">  wdc        7      WDC/Hitachi disk</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Vendor specific VPD pages:</span></span><br><span class="line"><span class="string">  aci        0xc5,6      ACI revision level (HP LTO)</span></span><br><span class="line"><span class="string">  datc       0xc1,0      Date code (Seagate)</span></span><br><span class="line"><span class="string">  dcrl       0xc0,5      Drive component revision levels (IBM LTO)</span></span><br><span class="line"><span class="string">  ddsver     0xc0,3      Firmware revision (DDS)</span></span><br><span class="line"><span class="string">  devb       0xc3,0      Device behavior (Seagate)</span></span><br><span class="line"><span class="string">  dsn        0xc1,5      Drive serial numbers (IBM LTO)</span></span><br><span class="line"><span class="string">  ducd       0xc7,5      Device unique configuration data (IBM LTO)</span></span><br><span class="line"><span class="string">  edid       0xc8,1      Extended device identification (RDAC)</span></span><br><span class="line"><span class="string">  firm       0xc0,0      Firmware numbers (Seagate)</span></span><br><span class="line"><span class="string">  frl        0xc0,6      Firmware revision level (HP LTO)</span></span><br><span class="line"><span class="string">  fwr4       0xc1,1      Firmware version (RDAC)</span></span><br><span class="line"><span class="string">  head       0xc4,6      Head Assy revision level (HP LTO)</span></span><br><span class="line"><span class="string">  hp3par     0xc0,4      Volume information (HP/3PAR)</span></span><br><span class="line"><span class="string">  hrl        0xc1,6      Hardware revision level (HP LTO)</span></span><br><span class="line"><span class="string">  hwr4       0xc0,1      Hardware version (RDAC)</span></span><br><span class="line"><span class="string">  jump       0xc2,0      Jump setting (Seagate)</span></span><br><span class="line"><span class="string">  mech       0xc3,6      Mechanism revision level (HP LTO)</span></span><br><span class="line"><span class="string">  mpds       0xc8,5      Mode parameter default settings (IBM LTO)</span></span><br><span class="line"><span class="string">  nicr       0xde,9      NVMe Identify Controller Response (sg3_utils)</span></span><br><span class="line"><span class="string">  pca        0xc2,6      PCA revision level (HP LTO)</span></span><br><span class="line"><span class="string">  prm4       0xc3,1      Feature Parameters (RDAC)</span></span><br><span class="line"><span class="string">  rvsi       0xca,1      Replicated volume source identifier (RDAC)</span></span><br><span class="line"><span class="string">  said       0xd0,1      Storage array world wide name (RDAC)</span></span><br><span class="line"><span class="string">  subs       0xc4,1      Subsystem identifier (RDAC)</span></span><br><span class="line"><span class="string">  swr4       0xc2,1      Software version (RDAC)</span></span><br><span class="line"><span class="string">  upr        0xc0,2      Unit path report (EMC)</span></span><br><span class="line"><span class="string">  vac1       0xc9,1      Volume access control (RDAC)</span></span><br><span class="line"><span class="string">  wp3        0x03,7      Page 0x3 (WDC/Hitachi)</span></span><br><span class="line"><span class="string">  wpd1       0xd1,7      Page 0xd1 (WDC/Hitachi)</span></span><br><span class="line"><span class="string">  wpd2       0xd2,7      Page 0xd2 (WDC/Hitachi)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ sg_vpd --page=sp /dev/sdj</span></span><br><span class="line"><span class="string">SCSI Ports VPD page:</span></span><br><span class="line"><span class="string">  Relative port=1</span></span><br><span class="line"><span class="string">    Target port descriptor(s):</span></span><br><span class="line"><span class="string">  SCSI Ports:</span></span><br><span class="line"><span class="string">    designator type: NAA,  code set: Binary</span></span><br><span class="line"><span class="string">     transport: Serial Attached SCSI Protocol (SPL-4)</span></span><br><span class="line"><span class="string">      0x5000cca26c1b8631</span></span><br><span class="line"><span class="string">  Relative port=2</span></span><br><span class="line"><span class="string">    Target port descriptor(s):</span></span><br><span class="line"><span class="string">  SCSI Ports:</span></span><br><span class="line"><span class="string">    designator type: NAA,  code set: Binary</span></span><br><span class="line"><span class="string">     transport: Serial Attached SCSI Protocol (SPL-4)</span></span><br><span class="line"><span class="string">      0x5000cca26c1b8632</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ sg_vpd --page=pc /dev/sdj</span></span><br><span class="line"><span class="string">Power condition VPD page:</span></span><br><span class="line"><span class="string">  Standby_y=0 Standby_z=1 Idle_c=1 Idle_b=1 Idle_a=1</span></span><br><span class="line"><span class="string">  Stopped condition recovery time (ms) 18400</span></span><br><span class="line"><span class="string">  Standby_z condition recovery time (ms) 10200</span></span><br><span class="line"><span class="string">  Standby_y condition recovery time (ms) 3350</span></span><br><span class="line"><span class="string">  Idle_a condition recovery time (ms) 10</span></span><br><span class="line"><span class="string">  Idle_b condition recovery time (ms) 900</span></span><br><span class="line"><span class="string">  Idle_c condition recovery time (ms) 3350</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$  sg_scan -x</span></span><br><span class="line"><span class="string">/dev/sg0: scsi0 channel=0 id=0 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg1: scsi0 channel=0 id=1 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg2: scsi0 channel=0 id=2 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg3: scsi0 channel=0 id=3 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg4: scsi0 channel=0 id=4 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg5: scsi0 channel=0 id=5 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg6: scsi0 channel=0 id=6 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg7: scsi0 channel=0 id=7 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg8: scsi0 channel=0 id=8 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg9: scsi0 channel=0 id=9 lun=0  cmd_per_lun=7 queue_depth=254</span></span><br><span class="line"><span class="string">/dev/sg10: scsi0 channel=0 id=10 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg11: scsi0 channel=0 id=11 lun=0  cmd_per_lun=7 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg12: scsi0 channel=0 id=12 lun=0  cmd_per_lun=7 queue_depth=254</span></span><br><span class="line"><span class="string">/dev/sg13: scsi15 channel=0 id=0 lun=0 [em]  cmd_per_lun=0 queue_depth=32</span></span><br><span class="line"><span class="string">/dev/sg14: scsi17 channel=0 id=0 lun=0 [em]  cmd_per_lun=0 queue_depth=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[0:0:0:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sda   -  /dev/sg0   10.0TB</span></span><br><span class="line"><span class="string">[0:0:1:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdb   -  /dev/sg1   10.0TB</span></span><br><span class="line"><span class="string">[0:0:2:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdc   -  /dev/sg2   10.0TB</span></span><br><span class="line"><span class="string">[0:0:3:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdd   -  /dev/sg3   10.0TB</span></span><br><span class="line"><span class="string">[0:0:4:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sde   -  /dev/sg4   10.0TB</span></span><br><span class="line"><span class="string">[0:0:5:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdf   -  /dev/sg5   10.0TB</span></span><br><span class="line"><span class="string">[0:0:6:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdg   -  /dev/sg6   10.0TB</span></span><br><span class="line"><span class="string">[0:0:7:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdh   -  /dev/sg7   10.0TB</span></span><br><span class="line"><span class="string">[0:0:8:0]    disk    ATA      HGST HUH721010AL LT14  /dev/sdi   -  /dev/sg8   10.0TB</span></span><br><span class="line"><span class="string">[0:0:9:0]    disk    HGST     HUH721010AL5200  LS14  /dev/sdj   35000cca26c1b8630  /dev/sg9   9.79TB</span></span><br><span class="line"><span class="string">[0:0:10:0]   disk    ATA      HGST HUH721010AL LT14  /dev/sdk   -  /dev/sg10  10.0TB</span></span><br><span class="line"><span class="string">[0:0:11:0]   disk    ATA      HGST HUH721010AL LT14  /dev/sdl   -  /dev/sg11  10.0TB</span></span><br><span class="line"><span class="string">[0:0:12:0]   enclosu DP       BP14G+EXP        2.25  -          -  /dev/sg12       -</span></span><br><span class="line"><span class="string">[15:0:0:0]   disk    ATA      DELLBOSS VD      00-0  /dev/sdm   -  /dev/sg13   119GB</span></span><br><span class="line"><span class="string">[17:0:0:0]   process Marvell  Console          1.01  -          -  /dev/sg14       -</span></span><br></pre></td></tr></table></figure>

<h4 id="sg-opcodes"><a href="#sg-opcodes" class="headerlink" title="sg_opcodes"></a>sg_opcodes</h4><p>sg_opcodes -n /dev/sg9</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sg_opcodes -n /dev/sg9</span><br><span class="line">--no-inquiry ignored because --pdt= not given</span><br><span class="line"></span><br><span class="line">Opcode  Service    CDB    CDLP   Name</span><br><span class="line">(hex)   action(h)  size</span><br><span class="line">-----------------------------------------------</span><br><span class="line"> 00                  6      0    Test Unit Ready</span><br><span class="line"> 01                  6      0    Rezero Unit</span><br><span class="line"> 03                  6      0    Request Sense</span><br><span class="line"> 04                  6      0    Format Unit</span><br><span class="line"> 07                  6      0    Reassign Blocks</span><br><span class="line"> 08                  6      0    Read(6)</span><br><span class="line"> 0a                  6      0    Write(6)</span><br><span class="line"> 0b                  6      0    Seek(6)</span><br><span class="line"> 12                  6      0    Inquiry</span><br><span class="line"> 15                  6      0    Mode select(6)</span><br><span class="line"> 16                  6      0    Reserve(6)</span><br><span class="line"> 17                  6      0    Release(6)</span><br><span class="line"> 1a                  6      0    Mode sense(6)</span><br><span class="line"> 1b                  6      0    Start stop unit</span><br><span class="line"> 1c                  6      0    Receive diagnostic results</span><br><span class="line"> 1d                  6      0    Send diagnostic</span><br><span class="line"> 25                 10      0    Read capacity(10)</span><br><span class="line"> 28                 10      0    Read(10)</span><br><span class="line"> 2a                 10      0    Write(10)</span><br><span class="line"> 2b                 10      0    Seek(10)</span><br><span class="line"> 2e                 10      0    Write and verify(10)</span><br><span class="line"> 2f                 10      0    Verify(10)</span><br><span class="line"> 34                 10      0    Pre-fetch(10)</span><br><span class="line"> 35                 10      0    Synchronize cache(10)</span><br><span class="line"> 37                 10      0    Read defect data(10)</span><br><span class="line"> 3b        0        10      0    Write buffer, combined header and data [or multiple modes]</span><br><span class="line"> 3b        2        10      0    Write buffer, data</span><br><span class="line"> 3b        4        10      0    Write buffer, download microcode and activate</span><br><span class="line"> 3b        5        10      0    Write buffer, download microcode, save, and activate</span><br><span class="line"> 3b        7        10      0    Write buffer, download microcode with offsets, save, and activate</span><br><span class="line"> 3b        a        10      0    Write buffer, write data to <span class="built_in">echo</span> buffer</span><br><span class="line"> 3b        d        10      0    Write buffer, download microcode with offsets, select activation events, save and defer activate</span><br><span class="line"> 3b        e        10      0    Write buffer, download microcode with offsets, save and defer activate</span><br><span class="line"> 3b        f        10      0    Write buffer, activate deferred microcode</span><br><span class="line"> 3b       1a        10      0    Write buffer, <span class="built_in">enable</span> expander comms protocol and <span class="built_in">echo</span> buffer</span><br><span class="line"> 3c        0        10      0    Read buffer(10), combined header and data [or multiple modes]</span><br><span class="line"> 3c        2        10      0    Read buffer(10), data</span><br><span class="line"> 3c        3        10      0    Read buffer(10), descriptor</span><br><span class="line"> 3c        a        10      0    Read buffer(10), <span class="built_in">read</span> data from <span class="built_in">echo</span> buffer</span><br><span class="line"> 3c        b        10      0    Read buffer(10), <span class="built_in">echo</span> buffer descriptor</span><br><span class="line"> 3c       1a        10      0    Read buffer(10), <span class="built_in">enable</span> expander comms protocol and <span class="built_in">echo</span> buffer</span><br><span class="line"> 3c       1c        10      0    Read buffer(10), error <span class="built_in">history</span></span><br><span class="line"> 3e                 10      0    Read long(10)</span><br><span class="line"> 3f                 10      0    Write long(10)</span><br><span class="line">41                 10      0    Write same(10)</span><br><span class="line"> 48        1        10      0    Sanitize, overwrite</span><br><span class="line"> 48        3        10      0    Sanitize, cryptographic erase</span><br><span class="line"> 48       1f        10      0    Sanitize, <span class="built_in">exit</span> failure mode</span><br><span class="line"> 4c                 10      0    Log select</span><br><span class="line"> 4d                 10      0    Log sense</span><br><span class="line"> 55                 10      0    Mode select(10)</span><br><span class="line"> 56                 10      0    Reserve(10)</span><br><span class="line"> 57                 10      0    Release(10)</span><br><span class="line"> 5a                 10      0    Mode sense(10)</span><br><span class="line"> 5e        0        10      0    Persistent reserve <span class="keyword">in</span>, <span class="built_in">read</span> keys</span><br><span class="line"> 5e        1        10      0    Persistent reserve <span class="keyword">in</span>, <span class="built_in">read</span> reservation</span><br><span class="line"> 5e        2        10      0    Persistent reserve <span class="keyword">in</span>, report capabilities</span><br><span class="line"> 5e        3        10      0    Persistent reserve <span class="keyword">in</span>, <span class="built_in">read</span> full status</span><br><span class="line"> 5f        0        10      0    Persistent reserve out, register</span><br><span class="line"> 5f        1        10      0    Persistent reserve out, reserve</span><br><span class="line"> 5f        2        10      0    Persistent reserve out, release</span><br><span class="line"> 5f        3        10      0    Persistent reserve out, clear</span><br><span class="line"> 5f        4        10      0    Persistent reserve out, preempt</span><br><span class="line"> 5f        5        10      0    Persistent reserve out, preempt and abort</span><br><span class="line"> 5f        6        10      0    Persistent reserve out, register and ignore existing key</span><br><span class="line"> 88                 16      0    Read(16)</span><br><span class="line"> 8a                 16      0    Write(16)</span><br><span class="line"> 8e                 16      0    Write and verify(16)</span><br><span class="line"> 8f                 16      0    Verify(16)</span><br><span class="line"> 90                 16      0    Pre-fetch(16)</span><br><span class="line"> 91                 16      0    Synchronize cache(16)</span><br><span class="line"> 93                 16      0    Write same(16)</span><br><span class="line"> 9e       10        16      0    Read capacity(16)</span><br><span class="line"> 9e       11        16      0    Read long(16)</span><br><span class="line"> 9f       11        16      0    Write long(16)</span><br><span class="line"> a0                 12      0    Report luns</span><br><span class="line"> a3        5        12      0    Report identifying information</span><br><span class="line"> a3        c        12      0    Report supported operation codes</span><br><span class="line"> a3        d        12      0    Report supported task management <span class="built_in">functions</span></span><br><span class="line"> a3        f        12      0    Report timestamp</span><br><span class="line"> a4        6        12      0    Set identifying information</span><br><span class="line"> a4        f        12      0    Set timestamp</span><br><span class="line"> a8                 12      0    Read(12)</span><br><span class="line"> aa                 12      0    Write(12)</span><br><span class="line"> ae                 12      0    Write and verify(12)</span><br><span class="line"> af                 12      0    Verify(12)</span><br><span class="line"> b7                 12      0    Read defect data(12)</span><br></pre></td></tr></table></figure>

<h4 id="sg-write-same"><a href="#sg-write-same" class="headerlink" title="sg_write_same"></a>sg_write_same</h4><p>not make sure<br>This command writes the given block NUM times to consecutive blocks on  the  DEVICE  starting  at  logical  block address LBA.<br>If these conditions are not met and the LBPRZ bit is set then the UNMAP  bit  is  ignored  and  the data-out  buffer  is  written  to the DEVICE as if the UNMAP bit was zero</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sg_write_same man page</span><br><span class="line">sg_unmap - send SCSI UNMAP <span class="built_in">command</span> (known as <span class="string">'trim'</span> <span class="keyword">in</span> ATA specs)</span><br><span class="line">$ cat /sys/block/sdb/size</span><br><span class="line">488397168</span><br><span class="line">$ sg_write_same --16 --unmap --verbose --lba=488397167 --num=1 /dev/sdb</span><br><span class="line"><span class="comment">#The same func with sg_write_same ?</span></span><br><span class="line">$ sg_unmap --verbose --lba=488397167 --num=1 /dev/sdb</span><br><span class="line"></span><br><span class="line">$ sg_unmap --verbose --lba=0 --num=488397167 /dev/sdb</span><br></pre></td></tr></table></figure>

<h4 id="sg-readcap-1"><a href="#sg-readcap-1" class="headerlink" title="sg_readcap"></a>sg_readcap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ --16   Use the 16 byte cdb variant of the READ CAPACITY <span class="built_in">command</span>. See the <span class="string">'--long'</span> option.  -b, --brief outputs two  hex  numbers  (prefixed  with  <span class="string">'0x'</span>  and  space  separated) to stdout. The first number is the maximum number of blocks on the device (<span class="built_in">which</span> is one plus the lba of the last accessible block). The second number is the size <span class="keyword">in</span> bytes of each block.If the operation fails <span class="keyword">then</span> <span class="string">"0x0 0x0"</span> is written to stdout.</span><br><span class="line"></span><br><span class="line">$ sg_readcap --16 -b /dev/sdj <span class="comment">#10TB 512E</span></span><br><span class="line">0x474800000 0x200</span><br><span class="line"></span><br><span class="line">Raw Size: 8.910 TB [0x474800000 Sectors]</span><br><span class="line">0x200 = 512 Bytes</span><br><span class="line"></span><br><span class="line">$ sg_readcap --16 -b /dev/sdj <span class="comment">#8TB 4KN</span></span><br><span class="line">0x74702556 0x1000</span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUH728080AL4200</span><br><span class="line">Revision:             A7J0</span><br><span class="line">User Capacity:        8,001,563,222,016 bytes [8.00 TB]</span><br><span class="line">Logical block size:   4096 bytes</span><br><span class="line">$ cat logical_block_size minimum_io_size physical_block_size</span><br><span class="line">4096</span><br><span class="line">4096</span><br><span class="line">4096</span><br></pre></td></tr></table></figure>

<p>Got the issue, dell SAS 10TB HDD looks like only 9.79TB, Dell hide some area, the physical sector only 4096 Bytes</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Dell SATA</span></span><br><span class="line">Model Family:     HGST Ultrastar He10</span><br><span class="line">Device Model:     HGST HUH721010ALE600</span><br><span class="line">LU WWN Device Id: 5 000cca 26aef5ec1</span><br><span class="line">Add. Product Id:  DELL(tm)</span><br><span class="line">Firmware Version: LHDELT14</span><br><span class="line">User Capacity:    10,000,831,348,736 bytes [10.0 TB]</span><br><span class="line">Sector Sizes:     512 bytes logical, 4096 bytes physical</span><br><span class="line"></span><br><span class="line"><span class="comment"># SAS 10TB</span></span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUH721010AL5200</span><br><span class="line">Revision:             A21D</span><br><span class="line">User Capacity:        10,000,831,348,736 bytes [10.0 TB]</span><br><span class="line">Logical block size:   512 bytes</span><br><span class="line">Physical block size:  4096 bytes</span><br><span class="line">Lowest aligned LBA:   0</span><br><span class="line">Logical block provisioning <span class="built_in">type</span> unreported, LBPME=0, LBPRZ=0</span><br><span class="line">Rotation Rate:        7200 rpm</span><br><span class="line">Device <span class="built_in">type</span>:          disk</span><br><span class="line">Transport protocol:   SAS</span><br><span class="line"></span><br><span class="line"><span class="comment">#Dell SAS 10TB</span></span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUH721010AL5200</span><br><span class="line">Revision:             LS14</span><br><span class="line">Compliance:           SPC-4</span><br><span class="line">User Capacity:        9,796,820,402,176 bytes [9.79 TB]</span><br><span class="line">Logical block size:   512 bytes</span><br><span class="line">Physical block size:  4096 bytes</span><br><span class="line">Formatted with <span class="built_in">type</span> 2 protection</span><br><span class="line">LU is fully provisioned</span><br><span class="line">Rotation Rate:        7200 rpm</span><br><span class="line">Form Factor:          3.5 inches</span><br><span class="line">Device <span class="built_in">type</span>:          disk</span><br><span class="line">Transport protocol:   SAS (SPL-3)</span><br></pre></td></tr></table></figure>

<h4 id="reset-bus-host-target"><a href="#reset-bus-host-target" class="headerlink" title="reset bus, host, target"></a><a href="http://fibrevillage.com/storage/593-sg-reset-command-examples" target="_blank" rel="noopener">reset bus, host, target</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reset bus</span></span><br><span class="line">$ sg_reset -v -b /dev/sdf</span><br><span class="line">[root@kvm-manager-a23-2 ~]<span class="comment"># sg_reset -v -b /dev/sdf</span></span><br><span class="line">sg_reset: starting bus reset</span><br><span class="line">sg_reset: completed bus (or host) reset</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">[Mon Jun 17 12:19:41 2019] mpt3sas_cm0: attempting host reset! scmd(ffff96107b4a6a00)</span><br><span class="line">[Mon Jun 17 12:19:41 2019] sd 0:0:3:0: CDB: Test Unit Ready</span><br><span class="line">[Mon Jun 17 12:19:41 2019] mpt3sas_cm0: sending diag reset !!</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: IOC Number : 0</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: LSISAS3008: FWVersion(15.00.03.00), ChipRevision(0x02), BiosVersion(08.31.00.00)</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: Protocol=(Initiator), Capabilities=(Raid,TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">[Mon Jun 17 12:19:42 2019] mpt3sas_cm0: sending port <span class="built_in">enable</span> !!</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:0: handle(0x000a), sas_address(0x5000c5003015b562), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:0: enclosure logical id(0x500304801d72dd7f), slot(0)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:1: handle(0x000b), sas_address(0x5000c5003015b786), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:1: enclosure logical id(0x500304801d72dd7f), slot(1)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:2: handle(0x000c), sas_address(0x5000c5003015b57a), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:2: enclosure logical id(0x500304801d72dd7f), slot(2)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:3: handle(0x000d), sas_address(0x5000c5003015b56a), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:3: enclosure logical id(0x500304801d72dd7f), slot(3)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:4: handle(0x000e), sas_address(0x5000c500301b8b06), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:4: enclosure logical id(0x500304801d72dd7f), slot(4)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:5: handle(0x000f), sas_address(0x5000c5003015b566), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:5: enclosure logical id(0x500304801d72dd7f), slot(5)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:6: handle(0x0010), sas_address(0x5000c5003015b582), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:6: enclosure logical id(0x500304801d72dd7f), slot(6)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:7: handle(0x0011), sas_address(0x5000c5003015b56e), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:7: enclosure logical id(0x500304801d72dd7f), slot(7)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:8: handle(0x0012), sas_address(0x5000c5003015b5b6), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:8: enclosure logical id(0x500304801d72dd7f), slot(8)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:9: handle(0x0013), sas_address(0x5000c500301b8b7e), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:9: enclosure logical id(0x500304801d72dd7f), slot(9)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:10: handle(0x0014), sas_address(0x500304801d72ddfd), port: 255</span><br><span class="line">[Mon Jun 17 12:19:49 2019] scsi target0:0:10: enclosure logical id(0x500304801d72dd7f), slot(24)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from _scsih_search_responding_sas_devices: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: complete</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> PCIe end-devices: complete</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> raid volumes: start</span><br><span class="line">[Mon Jun 17 12:19:49 2019] mpt3sas_cm0: search <span class="keyword">for</span> responding raid volumes: complete</span><br><span class="line">[Mon Jun 17 12:19:50 2019] mpt3sas_cm0: search <span class="keyword">for</span> expanders: start</span><br><span class="line">[Mon Jun 17 12:19:50 2019] 	expander present: handle(0x0009), sas_addr(0x500304801d72ddff), port:255</span><br><span class="line">[Mon Jun 17 12:19:50 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from _scsih_search_responding_expanders: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:19:50 2019] mpt3sas_cm0: search <span class="keyword">for</span> expanders: complete</span><br><span class="line">[Mon Jun 17 12:19:50 2019] mpt3sas_cm0: host reset: SUCCESS scmd(ffff96107b4a6a00)</span><br><span class="line">[Mon Jun 17 12:20:00 2019]  sdf: sdf1 sdf9</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: sas end-devices</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: pcie end-devices</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: volumes</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: expanders</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: removing unresponding devices: complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: scan devices: start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: expanders start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from expander scan: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: expanders complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: phys disk start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from phys disk scan: ioc_status(0x0022), loginfo(0x00000000)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: phys disk complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: volumes start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from volume scan: ioc_status(0x0022), loginfo(0x00000000)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: volumes complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: sas end devices start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from sas end device scan: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: sas end devices complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	scan devices: pcie end devices start</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from pcie end device scan: ioc_status(0x0021), loginfo(0x3003011d)</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: 	pcie devices: pcie end devices complete</span><br><span class="line">[Mon Jun 17 12:20:00 2019] mpt3sas_cm0: scan devices: complete</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset host</span></span><br><span class="line">$ sg_reset -v -H /dev/sdf</span><br><span class="line">sg_reset: starting host reset</span><br><span class="line">sg_reset: completed host reset</span><br><span class="line"></span><br><span class="line">[Mon Jun 17 12:22:10 2019] mpt3sas_cm0: attempting host reset! scmd(ffff9620717edc00)</span><br><span class="line">[Mon Jun 17 12:22:10 2019] sd 0:0:3:0: CDB: Test Unit Ready</span><br><span class="line">[Mon Jun 17 12:22:10 2019] mpt3sas_cm0: sending diag reset !!</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: IOC Number : 0</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: LSISAS3008: FWVersion(15.00.03.00), ChipRevision(0x02), BiosVersion(08.31.00.00)</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: Protocol=(Initiator), Capabilities=(Raid,TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">[Mon Jun 17 12:22:11 2019] mpt3sas_cm0: sending port <span class="built_in">enable</span> !!</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:0: handle(0x000a), sas_address(0x5000c5003015b562), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:0: enclosure logical id(0x500304801d72dd7f), slot(0)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:1: handle(0x000b), sas_address(0x5000c5003015b786), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:1: enclosure logical id(0x500304801d72dd7f), slot(1)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:2: handle(0x000c), sas_address(0x5000c5003015b57a), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:2: enclosure logical id(0x500304801d72dd7f), slot(2)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:3: handle(0x000d), sas_address(0x5000c5003015b56a), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:3: enclosure logical id(0x500304801d72dd7f), slot(3)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:4: handle(0x000e), sas_address(0x5000c500301b8b06), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:4: enclosure logical id(0x500304801d72dd7f), slot(4)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:5: handle(0x000f), sas_address(0x5000c5003015b566), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:5: enclosure logical id(0x500304801d72dd7f), slot(5)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:6: handle(0x0010), sas_address(0x5000c5003015b582), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:6: enclosure logical id(0x500304801d72dd7f), slot(6)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:7: handle(0x0011), sas_address(0x5000c5003015b56e), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:7: enclosure logical id(0x500304801d72dd7f), slot(7)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:8: handle(0x0012), sas_address(0x5000c5003015b5b6), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:8: enclosure logical id(0x500304801d72dd7f), slot(8)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:9: handle(0x0013), sas_address(0x5000c500301b8b7e), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:9: enclosure logical id(0x500304801d72dd7f), slot(9)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:10: handle(0x0014), sas_address(0x500304801d72ddfd), port: 255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] scsi target0:0:10: enclosure logical id(0x500304801d72dd7f), slot(24)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from _scsih_search_responding_sas_devices: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: complete</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> PCIe end-devices: complete</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> raid volumes: start</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> responding raid volumes: complete</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> expanders: start</span><br><span class="line">[Mon Jun 17 12:22:18 2019] 	expander present: handle(0x0009), sas_addr(0x500304801d72ddff), port:255</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from _scsih_search_responding_expanders: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: search <span class="keyword">for</span> expanders: complete</span><br><span class="line">[Mon Jun 17 12:22:18 2019] mpt3sas_cm0: host reset: SUCCESS scmd(ffff9620717edc00)</span><br><span class="line">[Mon Jun 17 12:22:28 2019]  sdf: sdf1 sdf9</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: sas end-devices</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: pcie end-devices</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: volumes</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: expanders</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: removing unresponding devices: complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: scan devices: start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: expanders start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from expander scan: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: expanders complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: phys disk start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from phys disk scan: ioc_status(0x0022), loginfo(0x00000000)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: phys disk complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: volumes start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from volume scan: ioc_status(0x0022), loginfo(0x00000000)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: volumes complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: sas end devices start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from sas end device scan: ioc_status(0x0022), loginfo(0x310f0400)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: sas end devices complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	scan devices: pcie end devices start</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: log_info(0x3003011d): originator(IOP), code(0x03), sub_code(0x011d)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	<span class="built_in">break</span> from pcie end device scan: ioc_status(0x0021), loginfo(0x3003011d)</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: 	pcie devices: pcie end devices complete</span><br><span class="line">[Mon Jun 17 12:22:28 2019] mpt3sas_cm0: scan devices: complete</span><br><span class="line"></span><br><span class="line"><span class="comment"># reset target, I guess there is the same with -d, looks like I 'm right</span></span><br><span class="line">$ sg_reset -v -t /dev/sdf</span><br><span class="line">sg_reset: starting target reset</span><br><span class="line">sg_reset: completed target (or bus or host) reset</span><br><span class="line"></span><br><span class="line">[Mon Jun 17 12:23:27 2019] scsi target0:0:3: attempting target reset! scmd(ffff961e69a2f2c0)</span><br><span class="line">[Mon Jun 17 12:23:27 2019] sd 0:0:3:0: CDB: Test Unit Ready</span><br><span class="line">[Mon Jun 17 12:23:27 2019] scsi target0:0:3: _scsih_tm_display_info: handle(0x000d), sas_address(0x5000c5003015b56a), phy(15)</span><br><span class="line">[Mon Jun 17 12:23:27 2019] scsi target0:0:3: enclosurelogical id(0x500304801d72dd7f), slot(3)</span><br><span class="line">[Mon Jun 17 12:23:27 2019] scsi target0:0:3: enclosure level(0x0000), connector name(     )</span><br><span class="line">[Mon Jun 17 12:23:27 2019] scsi target0:0:3: target reset: SUCCESS scmd(ffff961e69a2f2c0)</span><br><span class="line">[Mon Jun 17 12:23:27 2019]  sdf: sdf1 sdf9</span><br><span class="line"></span><br><span class="line">$ sg_reset -v -d /dev/sdf</span><br><span class="line">sg_reset: starting device reset</span><br><span class="line">sg_reset: completed device (or target or bus or host) reset</span><br><span class="line"></span><br><span class="line">[Mon Jun 17 12:24:35 2019] sd 0:0:3:0: attempting device reset! scmd(ffff9620717edf80)</span><br><span class="line">[Mon Jun 17 12:24:35 2019] sd 0:0:3:0: CDB: Test Unit Ready</span><br><span class="line">[Mon Jun 17 12:24:35 2019] scsi target0:0:3: _scsih_tm_display_info: handle(0x000d), sas_address(0x5000c5003015b56a), phy(15)</span><br><span class="line">[Mon Jun 17 12:24:35 2019] scsi target0:0:3: enclosurelogical id(0x500304801d72dd7f), slot(3)</span><br><span class="line">[Mon Jun 17 12:24:35 2019] scsi target0:0:3: enclosure level(0x0000), connector name(     )</span><br><span class="line">[Mon Jun 17 12:24:35 2019] sd 0:0:3:0: device reset: SUCCESS scmd(ffff9620717edf80)</span><br><span class="line">[Mon Jun 17 12:24:35 2019]  sdf: sdf1 sdf9</span><br></pre></td></tr></table></figure>

<h3 id="mpt2sas-driver-layer"><a href="#mpt2sas-driver-layer" class="headerlink" title="mpt2sas driver layer"></a>mpt2sas driver layer</h3><p>Because HBA connect with JBOD will cause some of counts<br>I will record all counts at boot time. compare these counts with future<br>If you found some number increase that means there are some errors in sas driver layer</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">lsscsi -g &gt; /dev/shm/lsscsi.log</span><br><span class="line"><span class="built_in">cd</span> /sys/class/sas_phy;<span class="keyword">for</span> i <span class="keyword">in</span> $(ls); <span class="keyword">do</span></span><br><span class="line">value=$(cat <span class="variable">$i</span>/running_disparity_error_count)</span><br><span class="line">value2=$(cat <span class="variable">$i</span>/phy_reset_problem_count)</span><br><span class="line">value3=$(cat <span class="variable">$i</span>/loss_of_dword_sync_count)</span><br><span class="line">value4=$(cat <span class="variable">$i</span>/invalid_dword_count)</span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">$value</span> -gt 0 ]] || [[ <span class="variable">$value2</span> -gt 0 ]] || [[ <span class="variable">$value3</span> -gt 0 ]] || [[ <span class="variable">$value4</span> -gt 0 ]]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">  devtype=$(cat /dev/shm/lsscsi.log | grep -i <span class="variable">$i</span> | awk <span class="string">'&#123;print $1,$NF&#125;'</span>)</span><br><span class="line">  <span class="built_in">echo</span> -n <span class="variable">$i</span><span class="string">"-- "</span></span><br><span class="line">  <span class="built_in">echo</span> -n running_disparity_error_count<span class="string">":"</span><span class="variable">$value</span><span class="string">" "</span><span class="variable">$devtype</span><span class="string">" "</span></span><br><span class="line">  <span class="built_in">echo</span> -n phy_reset_problem_count<span class="string">":"</span><span class="variable">$value2</span><span class="string">" "</span><span class="variable">$devtype</span><span class="string">" "</span></span><br><span class="line">  <span class="built_in">echo</span> -n loss_of_dword_sync_count<span class="string">":"</span><span class="variable">$value3</span><span class="string">" "</span><span class="variable">$devtype</span><span class="string">" "</span></span><br><span class="line">  <span class="built_in">echo</span> invalid_dword_count<span class="string">":"</span><span class="variable">$value4</span><span class="string">" "</span><span class="variable">$devtype</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">phy-0:0:13-- running_disparity_error_count:1  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:1</span><br><span class="line">phy-0:0:15-- running_disparity_error_count:0  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:5</span><br><span class="line">phy-0:0:17-- running_disparity_error_count:0  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:5</span><br><span class="line">phy-0:0:19-- running_disparity_error_count:1  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:1</span><br><span class="line">phy-0:0:21-- running_disparity_error_count:0  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:6</span><br><span class="line">phy-0:0:23-- running_disparity_error_count:1  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:5</span><br><span class="line">phy-0:0:25-- running_disparity_error_count:0  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:6</span><br><span class="line">phy-0:0:27-- running_disparity_error_count:0  phy_reset_problem_count:0  loss_of_dword_sync_count:1  invalid_dword_count:5</span><br><span class="line">phy-12:2:3-- running_disparity_error_count:0  phy_reset_problem_count:1  loss_of_dword_sync_count:0  invalid_dword_count:0</span><br><span class="line">phy-14:1:22-- running_disparity_error_count:2  phy_reset_problem_count:4  loss_of_dword_sync_count:4  invalid_dword_count:30</span><br></pre></td></tr></table></figure>


<p>Counters that are defined in SAS 1.1 Standard:</p>
<ul>
<li><p>Invalid DWORDs<br>A ‘dword’ that is not a data word or a primitive (i.e., in the character context, a dword that contains an invalid character, a control character not in the first character position, a control character other than K28.3 or K28.5 in the first character position or one or more characters with a ‘running disparity’ error). This could mark the beginning of a loss of dword synchronization. Synchronization is lost after the fourth non-nullified invalid dword (if followed by a valid dword).</p>
</li>
<li><p>Phy reset problem<br>Number of times a PHY reset problem occurs. When a PHY or link is reset, it will run through its reset sequence (OOB, Speed Negotiation, Multiplexing, and Identification).</p>
</li>
</ul>
<p><a href="https://www.seagate.com/staticfiles/support/disc/manuals/Interface%20manuals/100293071c.pdf" target="_blank" rel="noopener">for more scsi info</a></p>
<ul>
<li><p>Running disparity error count<br>A binary value indicating the cumulative encoded signal imbalance between the one and zero signal state of all characters since dword synchronization has been achieved.<br>Cumulative encoded signal imbalance between the one and zero signal state. Any dwords with one or more Running Disparity Errors will be considered as invalid dwords.</p>
</li>
<li><p>Loss of DWORD synchroization count<br>If occur loss_of_dword_sync, the sas protocol will retrans the bad data ? if occur multiple times at the short time(half an hour), the HBA will reset the link (from driver ? HBA itself ? )</p>
</li>
</ul>
<p>These errors mean that there is a problem in the link between the drive and the upstream port, if you have a cable in there the cable may be bad, if not it means one of the port is bad. Ofcourse even if you have a cable it can still mean one of the ports is bad.<br>After dword synchronization has been achieved, this machine monitors invalid dwords that are received. When an invalid dword is detected, it requires two valid dwords to nullify its effect. When four invalid dwords are detected without nullification, dword synchronization is considered lost.</p>
<p>The way to diagnose it is to use a different disk in the same slot and see if the error goes away or not, if it went away the disk is bad. If the error stayed the original disk is fine but the port on the server/chassis is bad and the server/chassis needs to be replaced.<br>The issue with loss of dword synchronization is that it means additional retries for some sent IOs and it will increase the latency of IOs by way of waiting more for data transmission due to these retransmits.</p>
<p>Errors are collected from PHYs on:<br>    SAS expanders.<br>    SAS disks.<br>    SAS I/O protocol ASICs.</p>
<p>SATA has its phy error count, just not same with T10, it ‘s T13<br><a href="https://kb.netapp.com/app/answers/answer_view/a_id/1072439/~/how-to-troubleshoot-a-sas-e-series-storage-system-using-the-sasphyerrorlogs.csv" target="_blank" rel="noopener">Note: SATA drives do not report any errors. As these are SATA devices, they do not have the same reporting functionality as their SAS counterparts.</a></p>
<h4 id="mpt2sas-debug-code"><a href="#mpt2sas-debug-code" class="headerlink" title="mpt2sas debug code"></a>mpt2sas debug code</h4><p>Example: this enables firmware events and reply with additional info<br>#insmod mpt2sas.ko logging_level=0x218<br>Example: this enables handshake and initialization logging<br>#insmod mpt2sas.ko logging_level=0x420<br>Example: this enables application using IOCTLS logging<br>#insmod mpt2sas.ko logging_level=0x8000<br>Example: this enables manufacture configuration logging<br>#insmod mpt2sas.ko logging_level=0x800<br>Example: this enables host reset and task management logging<br>#insmod mpt2sas.ko logging_level=0x2100<br>Example: this enables task set full logging<br>#insmod mpt2sas.ko logging_level=0x80000</p>
<p>NOTE: Many of the driver debug prints are using KERN_DEBUG and KERN_INFO logging level. Red Hat and SuSE tend to set the default logging level set to a higher level, perhaps KERN_WARNING. When set to KERN_WARNING you will be missing most the debug info. To turn on the additional logs, you will need to see the set klogd to KERN_DEBUG. In both SuSE and Red Hat offer configuration of klogd from the file /etc/sysconfig/syslog. Please refer to the klogd manual page for more info.</p>
<h5 id="mpt2sas-fwfault-debug"><a href="#mpt2sas-fwfault-debug" class="headerlink" title="mpt2sas_fwfault_debug"></a><a href="https://git.congatec.com/arm/qmx6_kernel/commit/fa7f31673583a6e0876f8bb420735cdd8a3ffa57" target="_blank" rel="noopener">mpt2sas_fwfault_debug</a></h5><p>Added command line option and shost sysfs attribute called mpt2sas_fwfault_debug. When enduser writes a “1” to this parameter, this will enable support in the driver for debugging firmware timeout related issues.</p>
<p>This handling was added in three areas<br>(a) scsi error handling callback called task_abort<br>(b) IOCTL interface<br>(c) other timeouts that result in diag resets, such as manufacturing config pages.<br>When this support is enabled, the driver will provide dump_stack to console, halt controller firmware, and panic driver. The end user probably would want to setup serial console redirection so the dump stack can be seen.</p>
<p>Here are the three methods for enable this support:</p>
<p>(a) # insmod mpt2sas.ko mpt2sas_fwfault_debug=1<br>(b) # echo 1 &gt; /sys/module/mpt2sas/parameters/mpt2sas_fwfault_debug<br>(c) # echo 1 &gt; /sys/class/scsi_host/host#/fwfault_debug  (where # is the host number)</p>
<p>At this time, there is some issues in HBA, you could compare with these time stamps.<br>The hardware(HBA) throw out a lot of error log to kernel<br>It ‘s not the kernel behavior, it ‘s the error log from HBA</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Feb 14 10:53:28 host1.example kernel: : mpt2sas0: fault_state(0x5841)!</span><br><span class="line">Feb 14 10:53:28 host1.example kernel: : mpt2sas0: sending diag reset !!</span><br><span class="line">...</span><br><span class="line">Feb 14 11:14:27 host1.example kernel: : mpt2sas0: fault_state(0x5841)!</span><br><span class="line">Feb 14 11:14:27 host1.example kernel: : mpt2sas0: sending diag reset !</span><br><span class="line">...</span><br><span class="line">Feb 14 11:25:30 host1.example kernel: : mpt2sas0: fault_state(0x5841)!</span><br><span class="line">Feb 14 11:25:30 host1.example kernel: : mpt2sas0: sending diag reset !!</span><br><span class="line">Feb 14 11:25:30 host1.example kernel: : mpt2sas0: fault_state(0x5841)!</span><br><span class="line">Feb 14 11:25:30 host1.example kernel: : mpt2sas0: sending diag reset !!</span><br><span class="line">Feb 14 11:25:31 host1.example kernel: : mpt2sas0: diag reset: SUCCESS</span><br><span class="line">Feb 14 11:25:31 host1.example kernel: : mpt2sas0: diag reset: SUCCESS</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: LSISAS2008: FWVersion(08.00.09.00), ChipRevision(0x03), BiosVersion(07.15.00.00)</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: Protocol&#x3D;(Initiator,Target), Capabilities&#x3D;(Raid,TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: sending port enable !!</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: port enable: SUCCESS</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_sas_devices</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: LSISAS2008: FWVersion(08.00.09.00), ChipRevision(0x03), BiosVersion(07.15.00.00)</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: Protocol&#x3D;(Initiator,Target), Capabilities&#x3D;(Raid,TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: sending port enable !!</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: port enable: SUCCESS</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_sas_devices</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_raid_devices</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_raid_devices</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_expanders</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _base_fault_reset_work: hard reset: success</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _scsih_search_responding_expanders</span><br><span class="line">Feb 14 11:25:38 host1.example kernel: : mpt2sas0: _base_fault_reset_work: hard reset: success</span><br><span class="line">...</span><br><span class="line">Feb 14 13:46:48 host1.example kernel: : mpt2sas0: fault_state(0x5841)!</span><br><span class="line">Feb 14 13:46:48 host1.example kernel: : mpt2sas0: sending diag reset !!</span><br><span class="line">&#96;&#96;&#96; </span><br><span class="line"></span><br><span class="line">### SATA log</span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line"># smartctl man page</span><br><span class="line">error  -  [ATA]  prints  the Summary SMART error log. SMART disks maintain a log of the most recent five non-trivial errors. For each of these errors, the disk power-on lifetime at which the error occurred is recorded, as is the device status  (idle, standby,  etc)  at  the time of the error.  For some common types of errors, the Error Register (ER) and Status Register (SR) values are decoded and printed as text. The meanings of these are:</span><br><span class="line">   ABRT:  Command ABoRTed</span><br><span class="line">   AMNF:  Address Mark Not Found</span><br><span class="line">   CCTO:  Command Completion Timed Out</span><br><span class="line">   EOM:   End Of Media</span><br><span class="line">   ICRC:  Interface Cyclic Redundancy Code (CRC) error</span><br><span class="line">   IDNF:  IDentity Not Found</span><br><span class="line">   ILI:   (packet command-set specific)</span><br><span class="line">   MC:    Media Changed</span><br><span class="line">   MCR:   Media Change Request</span><br><span class="line">   NM:    No Media</span><br><span class="line">   obs:   obsolete</span><br><span class="line">   TK0NF: TracK 0 Not Found</span><br><span class="line">   UNC:   UNCorrectable Error in Data</span><br><span class="line">   WP:    Media is Write Protected</span><br><span class="line"></span><br><span class="line">$ smartctl -l statphy &#x2F;dev&#x2F;sdb&quot;</span><br><span class="line">smartctl 6.6 2017-11-05 r4594 [x86_64-linux-4.18.0-80.11.2.el8_0.x86_64] (local build)</span><br><span class="line">Copyright (C) 2002-17, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; INVALID ARGUMENT TO -l: statphy</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt; VALID ARGUMENTS ARE: error, selftest, selective, directory[,g|s], xerror[,N][,error], xselftest[,N][,selftest], background, sasphy[,reset], sataphy[,reset], scttemp[sts,hist], scttempint,N[,p], scterc[,N,M], devstat[,N], defects[,N], ssd, gplog,N[,RANGE], smartlog,N[,RANGE], nvmelog,N,SIZE &lt;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">Use smartctl -h to get a usage summary</span><br><span class="line"></span><br><span class="line">$ smartctl -l sataphy &#x2F;dev&#x2F;sdb</span><br><span class="line">smartctl 6.6 2017-11-05 r4594 [x86_64-linux-4.18.0-80.11.2.el8_0.x86_64] (local build)</span><br><span class="line">Copyright (C) 2002-17, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">SATA Phy Event Counters (GP Log 0x11)</span><br><span class="line">ID      Size     Value  Description</span><br><span class="line">0x0001  2            0  Command failed due to ICRC error</span><br><span class="line">0x0002  2            0  R_ERR response for data FIS</span><br><span class="line">0x0003  2            0  R_ERR response for device-to-host data FIS</span><br><span class="line">0x0004  2            0  R_ERR response for host-to-device data FIS</span><br><span class="line">0x0005  2            0  R_ERR response for non-data FIS</span><br><span class="line">0x0006  2            0  R_ERR response for device-to-host non-data FIS</span><br><span class="line">0x0007  2            0  R_ERR response for host-to-device non-data FIS</span><br><span class="line">0x0008  2            0  Device-to-host non-data FIS retries</span><br><span class="line">0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy</span><br><span class="line">0x000a  2            1  Device-to-host register FISes sent due to a COMRESET</span><br><span class="line">0x000b  2            0  CRC errors within host-to-device FIS</span><br><span class="line">0x000d  2            0  Non-CRC errors within host-to-device FIS</span><br><span class="line"></span><br><span class="line">#except phy error, the smart attr will show some error too</span><br><span class="line">SMART 5 - Reallocated_Sector_Count.</span><br><span class="line">SMART 187 - Reported_Uncorrectable_Errors.</span><br><span class="line">SMART 188 - Command_Timeout.</span><br><span class="line">SMART 197 - Current_Pending_Sector_Count.</span><br><span class="line">SMART 198 - Offline_Uncorrectable</span><br><span class="line"></span><br><span class="line">$ smartclt -l defects -x &#x2F;dev&#x2F;sdx</span><br><span class="line">&#x3D;&#x3D;&#x3D; START OF INFORMATION SECTION &#x3D;&#x3D;&#x3D;</span><br><span class="line">Model Family:     HGST Ultrastar He10</span><br><span class="line">Device Model:     HGST HUH721010ALE600</span><br><span class="line">Serial Number:    XXXXXXXXXX</span><br><span class="line">LU WWN Device Id: 5 000cca xxxxxxxx</span><br><span class="line">Add. Product Id:  XXXXXX</span><br><span class="line">Firmware Version: LHDELT14</span><br><span class="line">User Capacity:    10,000,831,348,736 bytes [10.0 TB]</span><br><span class="line">Sector Sizes:     512 bytes logical, 4096 bytes physical</span><br><span class="line">Rotation Rate:    7200 rpm</span><br><span class="line">Form Factor:      3.5 inches</span><br><span class="line">Device is:        In smartctl database [for details use: -P show]</span><br><span class="line">ATA Version is:   ACS-2, ATA8-ACS T13&#x2F;1699-D revision 4</span><br><span class="line">SATA Version is:  SATA 3.2, 6.0 Gb&#x2F;s (current: 6.0 Gb&#x2F;s)</span><br><span class="line">Local Time is:    Tue Dec 10 11:18:58 2019 EST</span><br><span class="line">SMART support is: Available - device has SMART capability.</span><br><span class="line">SMART support is: Enabled</span><br><span class="line">AAM feature is:   Unavailable</span><br><span class="line">APM feature is:   Disabled</span><br><span class="line">Rd look-ahead is: Enabled</span><br><span class="line">Write cache is:   Disabled</span><br><span class="line">DSN feature is:   Unavailable</span><br><span class="line">ATA Security is:  Disabled, NOT FROZEN [SEC1]</span><br><span class="line">Wt Cache Reorder: Enabled</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&#x3D; START OF READ SMART DATA SECTION &#x3D;&#x3D;&#x3D;</span><br><span class="line">SMART overall-health self-assessment test result: PASSED</span><br><span class="line"></span><br><span class="line">General SMART Values:</span><br><span class="line">Offline data collection status:  (0x82) Offline data collection activity</span><br><span class="line">                                        was completed without error.</span><br><span class="line">                                        Auto Offline Data Collection: Enabled.</span><br><span class="line">Self-test execution status:      (  40) The self-test routine was interrupted</span><br><span class="line">                                        by the host with a hard or soft reset.</span><br><span class="line">Total time to complete Offline</span><br><span class="line">data collection:                (   90) seconds.</span><br><span class="line">Offline data collection</span><br><span class="line">capabilities:                    (0x5b) SMART execute Offline immediate.</span><br><span class="line">                                        Auto Offline data collection on&#x2F;off support.</span><br><span class="line">                                        Suspend Offline collection upon new</span><br><span class="line">                                        command.</span><br><span class="line">                                        Offline surface scan supported.</span><br><span class="line">                                        Self-test supported.</span><br><span class="line">                                        No Conveyance Self-test supported.</span><br><span class="line">                                        Selective Self-test supported.</span><br><span class="line">SMART capabilities:            (0x0003) Saves SMART data before entering</span><br><span class="line">                                        power-saving mode.</span><br><span class="line">                                        Supports SMART auto save timer.</span><br><span class="line">Error logging capability:        (0x01) Error logging supported.</span><br><span class="line">                                        General Purpose Logging supported.</span><br><span class="line">Short self-test routine</span><br><span class="line">recommended polling time:        (   2) minutes.</span><br><span class="line">Extended self-test routine</span><br><span class="line">recommended polling time:        (1063) minutes.</span><br><span class="line">SCT capabilities:              (0x003d) SCT Status supported.</span><br><span class="line">                                        SCT Error Recovery Control supported.</span><br><span class="line">                                        SCT Feature Control supported.</span><br><span class="line">                                        SCT Data Table supported.</span><br><span class="line"></span><br><span class="line">SMART Attributes Data Structure revision number: 16</span><br><span class="line">Vendor Specific SMART Attributes with Thresholds:</span><br><span class="line">ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE</span><br><span class="line">  1 Raw_Read_Error_Rate     PO-R--   100   100   016    -    0</span><br><span class="line">  2 Throughput_Performance  --S---   134   134   000    -    96</span><br><span class="line">  3 Spin_Up_Time            POS---   191   191   024    -    344 (Average 344)</span><br><span class="line">  4 Start_Stop_Count        -O--C-   100   100   000    -    33</span><br><span class="line">  5 Reallocated_Sector_Ct   PO--CK   100   100   005    -    0</span><br><span class="line">  7 Seek_Error_Rate         -O-R--   100   100   000    -    0</span><br><span class="line">  8 Seek_Time_Performance   --S---   128   128   000    -    18</span><br><span class="line">  9 Power_On_Hours          -O--C-   099   099   000    -    7599</span><br><span class="line"> 10 Spin_Retry_Count        -O--C-   100   100   000    -    0</span><br><span class="line"> 12 Power_Cycle_Count       -O--CK   100   100   000    -    33</span><br><span class="line"> 22 Helium_Level            PO---K   100   100   025    -    100</span><br><span class="line">192 Power-Off_Retract_Count -O--CK   100   100   000    -    547</span><br><span class="line">193 Load_Cycle_Count        -O--C-   100   100   000    -    547</span><br><span class="line">194 Temperature_Celsius     -O----   206   206   000    -    29 (Min&#x2F;Max 24&#x2F;32)</span><br><span class="line">196 Reallocated_Event_Count -O--CK   100   100   000    -    0</span><br><span class="line">197 Current_Pending_Sector  -O---K   100   100   000    -    0</span><br><span class="line">198 Offline_Uncorrectable   ---R--   100   100   000    -    0</span><br><span class="line">199 UDMA_CRC_Error_Count    -O-R--   200   200   000    -    0</span><br><span class="line">223 Load_Retry_Count        -O-R--   100   100   000    -    0</span><br><span class="line">241 Total_LBAs_Written      -O--C-   100   100   000    -    50732585533</span><br><span class="line">242 Total_LBAs_Read         -O--C-   100   100   000    -    26707853436</span><br><span class="line">                            ||||||_ K auto-keep</span><br><span class="line">                            |||||__ C event count</span><br><span class="line">                            ||||___ R error rate</span><br><span class="line">                            |||____ S speed&#x2F;performance</span><br><span class="line">                            ||_____ O updated online</span><br><span class="line">                            |______ P prefailure warning</span><br><span class="line"></span><br><span class="line">General Purpose Log Directory Version 1</span><br><span class="line">SMART           Log Directory Version 1 [multi-sector log support]</span><br><span class="line">Address    Access  R&#x2F;W   Size  Description</span><br><span class="line">0x00       GPL,SL  R&#x2F;O      1  Log Directory</span><br><span class="line">0x01           SL  R&#x2F;O      1  Summary SMART error log</span><br><span class="line">0x02           SL  R&#x2F;O      1  Comprehensive SMART error log</span><br><span class="line">0x03       GPL     R&#x2F;O      1  Ext. Comprehensive SMART error log</span><br><span class="line">0x04       GPL     R&#x2F;O    256  Device Statistics log</span><br><span class="line">0x04       SL      R&#x2F;O    255  Device Statistics log</span><br><span class="line">0x06           SL  R&#x2F;O      1  SMART self-test log</span><br><span class="line">0x07       GPL     R&#x2F;O      1  Extended self-test log</span><br><span class="line">0x08       GPL     R&#x2F;O      2  Power Conditions log</span><br><span class="line">0x09           SL  R&#x2F;W      1  Selective self-test log</span><br><span class="line">0x0c       GPL     R&#x2F;O   5501  Pending Defects log</span><br><span class="line">0x10       GPL     R&#x2F;O      1  NCQ Command Error log</span><br><span class="line">0x11       GPL     R&#x2F;O      1  SATA Phy Event Counters log</span><br><span class="line">0x12       GPL     R&#x2F;O      1  SATA NCQ Non-Data log</span><br><span class="line">0x13       GPL     R&#x2F;O      1  SATA NCQ Send and Receive log</span><br><span class="line">0x21       GPL     R&#x2F;O      1  Write stream error log</span><br><span class="line">0x22       GPL     R&#x2F;O      1  Read stream error log</span><br><span class="line">0x24       GPL     R&#x2F;O    256  Current Device Internal Status Data log</span><br><span class="line">0x25       GPL     R&#x2F;O    256  Saved Device Internal Status Data log</span><br><span class="line">0x30       GPL,SL  R&#x2F;O      9  IDENTIFY DEVICE data log</span><br><span class="line">0x80-0x9f  GPL,SL  R&#x2F;W     16  Host vendor specific log</span><br><span class="line">0xdf       GPL,SL  VS       1  Device vendor specific log</span><br><span class="line">0xe0       GPL,SL  R&#x2F;W      1  SCT Command&#x2F;Status</span><br><span class="line">0xe1       GPL,SL  R&#x2F;W      1  SCT Data Transfer</span><br><span class="line"></span><br><span class="line">SMART Extended Comprehensive Error Log Version: 1 (1 sectors)</span><br><span class="line">No Errors Logged</span><br><span class="line"></span><br><span class="line">SMART Extended Self-test Log Version: 1 (1 sectors)</span><br><span class="line">Num  Test_Description    Status                  Remaining  LifeTime(hours)  LBA_of_first_error</span><br><span class="line"># 1  Short offline       Interrupted (host reset)      80%      7288         -</span><br><span class="line"># 2  Short offline       Interrupted (host reset)      80%      5140         -</span><br><span class="line"># 3  Short offline       Interrupted (host reset)      80%      3438         -</span><br><span class="line"># 4  Short offline       Interrupted (host reset)      80%      3241         -</span><br><span class="line"># 5  Short offline       Interrupted (host reset)      80%      3196         -</span><br><span class="line"># 6  Short offline       Interrupted (host reset)      80%      3194         -</span><br><span class="line"># 7  Short offline       Interrupted (host reset)      80%      3102         -</span><br><span class="line"># 8  Short offline       Interrupted (host reset)      80%      3098         -</span><br><span class="line"># 9  Short offline       Interrupted (host reset)      80%      3080         -</span><br><span class="line">#10  Short offline       Interrupted (host reset)      80%      3080         -</span><br><span class="line">#11  Short offline       Interrupted (host reset)      80%      3080         -</span><br><span class="line">#12  Short offline       Interrupted (host reset)      80%      3080         -</span><br><span class="line">#13  Short offline       Interrupted (host reset)      80%      3079         -</span><br><span class="line">#14  Short offline       Interrupted (host reset)      80%      2574         -</span><br><span class="line">#15  Short offline       Interrupted (host reset)      80%      1013         -</span><br><span class="line">#16  Short offline       Interrupted (host reset)      70%      1011         -</span><br><span class="line">#17  Short offline       Interrupted (host reset)      70%       938         -</span><br><span class="line">#18  Short offline       Interrupted (host reset)      70%       814         -</span><br><span class="line">#19  Short offline       Interrupted (host reset)      70%       281         -</span><br><span class="line"></span><br><span class="line">SMART Selective self-test log data structure revision number 1</span><br><span class="line"> SPAN  MIN_LBA  MAX_LBA  CURRENT_TEST_STATUS</span><br><span class="line">    1        0        0  Not_testing</span><br><span class="line">    2        0        0  Not_testing</span><br><span class="line">    3        0        0  Not_testing</span><br><span class="line">    4        0        0  Not_testing</span><br><span class="line">    5        0        0  Not_testing</span><br><span class="line">Selective self-test flags (0x0):</span><br><span class="line">  After scanning selected spans, do NOT read-scan remainder of disk.</span><br><span class="line">If Selective self-test is pending on power-up, resume after 0 minute delay.</span><br><span class="line"></span><br><span class="line">SCT Status Version:                  3</span><br><span class="line">SCT Version (vendor specific):       256 (0x0100)</span><br><span class="line">SCT Support Level:                   0</span><br><span class="line">Device State:                        Active (0)</span><br><span class="line">Current Temperature:                    29 Celsius</span><br><span class="line">Power Cycle Min&#x2F;Max Temperature:     28&#x2F;30 Celsius</span><br><span class="line">Lifetime    Min&#x2F;Max Temperature:     24&#x2F;32 Celsius</span><br><span class="line">Under&#x2F;Over Temperature Limit Count:   0&#x2F;0</span><br><span class="line"></span><br><span class="line">SCT Temperature History Version:     2</span><br><span class="line">Temperature Sampling Period:         1 minute</span><br><span class="line">Temperature Logging Interval:        1 minute</span><br><span class="line">Min&#x2F;Max recommended Temperature:      0&#x2F;60 Celsius</span><br><span class="line">Min&#x2F;Max Temperature Limit:           -40&#x2F;70 Celsius</span><br><span class="line">Temperature History Size (Index):    128 (92)</span><br><span class="line">Index    Estimated Time   Temperature Celsius</span><br><span class="line">  93    2019-12-10 09:11    29  **********</span><br><span class="line"> ...    ..(126 skipped).    ..  **********</span><br><span class="line">  92    2019-12-10 11:18    29  **********</span><br><span class="line"></span><br><span class="line">SCT Error Recovery Control:</span><br><span class="line">           Read:     80 (8.0 seconds)</span><br><span class="line">          Write:     80 (8.0 seconds)</span><br><span class="line"></span><br><span class="line">Device Statistics (GP&#x2F;SMART Log 0x04) not supported</span><br><span class="line"></span><br><span class="line">Pending Defects log (GP Log 0x0c)</span><br><span class="line">No Defects Logged</span><br><span class="line"></span><br><span class="line">SATA Phy Event Counters (GP Log 0x11)</span><br><span class="line">ID      Size     Value  Description</span><br><span class="line">0x0001  2            0  Command failed due to ICRC error</span><br><span class="line">0x0002  2            0  R_ERR response for data FIS</span><br><span class="line">0x0003  2            0  R_ERR response for device-to-host data FIS</span><br><span class="line">0x0004  2            0  R_ERR response for host-to-device data FIS</span><br><span class="line">0x0005  2            0  R_ERR response for non-data FIS</span><br><span class="line">0x0006  2            0  R_ERR response for device-to-host non-data FIS</span><br><span class="line">0x0007  2            0  R_ERR response for host-to-device non-data FIS</span><br><span class="line">0x0008  2            0  Device-to-host non-data FIS retries</span><br><span class="line">0x0009  2            0  Transition from drive PhyRdy to drive PhyNRdy</span><br><span class="line">0x000a  2            1  Device-to-host register FISes sent due to a COMRESET</span><br><span class="line">0x000b  2            0  CRC errors within host-to-device FIS</span><br><span class="line">0x000d  2            0  Non-CRC errors within host-to-device FIS</span><br></pre></td></tr></table></figure>

<h3 id="How-to-calculate-scsi-logging-level"><a href="#How-to-calculate-scsi-logging-level" class="headerlink" title="How to calculate scsi logging_level"></a>How to calculate scsi logging_level</h3><p><a href="https://en.opensuse.org/SDB:SCSI_debugging_hints" target="_blank" rel="noopener">A useful setting of the logging level without being buried in logging details is ERROR=3, SCAN=3, MLQUEUE=2, MLCOMPLETE=2, which evaluates to echo 9411 &gt; /proc/sys/dev/scsi/logging_level</a></p>
<p>means 111(7) is the maximum value</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scsi_logging_level -g</span><br><span class="line">Current scsi logging level:</span><br><span class="line">&#x2F;proc&#x2F;sys&#x2F;dev&#x2F;scsi&#x2F;logging_level &#x3D; 9411 (10,010,011,000,011)</span><br><span class="line">SCSI_LOG_ERROR&#x3D;3</span><br><span class="line">SCSI_LOG_TIMEOUT&#x3D;0</span><br><span class="line">SCSI_LOG_SCAN&#x3D;3</span><br><span class="line">SCSI_LOG_MLQUEUE&#x3D;2</span><br><span class="line">SCSI_LOG_MLCOMPLETE&#x3D;2</span><br><span class="line">SCSI_LOG_LLQUEUE&#x3D;0</span><br><span class="line">SCSI_LOG_LLCOMPLETE&#x3D;0</span><br><span class="line">SCSI_LOG_HLQUEUE&#x3D;0</span><br><span class="line">SCSI_LOG_HLCOMPLETE&#x3D;0</span><br><span class="line">SCSI_LOG_IOCTL&#x3D;0</span><br><span class="line"></span><br><span class="line">$ scsi_logging_level -s --error 3 --timeout 3 --scan&#x3D;3 --mlqueue&#x3D;2 --mlcomplete&#x3D;3 --all 0</span><br><span class="line">New scsi logging level:</span><br><span class="line">&#x2F;proc&#x2F;sys&#x2F;dev&#x2F;scsi&#x2F;logging_level &#x3D; 13531(011,010,011,011,011)</span><br><span class="line">SCSI_LOG_ERROR&#x3D;3</span><br><span class="line">SCSI_LOG_TIMEOUT&#x3D;3</span><br><span class="line">SCSI_LOG_SCAN&#x3D;3</span><br><span class="line">SCSI_LOG_MLQUEUE&#x3D;2</span><br><span class="line">SCSI_LOG_MLCOMPLETE&#x3D;3</span><br><span class="line">SCSI_LOG_LLQUEUE&#x3D;0</span><br><span class="line">SCSI_LOG_LLCOMPLETE&#x3D;0</span><br><span class="line">SCSI_LOG_HLQUEUE&#x3D;0</span><br><span class="line">SCSI_LOG_HLCOMPLETE&#x3D;0</span><br><span class="line">SCSI_LOG_IOCTL&#x3D;0</span><br><span class="line">https:&#x2F;&#x2F;www.ibm.com&#x2F;support&#x2F;knowledgecenter&#x2F;en&#x2F;linuxonibm&#x2F;com.ibm.linux.z.lgdd&#x2F;lgdd_r_scsilog_cmd.html</span><br><span class="line"></span><br><span class="line">scsi debug</span><br><span class="line">scsi_logging_level syntax</span><br><span class="line">Read syntax diagramSkip visual syntax diagram</span><br><span class="line">                       .- ---------------------------.</span><br><span class="line">                       V                             |</span><br><span class="line">&gt;&gt;-scsi_logging_level----+-------------------------+-+--+- -s -+-&gt;&lt;</span><br><span class="line">                         +- -a-- &lt;level&gt;-----------+    +- -g -+</span><br><span class="line">                         +- -E-- &lt;level&gt;-----------+    &#39;- -c -&#39;</span><br><span class="line">                         +- -T-- &lt;level&gt;-----------+</span><br><span class="line">                         +- -S-- &lt;level&gt;-----------+</span><br><span class="line">                         +- -M-- &lt;level&gt;-----------+</span><br><span class="line">                         +- --mlqueue-- &lt;level&gt;----+</span><br><span class="line">                         +- --mlcomplete-- &lt;level&gt;-+</span><br><span class="line">                         +- -L-- &lt;level&gt;-----------+</span><br><span class="line">                         +- --llqueue-- &lt;level&gt;----+</span><br><span class="line">                         +- --llcomplete-- &lt;level&gt;-+</span><br><span class="line">                         +- -H-- &lt;level&gt;-----------+</span><br><span class="line">                         +- --hlqueue-- &lt;level&gt;----+</span><br><span class="line">                         +- --hlcomplete-- &lt;level&gt;-+</span><br><span class="line">                         &#39;- -I-- &lt;level&gt;-----------&#39;</span><br></pre></td></tr></table></figure>

<p>Error Type</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">drivers/scsi/scsi_logging.h</span><br></pre></td></tr></table></figure>
<p>ERROR<br>Used by any command which has to be retried/recovered via the SCSI error handling mechanism</p>
<p>TIMEOUT<br>Used by drivers/scsi/sg.c</p>
<p>SCAN<br>Used during device scan, ie whenever a new device HBA is initialized</p>
<p>MLQUEUE<br>Mid-layer queue; requests are being pulled from the block-layer queue and submitted to the HBA</p>
<p>MLCOMPLETE<br>Mid-layer queue; requests are being completed by the HBA and results are being pushed back to the block-layer</p>
<p>LLQUEUE<br>Low-layer queue; Not used<br>LLCOMPLETE<br>Low-layer queue; Not used</p>
<p>HLQUEUE<br>High-layer queue; command preparation in drivers/scsi/sd.c</p>
<p>HLCOMPLETE<br>High-layer queue; command completion in drivers/scsi/sd.c</p>
<p>IOCTL<br>SCSI IOCTL logging<br>The detailed error level description</p>
<p>ERROR<br>Error logging when a command is recovered via the SCSI error handling mechanism. The following levels are uses:<br>Error handler thread statistics<br>Error handler command statistics<br>Error handler command logging<br>Not used<br>Error handler command details<br>and higher: not used</p>
<p>SCAN<br>Logging during HBA / target scanning. The following levels are used:<br>Logging of unusual devices where LUN 0 has a pqual of 3<br>Logging of devices with pqual 3<br>Logging of SCSI commands sent during scanning<br>and higher: not used.</p>
<p>MLQUEUE<br>The MLQUEUE area is used when a command is being pulled from the block-layer queue and send to the HBA. It has the following levels:<br>nothing (match completion)<br>log opcode + command of all commands<br>same as 2 plus dump cmd address<br>same as 3 plus dump extra junk<br>and higher: not used</p>
<p>MLCOMPLETE<br>Logging of command completion from the HBA, before the completion is being called for the block-layer request. It has the following levels:<br>log disposition, result, opcode + command, and conditionally sense data for failures or non SUCCESS dispositions.<br>same as 1 but for all command completions.<br>same as 2 plus dump cmd address<br>same as 3 plus dump extra junk<br>and higher: not used</p>
<h3 id="smp-utils"><a href="#smp-utils" class="headerlink" title="smp_utils"></a>smp_utils</h3><p>not make sure<br>$ smp_rep_phy_sata –phy=2 /dev/sdb<br>$ smp_phy_control –phy=2 –op=tspss  /dev/sdb</p>
]]></content>
      <categories>
        <category>scsi</category>
      </categories>
      <tags>
        <tag>sata</tag>
        <tag>sas</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux memory</title>
    <url>/2018/07/02/mem/</url>
    <content><![CDATA[<h3 id="Make-sure-the-ECC-work-in-Linux"><a href="#Make-sure-the-ECC-work-in-Linux" class="headerlink" title="Make sure the ECC work in Linux"></a>Make sure the ECC work in Linux</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dmidecode -t memory  | grep -Ei <span class="string">'error correction type'</span></span><br><span class="line">	Error Correction Type: Single-bit ECC</span><br><span class="line">$ dmidecode -t memory  | grep -Ei <span class="string">'error correction type'</span></span><br><span class="line">	Error Correction Type: Multi-bit ECC</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="Monitor-ECC-error"><a href="#Monitor-ECC-error" class="headerlink" title="Monitor ECC error"></a>Monitor ECC error</h3><h4 id="From-OS"><a href="#From-OS" class="headerlink" title="From OS"></a>From OS</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Intel x86 scalable</span></span><br><span class="line">skx_edac</span><br><span class="line"></span><br><span class="line"><span class="comment">#Intel x86 E3</span></span><br><span class="line">ie31200-edac</span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers are loaded. 1 MC detected:</span><br><span class="line">  mc0:IE31200</span><br><span class="line"></span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers loaded. No memory controllers found</span><br><span class="line">$ <span class="built_in">echo</span> $?</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ edac-util -v</span><br><span class="line">mc0: 0 Uncorrected Errors with no DIMM info</span><br><span class="line">mc0: 0 Corrected Errors with no DIMM info</span><br><span class="line">mc0: csrow0: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow0: mc<span class="comment">#0csrow#0channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow0: mc<span class="comment">#0csrow#0channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow1: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow1: mc<span class="comment">#0csrow#1channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow1: mc<span class="comment">#0csrow#1channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow2: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow2: mc<span class="comment">#0csrow#2channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow2: mc<span class="comment">#0csrow#2channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow3: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow3: mc<span class="comment">#0csrow#3channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow3: mc<span class="comment">#0csrow#3channel#1: 0 Corrected Errors</span></span><br><span class="line">edac-util: No errors to report.</span><br><span class="line"></span><br><span class="line"><span class="comment">#CentOS 7.7 not support Xeon E2000, the kernel merge the patch at 2019-06, maybe wait CentOS 7.8</span></span><br><span class="line"><span class="comment">#Intel x86 E2000</span></span><br><span class="line">ie31200-edac</span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers loaded. No memory controllers found</span><br></pre></td></tr></table></figure>
<h4 id="From-IPMI"><a href="#From-IPMI" class="headerlink" title="From IPMI"></a>From IPMI</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ipmitool sel elist</span><br></pre></td></tr></table></figure>

<h3 id="NIC-page-allocation-failure"><a href="#NIC-page-allocation-failure" class="headerlink" title="NIC page allocation failure"></a><a href="https://access.redhat.com/solutions/641323" target="_blank" rel="noopener">NIC page allocation failure</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Mon Jul  2 10:38:25 2018] swapper/16: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Mon Jul  2 10:38:25 2018] CPU: 16 PID: 0 Comm: swapper/16 Tainted: P           OE  ------------   3.10.0-693.5.2.el7_lustre.x86_64 <span class="comment">#1</span></span><br><span class="line">[Mon Jul  2 10:38:25 2018] Hardware name: Dell Inc. PowerEdge R730/072T6D, BIOS 2.4.3 01/17/2017</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  0000000000104020 7e870b98ec876be5 ffff88103e8039d8 ffffffff816a3e2d</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  ffff88103e803a68 ffffffff81188820 0000000000000246 ffff88103e803a28</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  fffffffffffffffc 0010402000000000 ffff88107ffdb018 7e870b98ec876be5</span><br><span class="line">[Mon Jul  2 10:38:25 2018] Call Trace:</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  &lt;IRQ&gt;  [&lt;ffffffff816a3e2d&gt;] dump_stack+0x19/0x1b</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81188820&gt;] warn_alloc_failed+0x110/0x180</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff8169fe2a&gt;] __alloc_pages_slowpath+0x6b6/0x724</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff8118cdb5&gt;] __alloc_pages_nodemask+0x405/0x420</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff811d1078&gt;] alloc_pages_current+0x98/0x110</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff8118761e&gt;] __get_free_pages+0xe/0x40</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff811dca2e&gt;] kmalloc_order_trace+0x2e/0xa0</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff811e05c1&gt;] __kmalloc+0x211/0x230</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffffc031abba&gt;] bnx2x_frag_alloc.isra.61+0x2a/0x40 [bnx2x]</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffffc031ba97&gt;] bnx2x_rx_int+0x227/0x17c0 [bnx2x]</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81573004&gt;] ? consume_skb+0x34/0x80</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81585dad&gt;] ? __dev_kfree_skb_any+0x3d/0x50</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffffc031eecd&gt;] bnx2x_poll+0x1dd/0x260 [bnx2x]</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff8158799d&gt;] net_rx_action+0x16d/0x380</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81090b4f&gt;] __do_softirq+0xef/0x280</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff816b6b1c&gt;] call_softirq+0x1c/0x30</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81090ed5&gt;] irq_exit+0x105/0x110</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff816b76b6&gt;] do_IRQ+0x56/0xe0</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff816ac2ad&gt;] common_interrupt+0x6d/0x6d</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  &lt;EOI&gt;  [&lt;ffffffff816ab546&gt;] ? native_safe_halt+0x6/0x10</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff816ab3de&gt;] default_idle+0x1e/0xc0</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81035006&gt;] arch_cpu_idle+0x26/0x30</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff810e7bda&gt;] cpu_startup_entry+0x14a/0x1c0</span><br><span class="line">[Mon Jul  2 10:38:25 2018]  [&lt;ffffffff81051af6&gt;] start_secondary+0x1b6/0x230</span><br><span class="line"></span><br><span class="line"><span class="comment"># in my case </span></span><br><span class="line">[Thu Jul 12 04:23:16 2018]  00000000ad5e4ed1</span><br><span class="line">[Thu Jul 12 04:23:16 2018] Call Trace:</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  ffff88103e643a20 ffffffff81188820</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;IRQ&gt;</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  00000000000000c3</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816a3e2d&gt;] dump_stack+0x19/0x1b</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  0000000000000282</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81188820&gt;] warn_alloc_failed+0x110/0x180</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  fffffffffffffffc 0010402000000000</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  ffff8816385ea000 66b8573252d2c8c3</span><br><span class="line">[Thu Jul 12 04:23:16 2018] Call Trace:</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;IRQ&gt;  [&lt;ffffffff816a3e2d&gt;] dump_stack+0x19/0x1b</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8169fe2a&gt;] __alloc_pages_slowpath+0x6b6/0x724</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81188820&gt;] warn_alloc_failed+0x110/0x180</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811d1078&gt;] alloc_pages_current+0x98/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f8bba&gt;] bnx2x_frag_alloc.isra.61+0x2a/0x40 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8169fe2a&gt;] __alloc_pages_slowpath+0x6b6/0x724</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8118761e&gt;] __get_free_pages+0xe/0x40</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f9124&gt;] bnx2x_alloc_rx_data.isra.69+0x54/0x1c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8118cdb5&gt;] __alloc_pages_nodemask+0x405/0x420</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811dca2e&gt;] kmalloc_order_trace+0x2e/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fa0b6&gt;] bnx2x_rx_int+0x846/0x17c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811d1078&gt;] alloc_pages_current+0x98/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8118761e&gt;] __get_free_pages+0xe/0x40</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f8bba&gt;] bnx2x_frag_alloc.isra.61+0x2a/0x40 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811dca2e&gt;] kmalloc_order_trace+0x2e/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f9124&gt;] bnx2x_alloc_rx_data.isra.69+0x54/0x1c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] ? __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f8bba&gt;] bnx2x_frag_alloc.isra.61+0x2a/0x40 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8118cdb5&gt;] __alloc_pages_nodemask+0x405/0x420</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810d1c02&gt;] ? load_balance+0x192/0x9a0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fa0b6&gt;] bnx2x_rx_int+0x846/0x17c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f9124&gt;] bnx2x_alloc_rx_data.isra.69+0x54/0x1c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fcecd&gt;] bnx2x_poll+0x1dd/0x260 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810d1bd2&gt;] ? load_balance+0x162/0x9a0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fa0b6&gt;] bnx2x_rx_int+0x846/0x17c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fcecd&gt;] bnx2x_poll+0x1dd/0x260 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81573004&gt;] ? consume_skb+0x34/0x80</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8158799d&gt;] net_rx_action+0x16d/0x380</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81585dad&gt;] ? __dev_kfree_skb_any+0x3d/0x50</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090b4f&gt;] __do_softirq+0xef/0x280</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f95c2&gt;] ? bnx2x_free_tx_pkt+0x1f2/0x2d0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f0062&gt;] ? bnx2x_test_link+0x42/0x270 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fcecd&gt;] bnx2x_poll+0x1dd/0x260 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8158799d&gt;] net_rx_action+0x16d/0x380</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8158799d&gt;] net_rx_action+0x16d/0x380</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811d1078&gt;] alloc_pages_current+0x98/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b6b1c&gt;] call_softirq+0x1c/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090b4f&gt;] __do_softirq+0xef/0x280</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8118761e&gt;] __get_free_pages+0xe/0x40</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b6b1c&gt;] call_softirq+0x1c/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811dca2e&gt;] kmalloc_order_trace+0x2e/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090ed5&gt;] irq_exit+0x105/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] ? __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b76b6&gt;] do_IRQ+0x56/0xe0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811e05c1&gt;] __kmalloc+0x211/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816ac2ad&gt;] common_interrupt+0x6d/0x6d</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;EOI&gt;</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f8bba&gt;] bnx2x_frag_alloc.isra.61+0x2a/0x40 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81527d42&gt;] ? cpuidle_enter_state+0x52/0xc0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81527e88&gt;] cpuidle_idle_call+0xd8/0x210</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81034fee&gt;] arch_cpu_idle+0xe/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810e7bda&gt;] cpu_startup_entry+0x14a/0x1c0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090b4f&gt;] __do_softirq+0xef/0x280</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02f9124&gt;] bnx2x_alloc_rx_data.isra.69+0x54/0x1c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090ed5&gt;] irq_exit+0x105/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b6b1c&gt;] call_softirq+0x1c/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fa0b6&gt;] bnx2x_rx_int+0x846/0x17c0 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b76b6&gt;] do_IRQ+0x56/0xe0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810d1c02&gt;] ? load_balance+0x192/0x9a0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816ac2ad&gt;] common_interrupt+0x6d/0x6d</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090ed5&gt;] irq_exit+0x105/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;EOI&gt;</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81527d42&gt;] ? cpuidle_enter_state+0x52/0xc0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b76b6&gt;] do_IRQ+0x56/0xe0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81527e88&gt;] cpuidle_idle_call+0xd8/0x210</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816ac2ad&gt;] common_interrupt+0x6d/0x6d</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;EOI&gt;  [&lt;ffffffff81527d42&gt;] ? cpuidle_enter_state+0x52/0xc0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81527e88&gt;] cpuidle_idle_call+0xd8/0x210</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81034fee&gt;] arch_cpu_idle+0xe/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810e7bda&gt;] cpu_startup_entry+0x14a/0x1c0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81051af6&gt;] start_secondary+0x1b6/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81034fee&gt;] arch_cpu_idle+0xe/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81051af6&gt;] start_secondary+0x1b6/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffffc02fcecd&gt;] bnx2x_poll+0x1dd/0x260 [bnx2x]</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8158799d&gt;] net_rx_action+0x16d/0x380</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090b4f&gt;] __do_softirq+0xef/0x280</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810e7bda&gt;] cpu_startup_entry+0x14a/0x1c0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81051af6&gt;] start_secondary+0x1b6/0x230</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b6b1c&gt;] call_softirq+0x1c/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff8102d3c5&gt;] do_softirq+0x65/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81090ed5&gt;] irq_exit+0x105/0x110</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b76b6&gt;] do_IRQ+0x56/0xe0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816ac2ad&gt;] common_interrupt+0x6d/0x6d</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  &lt;EOI&gt;  [&lt;ffffffff811a3178&gt;] ? fragmentation_index+0x38/0xa0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811a877b&gt;] compaction_suitable+0x5b/0xb0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81199102&gt;] balance_pgdat+0x502/0x5e0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff81199353&gt;] kswapd+0x173/0x440</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810b1920&gt;] ? wake_up_atomic_t+0x30/0x30</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff811991e0&gt;] ? balance_pgdat+0x5e0/0x5e0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810b099f&gt;] kthread+0xcf/0xe0</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff816b4fd8&gt;] ret_from_fork+0x58/0x90</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Thu Jul 12 04:23:16 2018] swapper/32: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Thu Jul 12 04:23:16 2018] CPU: 32 PID: 0 Comm: swapper/32 Tainted: P           OE  ------------   3.10.0-693.5.2.el7_lustre.x86_64 <span class="comment">#1</span></span><br><span class="line">[Thu Jul 12 04:23:16 2018] Hardware name: Dell Inc. PowerEdge R730/072T6D, BIOS 2.4.3 01/17/2017</span><br><span class="line">[Thu Jul 12 04:23:16 2018]  0000000000104020 1fc54c56797ed550 ffff88103ea03990 ffffffff816a3e2d</span><br><span class="line">[Thu Jul 12 04:23:16 2018] swapper/12: page allocation failure: order:2, mode:0x104020</span><br><span class="line"></span><br><span class="line"><span class="comment"># you can see a lot of processes could not allocation memory</span></span><br><span class="line">[Fri Aug 16 06:08:22 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:08:22 2019] kswapd0: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] kswapd0: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] swapper/10: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 06:43:21 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:53:18 2019] swapper/22: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 07:58:18 2019] java: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] kswapd0: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] kswapd0: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] kswapd0: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] systemd-journal: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] swapper/16: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] swapper/16: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] swapper/4: page allocation failure: order:2, mode:0x104020</span><br><span class="line">[Fri Aug 16 09:08:22 2019] swapper/4: page allocation failure: order:2, mode:0x104020</span><br></pre></td></tr></table></figure>
<p>zone_reclaim_mode<br>sysctl -w vm.zone_reclaim_mode=1<br>#This can be set to 1 to attempt reclamation of memory within a NUMA node before reclaiming from other NUMA nodes.</p>
<p><code>min_free_kbytes</code> (it ‘s worked, the page allocation failure was missing)<br>sysctl -w vm.min_free_kbytes = 540672<br>#Increase the virtual memory kernel tunable vm.min_free_kbytes, which instructs the virtual memory subsystem to try keep a certain amount of memory always free for allocation.</p>
<p>The network driver is receiving traffic from the network adapter into kernel memory, however when the driver tried to allocate memory, the allocation failed.</p>
<p>Kernel memory is required to be a contiguous block of a certain size. The kernel memory may be all consumed or fragmented, hence why a contiguous block could not be allocated.</p>
<p>Tuning of the system’s use of kernel memory is required to ensure the kernel can always service kernel memory allocations when running in interrupt context.</p>
<p>kswapd high cpu usage in some of not enough memory case<br>reslove the issue temporary</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sar -rB 2 <span class="comment"># check system vm status</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/compact_memory</span><br></pre></td></tr></table></figure>

<p>reduce memory compaction ratio<br>echo 1000 &gt;  /proc/sys/vm/extfrag_threshold # 0~1000 ,1000 means the minimum trigger memory compaction operation, it means kernel memory is required to be a contiguous block of a certain size. The kernel memory may be all consumed or fragmented, hence why a contiguous block could not be allocated.</p>
<p>Some times there are a lot of free memory in your system. But I ‘m not sure the sk_buffer to malloc memroy range from the linux kernel when the NIC driver tirgger the issue.</p>
<h4 id="dirty-page"><a href="#dirty-page" class="headerlink" title="dirty page"></a>dirty page</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sysctl -a | grep dirty</span><br><span class="line">vm.dirty_background_bytes = 0</span><br><span class="line">vm.dirty_background_ratio = 10</span><br><span class="line">vm.dirty_bytes = 0</span><br><span class="line">vm.dirty_expire_centisecs = 3000</span><br><span class="line">vm.dirty_ratio = 20</span><br><span class="line">vm.dirty_writeback_centisecs = 500</span><br></pre></td></tr></table></figure>


<h3 id="ZRAM"><a href="#ZRAM" class="headerlink" title="ZRAM"></a><a href="http://liujunming.top/2016/07/04/Linux%E5%86%85%E6%A0%B8%E4%B8%ADzram%E6%A8%A1%E5%9D%97%E7%9A%84%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">ZRAM</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">description <span class="string">"Initializes zram swaping"</span></span><br><span class="line">start on runlevel [2345]</span><br><span class="line">stop on runlevel [!2345]</span><br><span class="line">pre-start script</span><br><span class="line"><span class="comment"># load dependency modules</span></span><br><span class="line">modprobe zram num_devices=2</span><br><span class="line"><span class="comment"># initialize the devices</span></span><br><span class="line"><span class="built_in">echo</span> 1073741824 &gt; /sys/block/zram0/disksize</span><br><span class="line"><span class="built_in">echo</span> 1073741824 &gt; /sys/block/zram1/disksize</span><br><span class="line"><span class="comment"># Creating swap filesystems</span></span><br><span class="line">mkswap /dev/zram0</span><br><span class="line">mkswap /dev/zram1</span><br><span class="line"><span class="comment"># Switch the swaps on</span></span><br><span class="line">swapon -p 5 /dev/zram0</span><br><span class="line">swapon -p 5 /dev/zram1</span><br><span class="line">end script</span><br><span class="line">post-stop script</span><br><span class="line"><span class="comment"># Switching off swap</span></span><br><span class="line">swapoff /dev/zram0</span><br><span class="line">swapoff /dev/zram1</span><br><span class="line">rmmod zram</span><br><span class="line">end script</span><br><span class="line"></span><br><span class="line"><span class="comment">##test zram</span></span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"><span class="comment">#include &lt;stdlib.h&gt;  </span></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	//<span class="built_in">printf</span>(<span class="string">"%d\n"</span>, sizeof(int));</span><br><span class="line">	int  *mem;</span><br><span class="line">	int i, size;</span><br><span class="line">	size = 0x70000000;</span><br><span class="line">	mem = (int*)malloc(size*sizeof(int));</span><br><span class="line">	<span class="keyword">for</span>(i = 0; i &lt; size; i++)</span><br><span class="line">		mem[i] = (i%1024);</span><br><span class="line">	getchar();</span><br><span class="line">	free(mem);</span><br><span class="line">	<span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Some-parameter-about-numa"><a href="#Some-parameter-about-numa" class="headerlink" title="Some parameter about numa"></a>Some parameter about numa</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">numactl --interleave=all <span class="comment">#Two/Four socket CPU will share all memory, but you can' t got the best performance except you need more memory</span></span><br><span class="line">vm.zone_reclaim_mode = 0  <span class="comment">#</span></span><br><span class="line"><span class="built_in">echo</span> -15 &gt; /proc/&lt;pid&gt;/oom_adj <span class="comment">#reduce kill ratio	</span></span><br><span class="line">huge page will not be swaped</span><br></pre></td></tr></table></figure>

<h3 id="Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage"><a href="#Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage" class="headerlink" title="Soft lockup detected on a large NUMA system under a heavy memory usage"></a><a href="https://access.redhat.com/solutions/1560893" target="_blank" rel="noopener">Soft lockup detected on a large NUMA system under a heavy memory usage</a></h3><p>Systems with numa factor lower than or equal to 30 may hang under the high load.<br>Soft lockup detected under a heavy memory pressure on a large NUMA system.</p>
<p>For RHEL 7, users must be aware of the following steps that can avoid the soft lockup from occurring. Disable Transparent Huge Page (THP) to avoid such busy memory-compaction, and add “numa_balancing=disable” to the kernel parameter followed by reboot, OR set /proc/sys/vm/zone_reclaim_mode to 1.<br>For RHEL 6,<br>disable Transparent Huge Page (THP) to avoid such busy memory-compaction OR set /proc/sys/vm/zone_reclaim_mode to 1.</p>
<p>The default value(zero) of /proc/sys/vm/zone_reclaim_mode results in the CPUs running on the memory exhausted nodes to skip over to the next node with available memory to attempt the memory allocation.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kernel: BUG: soft lockup - CPU<span class="comment">#102 stuck for 22s! [forkoff:235364]</span></span><br><span class="line">kernel: Modules linked <span class="keyword">in</span>: fuse btrfs zlib_deflate raid6_pq xor msdos ext4</span><br><span class="line">mbcache jbd2 binfmt_misc xt_CHECKSUM iptable_mangle ipt_MASQUERADE</span><br><span class="line">nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4</span><br><span class="line">nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT iptable_filter ip_tables</span><br><span class="line">tun bridge stp llc dm_mirror dm_region_hash dm_log dm_mod iTCO_wdt</span><br><span class="line">iTCO_vendor_support vfat fat intel_powerclamp coretemp intel_rapl kvm_intel</span><br><span class="line">kvm crct10dif_pclmul crc32_pclmul crc32c_intel ghash_clmulni_intel aesni_intel</span><br><span class="line">lrw gf128mul glue_helper ablk_helper cryptd pcspkr sb_edac edac_core lpc_ich</span><br><span class="line">i2c_i801 mfd_core shpchp ipmi_si ipmi_msghandler tpm_infineon nls_utf8 isofs</span><br><span class="line">loop uinput xfs libcrc32c sd_mod crc_t10dif crct10dif_common mgag200</span><br><span class="line">syscopyarea sysfillrect sysimgblt drm_kms_helper igb qla2xxx e1000e</span><br><span class="line">kernel: ttm dca ptp scsi_transport_fc drm i2c_algo_bit pps_core scsi_tgt</span><br><span class="line">megaraid_sas i2c_core</span><br><span class="line">kernel: CPU: 102 PID: 235364 Comm: forkoff Not tainted 3.10.0-229.el7.x86_64</span><br><span class="line"><span class="comment">#1                                                                                                                                                                                                                                                                             </span></span><br><span class="line">kernel:</span><br><span class="line">kernel: task: ffff911d25eea220 ti: ffff927835154000 task.ti: ffff927835154000</span><br><span class="line">kernel: RIP: 0010:[&lt;ffffffff811798ef&gt;]  [&lt;ffffffff811798ef&gt;]</span><br><span class="line">isolate_freepages_block+0xaf/0x380</span><br><span class="line">kernel: RSP: 0000:ffff927835157860  EFLAGS: 00000286</span><br><span class="line">kernel: RAX: 00000000ffffffff RBX: 00000014e4b60000 RCX: ffff927835157aa8</span><br><span class="line">kernel: RDX: 0000000053345c00 RSI: 0000000053345a00 RDI: ffff927835157a50</span><br><span class="line">kernel: RBP: ffff9278351578f8 R08: 0000000000000000 R09: ffff8e007ffda000</span><br><span class="line">kernel: R10: 00000014cd168000 R11: 0000000060080000 R12: 0000000000000301</span><br><span class="line">kernel: R13: ffff927835157850 R14: 0000000000000000 R15: 00007f4501f64000</span><br><span class="line">kernel: FS:  00007f4f4d540740(0000) GS:ffff90fa7f9a0000(0000)</span><br><span class="line">knlGS:0000000000000000</span><br><span class="line">kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033</span><br><span class="line">kernel: CR2: 00007f4690400000 CR3: 000009baa7060000 CR4: 00000000001407e0</span><br><span class="line">kernel: DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000</span><br><span class="line">kernel: DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400</span><br><span class="line">kernel: Stack:</span><br><span class="line">kernel: ffffea2a02311040 0000000060080000 0000000000000094 ffff927835157948</span><br><span class="line">kernel: 0000000053345a00 ffff927835157a50 000000004808ec38 00ff8e0000000000</span><br><span class="line">kernel: ffff927835157a90 ffff927835157aa8 ffff927835157a50 ffff8e007ffad000</span><br><span class="line">kernel: Call Trace:</span><br><span class="line">kernel: [&lt;ffffffff81179d8f&gt;] compaction_alloc+0x1cf/0x240</span><br><span class="line">kernel: [&lt;ffffffff811b15ce&gt;] migrate_pages+0xce/0x610</span><br><span class="line">kernel: [&lt;ffffffff81179bc0&gt;] ? isolate_freepages_block+0x380/0x380</span><br><span class="line">kernel: [&lt;ffffffff8117abb9&gt;] compact_zone+0x299/0x400</span><br><span class="line">kernel: [&lt;ffffffff8117adbc&gt;] compact_zone_order+0x9c/0xf0</span><br><span class="line">kernel: [&lt;ffffffff8117b171&gt;] try_to_compact_pages+0x121/0x1a0</span><br><span class="line">kernel: [&lt;ffffffff815ff336&gt;] __alloc_pages_direct_compact+0xac/0x196</span><br><span class="line">kernel: [&lt;ffffffff81160758&gt;] __alloc_pages_nodemask+0x788/0xb90</span><br><span class="line">kernel: [&lt;ffffffff810b11c0&gt;] ? task_numa_fault+0x8d0/0xbb0</span><br><span class="line">kernel: [&lt;ffffffff811a24aa&gt;] alloc_pages_vma+0x9a/0x140</span><br><span class="line">kernel: [&lt;ffffffff811b674b&gt;] do_huge_pmd_anonymous_page+0x10b/0x410</span><br><span class="line">kernel: [&lt;ffffffff81182334&gt;] handle_mm_fault+0x184/0xd60</span><br><span class="line">kernel: [&lt;ffffffff8160f1e6&gt;] __do_page_fault+0x156/0x520</span><br><span class="line">kernel: [&lt;ffffffff8118a945&gt;] ? change_protection+0x65/0xa0</span><br><span class="line">kernel: [&lt;ffffffff811a0dbb&gt;] ? change_prot_numa+0x1b/0x40</span><br><span class="line">kernel: [&lt;ffffffff810adb86&gt;] ? task_numa_work+0x266/0x300</span><br><span class="line">kernel: [&lt;ffffffff8160f5ca&gt;] do_page_fault+0x1a/0x70</span><br><span class="line">kernel: [&lt;ffffffff81013b0c&gt;] ? do_notify_resume+0x9c/0xb0</span><br><span class="line">kernel: [&lt;ffffffff8160b808&gt;] page_fault+0x28/0x30</span><br><span class="line">kernel: Code: 89 ee 48 89 4d b0 41 89 c5 eb 1d 90 49 83 c7 01 48 83 c3 40 4d</span><br><span class="line">39 <span class="built_in">fc</span> 0f 86 07 01 00 00 41 83 c5 01 4d 85 f6 4c 0f 44 f3 8b 43 18 &lt;83&gt; f8 80</span><br><span class="line">75 dc 48 8b 45 b8 0f b6 55 c0 48 8d 75 c8 4c 8b 45 b0</span><br></pre></td></tr></table></figure>

<h3 id="Transparent-Huge-Page"><a href="#Transparent-Huge-Page" class="headerlink" title="Transparent Huge Page"></a><a href="https://access.redhat.com/solutions/46111" target="_blank" rel="noopener">Transparent Huge Page</a></h3><p>The transparent Huge Page implementation in the Linux kernel includes functionality that provides compaction. Compaction operations are system level processes that are resource intensive</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#### Enable or disable</span></span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">[always] madvise never</span><br><span class="line">$ cat /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">[always] madvise never</span><br><span class="line"></span><br><span class="line">$ grep AnonHugePages /proc/meminfo</span><br><span class="line">AnonHugePages:  19523584 kB</span><br><span class="line"></span><br><span class="line">$ cat /proc/meminfo|grep Huge</span><br><span class="line">AnonHugePages:  1681086464 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line"></span><br><span class="line"><span class="comment">## means 1681086464/2048 = 820843x 2MB huge pages</span></span><br><span class="line"></span><br><span class="line">$ grep -Ei <span class="string">'trans|thp'</span> /proc/vmstat</span><br><span class="line">nr_anon_transparent_hugepages 9533</span><br><span class="line">thp_fault_alloc 24017</span><br><span class="line">thp_fault_fallback 16231</span><br><span class="line">thp_collapse_alloc 1687</span><br><span class="line">thp_collapse_alloc_failed 39449</span><br><span class="line">thp_split 2926</span><br><span class="line">thp_zero_page_alloc 1</span><br><span class="line">thp_zero_page_alloc_failed 0</span><br><span class="line"></span><br><span class="line"><span class="comment">### check the process</span></span><br><span class="line">$ grep -e AnonHugePages  /proc/*/smaps | awk -F <span class="string">'[/ ]+'</span> <span class="string">'$(NF-1)&gt;4 &#123;system("ps -fp  "$3)&#125;'</span></span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">nobody    5023  5016  3 Jun07 ?        22:28:27 /sbin/lustre_exporter --collector.ost=disabled --collector.mdt=core</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">polkitd    743     1  0 Jun07 ?        00:00:19 /usr/lib/polkit-1/polkitd --no-debug</span><br></pre></td></tr></table></figure>


<h3 id="Single-process-memory"><a href="#Single-process-memory" class="headerlink" title="Single process memory"></a>Single process memory</h3><pre><code>/proc/[pid]/statm
       Provides information about memory usage, measured in pages.
       The columns are:

           size       (1) total program size
                      (same as VmSize in /proc/[pid]/status)
           resident   (2) resident set size
                      (same as VmRSS in /proc/[pid]/status)
           shared     (3) number of resident shared pages (i.e., backed by a file)
                      (same as RssFile+RssShmem in /proc/[pid]/status)
           text       (4) text (code)
           lib        (5) library (unused since Linux 2.6; always 0)
           data       (6) data + stack
           dt         (7) dirty pages (unused since Linux 2.6; always 0)</code></pre><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/3760/statm</span><br><span class="line">400865 96456 37653 27355 0 157019 0</span><br></pre></td></tr></table></figure>

<p>Second field means res (resident)<br>pmap $(pgrep bash)</p>
<p>There are some of share library in each resident</p>
<p>If you want get share library memory consumption.<br>/proc/[pid]/smaps (since Linux 2.6.14)<br>              This file shows memory consumption for each of the process’s<br>              mappings.  (The pmap(1) command displays similar information,<br>              in a form that may be easier for parsing.)</p>
<h4 id="Slab"><a href="#Slab" class="headerlink" title="Slab"></a>Slab</h4><p>In-kernel data structures cache.</p>
<p>Cache pool for often userd dupulication objects<br>you could found these objects from slabtop</p>
<p>Get all slabsize<br>awk ‘BEGIN{sum=0;}{sum=sum+$3*$4;}END{print sum/1024/1024}’ /proc/slabinfo MB</p>
<h4 id="Page-table"><a href="#Page-table" class="headerlink" title="Page table"></a>Page table</h4><p>awk ‘$0~/PageTables/ {print $2}’ /proc/meminfo KB</p>
<h4 id="Struct-page"><a href="#Struct-page" class="headerlink" title="Struct page"></a>Struct page</h4><p>page frame minimum unit. every page frame has a struct page to point<br><a href="https://stackoverflow.com/questions/34836806/how-to-get-physical-address-from-struct-page-in-linux-kernel" target="_blank" rel="noopener">struct page could mapping page frame to physical address</a><br>all page frame in the LUR list.<br>There are 2.3%(96/4096) usage in linux 2.6.32</p>
<h3 id="Use-hugetlbfs"><a href="#Use-hugetlbfs" class="headerlink" title="Use hugetlbfs"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/tuning_and_optimizing_red_hat_enterprise_linux_for_oracle_9i_and_10g_databases/sect-oracle_9i_and_10g_tuning_guide-large_memory_optimization_big_pages_and_huge_pages-configuring_huge_pages_in_red_hat_enterprise_linux_4_or_5" target="_blank" rel="noopener">Use hugetlbfs</a></h3><h4 id="set-hugepage"><a href="#set-hugepage" class="headerlink" title="set hugepage"></a><a href="https://paolozaino.wordpress.com/2016/10/02/how-to-force-any-linux-application-to-use-hugepages-without-modifying-the-source-code/" target="_blank" rel="noopener">set hugepage</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum -y install libhugetlbfs-utils libhugetlbfs</span><br><span class="line"><span class="comment"># To set the 2MB pool minimum to 512 pages:</span></span><br><span class="line">$ hugeadm --pool-pages-min 2MB:512</span><br><span class="line">$ hugeadm --pool-pages-max 2MB:2048</span><br><span class="line">$ hugeadm --pool-list</span><br><span class="line"></span><br><span class="line">$ mkdir -p /mnt/hugetlbfs-64K</span><br><span class="line">$ mount -t hugetlbfs none -opagesize=64k /mnt/hugetlbfs-64K</span><br><span class="line">or </span><br><span class="line">$ mount -t hugetlbfs none /mnt/hugetlbfs -o uid=postfix -o gid=postfix</span><br><span class="line">$ hugeadm --<span class="built_in">set</span>-recommended-shmmax</span><br><span class="line">$ hugeadm --explain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ LD_PRELOAD=libhugetlbfs.so HUGETLB_MORECORE=yes ./run_your_cmd</span><br><span class="line"><span class="comment">## looks like it 's ok</span></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">31723 root      20   0  642.7g  94996   1348 S  1734  0.0 101:41.53 ./ft.E.x</span><br></pre></td></tr></table></figure>

<h4 id="Defragment-kernel-memory"><a href="#Defragment-kernel-memory" class="headerlink" title="Defragment kernel memory"></a>Defragment kernel memory</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mount -t debugfs none  /sys/kernel/debug</span><br><span class="line">$ cat /proc/buddyinfo</span><br><span class="line">                          2^0    2^1    2^2    2^3    2^4    2^5    2^6    2^7    2^8    2^9    2^10 (1024K)</span><br><span class="line">Node 0, zone      DMA      1      0      1      0      2      1      1      0      1      1      3</span><br><span class="line">Node 0, zone    DMA32      3      3      2      2      3      2      4      2      1      0    458</span><br><span class="line">Node 0, zone   Normal 579745  20289    510     94      0      0      0      0      0      0      0</span><br><span class="line">Node 1, zone   Normal 2230945 587911  23339      0      0      0      0      0      0      0      0</span><br><span class="line">Node 2, zone   Normal 2332274 236445  28213   9936    922     23      9      0      0      0      0</span><br><span class="line">Node 3, zone   Normal 654981   7909    163     47     11      1      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> m &gt; /proc/sysrq-trigger <span class="comment"># same with buddyinfo</span></span><br><span class="line"></span><br><span class="line">$ cat /sys/kernel/debug/extfrag/extfrag_index</span><br><span class="line">Node 0, zone      DMA -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span><br><span class="line">Node 0, zone    DMA32 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span><br><span class="line">Node 0, zone   Normal -1.000 0.500 0.750 0.875 0.938 0.969 0.985 0.993 0.996 0.998 0.999</span><br><span class="line">Node 1, zone   Normal -1.000 -1.000 0.750 0.875 0.938 0.969 0.985 0.993 0.996 0.998 0.999</span><br><span class="line">Node 2, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.990 0.995</span><br><span class="line">Node 3, zone   Normal -1.000 -1.000 -1.000 0.860 0.930 0.965 0.983 0.992 0.996 0.998 0.999</span><br><span class="line"><span class="comment">#### -1.000 is ok,</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /proc/sys/vm/compact_memory</span><br><span class="line"></span><br><span class="line">$ cat /proc/buddyinfo</span><br><span class="line">Node 0, zone      DMA      1      0      1      0      2      1      1      0      1      1      3</span><br><span class="line">Node 0, zone    DMA32      3      3      2      2      3      2      4      2      1      0    458</span><br><span class="line">Node 0, zone   Normal  13927    290      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node 1, zone   Normal 415126 488237 168310  90724  38203   7840    443      9      0      0      0</span><br><span class="line">Node 2, zone   Normal 2068303 341591  58672  25946   4986    920    239     40      1      0      0</span><br><span class="line">Node 3, zone   Normal   9030    668      5     23      7      1      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">$ cat /sys/kernel/debug/extfrag/extfrag_index</span><br><span class="line">Node 0, zone      DMA -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span><br><span class="line">Node 0, zone    DMA32 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span><br><span class="line">Node 0, zone   Normal -1.000 -1.000 -1.000 -1.000 0.935 0.968 0.984 0.992 0.996 0.998 0.999</span><br><span class="line">Node 1, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.987 0.994 0.997</span><br><span class="line">Node 2, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.998 0.999</span><br><span class="line">Node 3, zone   Normal -1.000 -1.000 -1.000 -1.000 0.933 0.967 0.984 0.992 0.996 0.998 0.999</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/vm/extfrag_threshold</span><br><span class="line">500</span><br><span class="line"><span class="comment">## if the value large than extfrag_threshold ,the kswapd will trigger memory compaction, reduce the value to 200 ? </span></span><br><span class="line"><span class="comment">## -1.000 means engough memory, if the value near the 1.000 that means more extfrag_threshold</span></span><br></pre></td></tr></table></figure>
<p>compact_memory<br>Available only when CONFIG_COMPACTION is set. When 1 is written to the file, all zones are compacted such that free memory is available in contiguous blocks where possible. This can be important for example in the allocation of huge pages although processes will also directly compact memory as required.</p>
<p>in another node, looks like there are 8 ~ 16K page in centos 7.6 ,3.10.0-957.10.1.el7.x86_64</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> cat /proc/buddyinfo</span><br><span class="line">Node 0, zone      DMA      0      1      1      0      2      1      1      0      1      1      3</span><br><span class="line">Node 0, zone    DMA32     12   6308  12784   5977   1766    440     93     11     19      0      0</span><br><span class="line">Node 0, zone   Normal    190   2376   2586 170387  88814  17747   4114   1895   1211      0      0</span><br><span class="line">Node 1, zone   Normal    186    871    437 165556  81041  12616   2113    978    607      0      0</span><br><span class="line">Node 2, zone   Normal    427   2125   1593 137923  87498  19412   6125    883    552      0      0</span><br><span class="line">Node 3, zone   Normal    280   2686   1041  74979  80569  16675   4315   1792   1145      0      0</span><br></pre></td></tr></table></figure>

<p><a href="https://paolozaino.wordpress.com/2016/10/02/how-to-force-any-linux-application-to-use-hugepages-without-modifying-the-source-code/" target="_blank" rel="noopener">At this point is time to start running your application using libhugetlbfs so that your app will use Hugepages.</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grep Hugepagesize /proc/meminfo</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">$ <span class="built_in">echo</span> 512 &gt; /proc/sys/vm/nr_hugepages</span><br><span class="line">This means <span class="keyword">if</span> a 1GB Huge Pages pool should be allocated, <span class="keyword">then</span> 512 Huge Pages need to be allocated</span><br><span class="line">it <span class="string">'s the static huge page</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">echo 1000 &gt; /proc/sys/vm/hugetlb_shm_group</span></span><br><span class="line"><span class="string">##means only members of group testuser(1000) can allocate "huge" Shared memory segment</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o uid=1000,gid=1000,mode=775,size=10g none /data/hugeshm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#kernel parameter, 4x1G, 1024x2M</span></span><br><span class="line"><span class="string">default_hugepagesz=1G hugepagesz=1G hugepages=4 hugepagesz=2M hugepages=1024</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o pagesize=1G none /dev/hugepages1G</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o pagesize=2M none /dev/hugepages2M</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">echo 4 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages</span></span><br><span class="line"><span class="string">echo 1024 &gt; /sys/devices/system/node/node3/hugepages/hugepages-2048kB/nr_hugepages</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/proc/sys/vm/nr_overcommit_hugepages</span></span><br><span class="line"><span class="string">Defines the maximum number of additional huge pages that can be created and used by the system through overcommitting memory. Writing any non-zero value into this file indicates that the system obtains that number of huge pages from the kernel'</span>s normal page pool <span class="keyword">if</span> the persistent huge page pool is exhausted. As these surplus huge pages become unused, they are <span class="keyword">then</span> freed and returned to the kernel<span class="string">'s normal page pool.</span></span><br></pre></td></tr></table></figure>

<h4 id="memlock"><a href="#memlock" class="headerlink" title="memlock"></a>memlock</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">oracle           soft    memlock         1048576</span><br><span class="line">oracle           hard    memlock         1048576</span><br></pre></td></tr></table></figure>
<p>The memlock parameter specifies how much memory the oracle user can lock into its address space. Note that Huge Pages are locked in physical memory. The memlock setting is specified in KB and must match the memory size of the number of Huge Pages that Oracle should be able to allocate. So if the Oracle database should be able to use 512 Huge Pages, then memlock must be set to at least 512 * Hugepagesize, which on this system would be 1048576 KB (512<em>1024</em>2). If memlock is too small, then no single Huge Page will be allocated when the Oracle database starts</p>
<h4 id="free-hugepage"><a href="#free-hugepage" class="headerlink" title="free hugepage"></a>free hugepage</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 &gt; /proc/sys/vm/nr_hugepages</span><br></pre></td></tr></table></figure>

<h4 id="manage-tools"><a href="#manage-tools" class="headerlink" title="manage tools"></a>manage tools</h4><p>hugeadm</p>
<h4 id="reduce-OOM-kill-rate"><a href="#reduce-OOM-kill-rate" class="headerlink" title="reduce OOM kill rate"></a>reduce OOM kill rate</h4><p>echo -15 &gt; /proc/${pid}/oom_adj</p>
<h4 id="get-pagesize"><a href="#get-pagesize" class="headerlink" title="get pagesize"></a>get pagesize</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">getconf PAGESIZE</span><br><span class="line">4096</span><br></pre></td></tr></table></figure>

<h3 id="enable-huge-page-for-performance"><a href="#enable-huge-page-for-performance" class="headerlink" title="enable huge page for performance"></a>enable huge page for performance</h3><ul>
<li>Hugepages is a feature that allows the Linux kernel to utilize the multiple page size capabilities of<br>modern hardware architectures.<ul>
<li>A page is the basic unit of virtual memory, with the default page size being 4 KB in the x86 architecture.</li>
</ul>
</li>
<li>Leave sufficient memory for OS use<ul>
<li>no longer subject to normal memory allocations</li>
</ul>
</li>
<li>Huge Pages are ‘pinned’ to physical RAM and cannot be swapped/paged out.<ul>
<li>Preference is for 1G hugepages.</li>
<li>Each hugepage requires a TLB entry. Smaller hugepages =&gt; more TLBs and TLB lookups due to page faults =&gt; higher probability of packet drop blips</li>
</ul>
</li>
</ul>
<p>hugepagesz<br>[HW,IA-64,PPC,X86-64] The size of the HugeTLB pages.<br>On x86-64 and powerpc, this option can be specified multiple times interleaved with hugepages= to reserve huge pages of different sizes. Valid pages sizes on x86-64 are 2M (when the CPU supports “pse”) and 1G (when the CPU supports the “pdpe1gb” cpuinfo flag).</p>
<p>hugepages<br>[HW,X86-32,IA-64] HugeTLB pages to allocate at boot.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hugepagesz&#x3D;1G hugepages&#x3D;224</span><br></pre></td></tr></table></figure>

<h3 id="slab-exhaust-all-memory-because-a-lot-of-scan"><a href="#slab-exhaust-all-memory-because-a-lot-of-scan" class="headerlink" title="slab exhaust all memory because a lot of scan"></a>slab exhaust all memory because a lot of scan</h3><p>top<br><img src="/img/top-mem1.png" alt=""></p>
<p>Why Slab=24021820 kB (24GB)</p>
<p>slabtop<br><img src="/img/slabtop1.png" alt=""> </p>
<p>meminfo</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">MemTotal:       32832340 kB</span><br><span class="line">MemFree:          829744 kB</span><br><span class="line">Buffers:           85096 kB</span><br><span class="line">Cached:          1895216 kB</span><br><span class="line">SwapCached:       164592 kB</span><br><span class="line">Active:          4572372 kB</span><br><span class="line">Inactive:        2520612 kB</span><br><span class="line">Active(anon):    4131784 kB</span><br><span class="line">Inactive(anon):   983612 kB</span><br><span class="line">Active(file):     440588 kB</span><br><span class="line">Inactive(file):  1537000 kB</span><br><span class="line">Unevictable:       25864 kB</span><br><span class="line">Mlocked:            9512 kB</span><br><span class="line">SwapTotal:       8191996 kB</span><br><span class="line">SwapFree:        5818864 kB</span><br><span class="line">Dirty:               120 kB</span><br><span class="line">Writeback:             0 kB</span><br><span class="line">AnonPages:       4974644 kB</span><br><span class="line">Mapped:            12584 kB</span><br><span class="line">Shmem:                36 kB</span><br><span class="line">Slab:           24021820 kB</span><br><span class="line">SReclaimable:   11202668 kB</span><br><span class="line">SUnreclaim:     12819152 kB</span><br><span class="line">KernelStack:       10624 kB</span><br><span class="line">PageTables:        38088 kB</span><br><span class="line">NFS_Unstable:          0 kB</span><br><span class="line">Bounce:                0 kB</span><br><span class="line">WritebackTmp:          0 kB</span><br><span class="line">CommitLimit:    24608164 kB</span><br><span class="line">Committed_AS:    7777952 kB</span><br><span class="line">VmallocTotal:   34359738367 kB</span><br><span class="line">VmallocUsed:      413916 kB</span><br><span class="line">VmallocChunk:   34359269244 kB</span><br><span class="line">HardwareCorrupted:     0 kB</span><br><span class="line">AnonHugePages:     30720 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">DirectMap4k:        4096 kB</span><br><span class="line">DirectMap2M:     1957888 k</span><br></pre></td></tr></table></figure>

<h3 id="memmap"><a href="#memmap" class="headerlink" title="memmap"></a><a href="https://docs.pmem.io/getting-started-guide/creating-development-environments/linux-environments/linux-memmap" target="_blank" rel="noopener">memmap</a></h3><p><a href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt" target="_blank" rel="noopener">https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">memmap=exactmap	[KNL,X86] Enable setting of an exact</span><br><span class="line">			E820 memory map, as specified by the user.</span><br><span class="line">			Such memmap=exactmap lines can be constructed based on</span><br><span class="line">			BIOS output or other requirements. See the memmap=nn@ss</span><br><span class="line">			option description.</span><br><span class="line"></span><br><span class="line">	memmap=nn[KMG]@ss[KMG]</span><br><span class="line">			[KNL] Force usage of a specific region of memory.</span><br><span class="line">			Region of memory to be used is from ss to ss+nn.</span><br><span class="line">			If @ss[KMG] is omitted, it is equivalent to mem=nn[KMG],</span><br><span class="line">			<span class="built_in">which</span> limits max address to nn[KMG].</span><br><span class="line">			Multiple different regions can be specified,</span><br><span class="line">			comma delimited.</span><br><span class="line">			Example:</span><br><span class="line">				memmap=100M@2G,100M<span class="comment">#3G,1G!1024G</span></span><br><span class="line"></span><br><span class="line">	memmap=nn[KMG]<span class="comment">#ss[KMG]</span></span><br><span class="line">			[KNL,ACPI] Mark specific memory as ACPI data.</span><br><span class="line">			Region of memory to be marked is from ss to ss+nn.</span><br><span class="line"></span><br><span class="line">	memmap=nn[KMG]<span class="variable">$ss</span>[KMG]</span><br><span class="line">			[KNL,ACPI] Mark specific memory as reserved.</span><br><span class="line">			Region of memory to be reserved is from ss to ss+nn.</span><br><span class="line">			Example: Exclude memory from 0x18690000-0x1869ffff</span><br><span class="line">			         memmap=64K<span class="variable">$0x18690000</span></span><br><span class="line">			         or</span><br><span class="line">			         memmap=0x10000<span class="variable">$0x18690000</span></span><br><span class="line">			Some bootloaders may need an escape character before <span class="string">'$'</span>,</span><br><span class="line">			like Grub2, otherwise <span class="string">'$'</span> and the following number</span><br><span class="line">			will be eaten.</span><br><span class="line"></span><br><span class="line">	memmap=nn[KMG]!ss[KMG]</span><br><span class="line">			[KNL,X86] Mark specific memory as protected.</span><br><span class="line">			Region of memory to be used, from ss to ss+nn.</span><br><span class="line">			The memory region may be marked as e820 <span class="built_in">type</span> 12 (0xc)</span><br><span class="line">			and is NVDIMM or ADR memory.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ dmesg | grep e820</span><br><span class="line">$ grubby --args&#x3D;&quot;memmap&#x3D;96G:32G&quot; --update-kernel&#x3D;ALL</span><br></pre></td></tr></table></figure>
<p>mapping /dev/pmemX can be used to create a DAX filesystem</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ sudo parted &#x2F;dev&#x2F;pmem0</span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition type?  primary&#x2F;extended? p</span><br><span class="line">File system type?  [ext2]? ext4</span><br><span class="line">Start? 2MiB</span><br><span class="line">End? 100GiB</span><br><span class="line">(parted)</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition type?  primary&#x2F;extended? p</span><br><span class="line">File system type?  [ext2]? ext4</span><br><span class="line">Start? 100GiB</span><br><span class="line">End? 200GiB</span><br><span class="line"></span><br><span class="line">$  getconf PAGE_SIZE</span><br><span class="line">4096</span><br><span class="line"></span><br><span class="line">$ sudo mkfs.ext4 -b 4096 -E stride&#x3D;512 -F &#x2F;dev&#x2F;pmem0</span><br><span class="line">$ sudo mkdir &#x2F;pmem</span><br><span class="line">$ sudo mount -o dax &#x2F;dev&#x2F;pmem0p1 &#x2F;pmem</span><br><span class="line">$ sudo mount -v | grep &#x2F;pmem</span><br><span class="line">$ fallocate --length 1G &#x2F;pmem&#x2F;data</span><br><span class="line">$ echo 1 &gt; &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;events&#x2F;fs_dax&#x2F;dax_pmd_fault_done&#x2F;enable</span><br><span class="line">$ echo 0 &gt; &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;events&#x2F;fs_dax&#x2F;dax_pmd_fault_done&#x2F;enable</span><br><span class="line"></span><br><span class="line"># Verify the namespace is in fsdax mode</span><br><span class="line">$ ndctl list -u</span><br><span class="line">$ cat &#x2F;proc&#x2F;iomem</span><br></pre></td></tr></table></figure>


<h4 id="Monitor-ecc-error"><a href="#Monitor-ecc-error" class="headerlink" title="Monitor ecc error"></a>Monitor ecc error</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dmesg -T |  grep -Ei <span class="string">"Err.*bit ECC"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Monitor multiple bit</span></span><br><span class="line">$ cat /sys/devices/system/edac/mc/mc*/[u,c]e_count | <span class="keyword">while</span> <span class="built_in">read</span> line; <span class="keyword">do</span> [[ ! <span class="variable">$line</span> -eq 0 ]] &amp;&amp; <span class="built_in">echo</span> <span class="string">"Got DRAM ecc error"</span> &amp;&amp; <span class="built_in">exit</span> 2; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="Benchmark-by-perf"><a href="#Benchmark-by-perf" class="headerlink" title="Benchmark by perf"></a>Benchmark by perf</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lscpu</span><br><span class="line">......</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19</span><br><span class="line"></span><br><span class="line">$ perf bench numa mem  -p 1 -t 10  -T 12000  -Irq -H 1 -C 1,3,5,7,9,11,13,15,17,19 -M 0,0,0,0,0,0,0,0,0,0</span><br><span class="line"><span class="comment"># Running 'numa/mem' benchmark:</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># Running main, "perf bench numa numa-mem -p 1 -t 10 -T 12000 -Irq -H 1 -C 1,3,5,7,9,11,13,15,17,19 -M 0,0,0,0,0,0,0,0,0,0"</span></span><br><span class="line">          8.837 secs slowest (max) thread-runtime</span><br><span class="line">          8.000 secs fastest (min) thread-runtime</span><br><span class="line">          8.182 secs average thread-runtime</span><br><span class="line">          4.738 % difference between max/avg runtime</span><br><span class="line">         12.584 GB data processed, per thread</span><br><span class="line">        125.840 GB data processed, total</span><br><span class="line">          0.702 nsecs/byte/thread runtime</span><br><span class="line">          1.424 GB/sec/thread speed</span><br><span class="line">         14.239 GB/sec total speed</span><br><span class="line"></span><br><span class="line">$ perf bench numa mem  -p 1 -t 10  -T 12000  -Irq -H 1 -C 0,2,4,6,8,10,12,14,16,18 -M 0,0,0,0,0,0,0,0,0,0</span><br><span class="line"><span class="comment"># Running 'numa/mem' benchmark:</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># Running main, "perf bench numa numa-mem -p 1 -t 10 -T 12000 -Irq -H 1 -C 0,2,4,6,8,10,12,14,16,18 -M 0,0,0,0,0,0,0,0,0,0"</span></span><br><span class="line">          5.528 secs slowest (max) thread-runtime</span><br><span class="line">          5.000 secs fastest (min) thread-runtime</span><br><span class="line">          5.084 secs average thread-runtime</span><br><span class="line">          4.776 % difference between max/avg runtime</span><br><span class="line">         12.584 GB data processed, per thread</span><br><span class="line">        125.840 GB data processed, total</span><br><span class="line">          0.439 nsecs/byte/thread runtime</span><br><span class="line">          2.276 GB/sec/thread speed</span><br><span class="line">         22.764 GB/sec total speed</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Memory</category>
      </categories>
      <tags>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title>cpu tips</title>
    <url>/2018/05/27/cpu/</url>
    <content><![CDATA[<h3 id="E3-v6-Turbo-not-work-in-CentOS6"><a href="#E3-v6-Turbo-not-work-in-CentOS6" class="headerlink" title="E3 v6 Turbo not work in CentOS6"></a>E3 v6 Turbo not work in CentOS6</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cpupower frequency-set --governor ondemand/powersave/performance</span><br><span class="line"></span><br><span class="line">CentOS 6</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line"></span><br><span class="line">CentOS 7</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h5 id="Check-cpu-freq-instead-of-cat-proc-cpuinfo-grep-MHz"><a href="#Check-cpu-freq-instead-of-cat-proc-cpuinfo-grep-MHz" class="headerlink" title="Check cpu freq, instead of cat /proc/cpuinfo | grep MHz"></a>Check cpu freq, instead of cat /proc/cpuinfo | grep MHz</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># if you could not get the cpu freq from &quot;cpupower frequency-info&quot;</span><br><span class="line"># use the command </span><br><span class="line">cpupower monitor -m Mperf</span><br><span class="line"></span><br><span class="line">cpupower frequency-set -g performance</span><br><span class="line">cpupower idle-set -d 0</span><br><span class="line">cpupower idle-set -d 1</span><br><span class="line">cpupower idle-set -d 2</span><br><span class="line">tuned-adm profile throughput-performance</span><br><span class="line">tuned-adm active</span><br></pre></td></tr></table></figure>

<h4 id="check-driver"><a href="#check-driver" class="headerlink" title="check driver"></a>check driver</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpupower frequency-info</span><br><span class="line">analyzing CPU 0:</span><br><span class="line">  driver: acpi-cpufreq</span><br><span class="line">  CPUs <span class="built_in">which</span> run at the same hardware frequency: 0 1 2 3 4 5 6 7</span><br><span class="line">  CPUs <span class="built_in">which</span> need to have their frequency coordinated by software: 0</span><br><span class="line">  maximum transition latency: 10.0 us</span><br><span class="line">  hardware limits: 800 MHz - 3.70 GHz</span><br><span class="line">  available frequency steps:  3.70 GHz, 3.70 GHz, 3.50 GHz, 3.30 GHz, 3.10 GHz, 2.90 GHz, 2.70 GHz, 2.50 GHz, 2.20 GHz, 2.00 GHz, 1.80 GHz, 1.60 GHz, 1.40 GHz, 1.20 GHz, 1000 MHz, 800 MHz</span><br><span class="line">  available cpufreq governors: ondemand userspace performance</span><br><span class="line">  current policy: frequency should be within 800 MHz and 3.70 GHz.</span><br><span class="line">                  The governor <span class="string">"ondemand"</span> may decide <span class="built_in">which</span> speed to use</span><br><span class="line">                  within this range.</span><br><span class="line">  current CPU frequency: 800 MHz (asserted by call to hardware)</span><br><span class="line">  boost state support:</span><br><span class="line">    Supported: yes</span><br><span class="line">    Active: yes</span><br></pre></td></tr></table></figure>
<p>It ‘s looks like it cound not work in 3.9GHz or 4.1GHz<br>Replace driver to intel_pstate</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">modprobe -r acpi_cpufreq</span><br><span class="line">modprobe intel_pstate</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'install acpi_cpufreq /sbin/modprobe intel_pstate'</span> &gt;/etc/modprobe.d/cpu.conf</span><br><span class="line">dracut -f --add-drivers intel_pstate</span><br><span class="line"></span><br><span class="line">cpupower frequency-info</span><br><span class="line">analyzing CPU 0:</span><br><span class="line">  driver: intel_pstate</span><br><span class="line">  CPUs <span class="built_in">which</span> run at the same hardware frequency: 0</span><br><span class="line">  CPUs <span class="built_in">which</span> need to have their frequency coordinated by software: 0</span><br><span class="line">  maximum transition latency:  Cannot determine or is not supported.</span><br><span class="line">  hardware limits: 800 MHz - 4.10 GHz</span><br><span class="line">  available cpufreq governors: performance powersave</span><br><span class="line">  current policy: frequency should be within 800 MHz and 4.10 GHz.</span><br><span class="line">                  The governor <span class="string">"powersave"</span> may decide <span class="built_in">which</span> speed to use</span><br><span class="line">                  within this range.</span><br><span class="line">  current CPU frequency: 4.00 GHz (asserted by call to hardware)</span><br><span class="line">  boost state support:</span><br><span class="line">    Supported: yes</span><br><span class="line">    Active: yes</span><br><span class="line"></span><br><span class="line">cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line"></span><br><span class="line"><span class="comment">#The intel_pstate driver was added later during RHEL6 life cycle and is available in our kernel since RHEL6.5, but Red Hat decided to keep acpi_cpufreq as default to prevent any potential compatibility issues.</span></span><br></pre></td></tr></table></figure>

<h4 id="When-I-run-pigz-to-benmark"><a href="#When-I-run-pigz-to-benmark" class="headerlink" title="When I run pigz to benmark"></a>When I run pigz to benmark</h4><p>Enable Turbo, acpi-cpufreq(show 3.7GHz) and intel_pstate(3.89GHz) has the same performance (608s finish the benchmark), I can ‘t believe. I test opnessl, Intel® Math Kernel Library , all test result is same. that means in CentOS 6.9 with acpi-cpufreq, just show 3.7GHz, the cpu work in 3.9GHz<br>Disable Turbo in BIOS, acpi-cpufreq(show 3.7GHz) and intel_pstate(3.7GHz) has the same performance too (640s finish the benchmark)<br>About 5% raise, same with cpu frequency distance</p>
<h3 id="Isolated-CPUs-example"><a href="#Isolated-CPUs-example" class="headerlink" title="Isolated CPUs example"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime" target="_blank" rel="noopener">Isolated CPUs example</a></h3><p>Isolating CPUs generally involves:<br>removing all user-space threads;<br>removing any unbound kernel threads (bound kernel threads are tied to a specific CPU and may not be moved);<br>removing interrupts by modifying the /proc/irq/N/smp_affinity property of each Interrupt Request (IRQ) number N in the system.</p>
<p>Plan out CPU allocation, e.g.<br>    * CPUs should NOT be used for more than one task.<br>    * CPUs for host OS<br>    * CPUs for XVIO<br>    * CPUs for VMs<br>        * CPUs for guest OS<br>        * CPUs for VNF or application</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install tuned tuned-profiles-realtime</span><br><span class="line">$ cat /etc/tuned/realtime-variables.conf</span><br><span class="line">isolated_cores=0-3,5,7</span><br><span class="line">$ tuned-adm profile realtime</span><br><span class="line">$ grep isolcpus /proc/cmdline</span><br><span class="line">$</span><br></pre></td></tr></table></figure>
<p><img src="/img/cpu-Isolated-334379.png" alt=""></p>
<h3 id="kernel-parameter"><a href="#kernel-parameter" class="headerlink" title="kernel parameter"></a>kernel parameter</h3><p>○ intel_idle.max_cstate=0 processor.max_cstate=0 idle=mwait<br>    ○ idle=mwait C1 will be entered whenever the core is idle irrespective of other settings<br>    ○ idle=poll keeps the processing cores in C0 state when used in conjunction with intel_idle.max_cstate=0</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grubby --update-kernel=ALL --args=<span class="string">"isolcpus=32-47,48-63 default_hugepagesz=1GB hugepagesz=1G hugepages=12 intel_idle.max_cstate=0 processor.max_cstate=0 idle=mwait"</span></span><br></pre></td></tr></table></figure>

<h4 id="tuna"><a href="#tuna" class="headerlink" title="tuna"></a><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-CPU-Configuration_suggestions#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_suggestions-Configuring_CPU_thread_and_interrupt_affinity_with_Tuna" target="_blank" rel="noopener">tuna</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tuna --cpus CPUs --isolate</span><br><span class="line">$ tuna --cpus CPUs --include</span><br><span class="line">$ tuna --irqs IRQs --cpus CPU --move</span><br><span class="line">$ tuna --irqs sfc1* -c7 -m -x</span><br><span class="line">$ tuna --irqs <span class="string">'enp1s0f*'</span> --socket 0 --spread --show_irqs</span><br><span class="line">$ tuna --threads thread --priority policy:level  <span class="comment">#level:0-99</span></span><br><span class="line">$ tuna --cpus=0,1 --threads=ssh* --move --cpus=2,3 --threads=http* --move</span><br><span class="line">$ tuna --threads 7861 --show_threads</span><br><span class="line">$ tuna --threads 7861 --priority=RR:40</span><br><span class="line">$ tuna -t qemu* -P -c0 -mP -c1 -mP -c+0 -mP</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0 0xff,0xffffffff    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0 0xff,0xffffffff   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0 0xff,0xffffffff   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0 0xff,0xffffffff  54315196        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0 0xff,0xffffffff  42864592         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0        0    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0        0   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0        0   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0        0  54315198        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0        0  42864593         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0        1    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0        1   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0        1   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0        1  54315200        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0        1  42864595         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0      0,1    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0      0,1   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0      0,1   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0      0,1  54315202        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0      0,1  42864597         6630        qemu-kvm</span><br></pre></td></tr></table></figure>

<h3 id="cpu-IPC-Instruction-Per-Clock-cycle"><a href="#cpu-IPC-Instruction-Per-Clock-cycle" class="headerlink" title="cpu IPC (Instruction Per Clock cycle)"></a>cpu IPC (Instruction Per Clock cycle)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpu FSB x multiplier = frequency</span><br><span class="line">cpu frequency x IPC</span><br></pre></td></tr></table></figure>

<h3 id="Benchmark-CPU"><a href="#Benchmark-CPU" class="headerlink" title="Benchmark CPU"></a>Benchmark CPU</h3><h4 id="gcc-x86-options"><a href="#gcc-x86-options" class="headerlink" title="gcc_x86_options"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html" target="_blank" rel="noopener">gcc_x86_options</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-march=cpu-type</span><br><span class="line">Generate instructions <span class="keyword">for</span> the machine <span class="built_in">type</span> cpu-type. </span><br><span class="line"></span><br><span class="line">-mtune=cpu-type ; only generic and intel</span><br><span class="line">Tune to cpu-type everything applicable about the generated code, except <span class="keyword">for</span> the ABI and the <span class="built_in">set</span> of available instructions. </span><br><span class="line"></span><br><span class="line">-mincoming-stack-boundary=num</span><br><span class="line">Assume the incoming stack is aligned to a 2 raised to num byte boundary. If -mincoming-stack-boundary is not specified, the one specified by -mpreferred-stack-boundary is used.</span><br><span class="line">-mmmx</span><br><span class="line">-msse</span><br><span class="line">-msse2</span><br><span class="line">-msse3</span><br><span class="line">-mssse3</span><br><span class="line">-msse4</span><br><span class="line">-msse4a</span><br><span class="line">-msse4.1</span><br><span class="line">-msse4.2</span><br><span class="line">-mavx</span><br><span class="line">-mavx2</span><br><span class="line">-mavx512f</span><br><span class="line">-mavx512pf</span><br><span class="line">-mavx512er</span><br><span class="line">-mavx512cd</span><br><span class="line">-mavx512vl</span><br><span class="line">-mavx512bw</span><br><span class="line">-mavx512dq</span><br><span class="line">-mavx512ifma</span><br><span class="line">-mavx512vbmi</span><br><span class="line">-mavx512vbmi2</span><br><span class="line">-mavx512bf16</span><br><span class="line">-mavx512vpopcntdq</span><br><span class="line">-mavx512vp2intersect</span><br><span class="line">-mavx5124fmaps</span><br><span class="line">-mavx512vnni</span><br><span class="line">-mavx5124vnniw</span><br><span class="line">-mcldemote</span><br><span class="line">.... <span class="comment">## too many</span></span><br><span class="line"></span><br><span class="line">-mprefer-vector-width=opt</span><br><span class="line">This option instructs GCC to use opt-bit vector width <span class="keyword">in</span> instructions instead of default on the selected platform.</span><br><span class="line">none/128/256/512</span><br></pre></td></tr></table></figure>
<p>Check gcc parameters</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ gcc -march&#x3D;native -Q --help&#x3D;target</span><br><span class="line"></span><br><span class="line">$ gcc -march&#x3D;native -Q --help&#x3D;target|grep march</span><br><span class="line">-march&#x3D;                               broadwell</span><br></pre></td></tr></table></figure>

<h3 id="Disable-AVX"><a href="#Disable-AVX" class="headerlink" title="Disable AVX"></a><a href="https://software.intel.com/en-us/articles/introduction-to-intel-advanced-vector-extensions" target="_blank" rel="noopener">Disable AVX</a></h3><p>To use the Intel AVX extensions reliably in most settings, the operating system must support saving and loading the new registers (with XSAVE/XRSTOR) on thread context switches to prevent data corruption. To help avoid such errors, operating systems supporting Intel AVX-aware context switches explicitly set a CPU bit enabling the new instructions; otherwise, an undefined opcode (#UD) exception is generated when Intel AVX instructions are used.</p>
<p>Add kernel parameter to disable AVX </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">noxsave         [BUGS=X86] Disables x86 extended register state save</span><br><span class="line">                and restore using xsave. The kernel will fallback to</span><br><span class="line">                enabling legacy floating-point and sse state.</span><br><span class="line"></span><br><span class="line">noxsaveopt      [X86] Disables xsaveopt used <span class="keyword">in</span> saving x86 extended</span><br><span class="line">                register states. The kernel will fall back to use</span><br><span class="line">                xsave to save the states. By using this parameter,</span><br><span class="line">                performance of saving the states is degraded because</span><br><span class="line">                xsave doesn<span class="string">'t support modified optimization while</span></span><br><span class="line"><span class="string">                xsaveopt supports it on xsaveopt enabled systems.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">noxsaves        [X86] Disables xsaves and xrstors used in saving and</span></span><br><span class="line"><span class="string">                restoring x86 extended register state in compacted</span></span><br><span class="line"><span class="string">                form of xsave area. The kernel will fall back to use</span></span><br><span class="line"><span class="string">                xsaveopt and xrstor to save and restore the states</span></span><br><span class="line"><span class="string">                in standard form of xsave area. By using this</span></span><br><span class="line"><span class="string">                parameter, xsave area per process might occupy more</span></span><br><span class="line"><span class="string">                memory on xsaves enabled systems.</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cpuid | grep XSAVE -i | head</span><br><span class="line">Disclaimer: cpuid may not support decoding of all cpuid registers.</span><br><span class="line">      FXSAVE/FXRSTOR                         = <span class="literal">true</span></span><br><span class="line">      XSAVE/XSTOR states                      = <span class="literal">true</span></span><br><span class="line">      OS-enabled XSAVE/XSTOR                  = <span class="literal">true</span></span><br><span class="line">   XSAVE features (0xd/0):</span><br><span class="line">      bytes required by XSAVE/XRSTOR area     = 0x00000340 (832)</span><br><span class="line">   XSAVE features (0xd/1):</span><br><span class="line">      XSAVEOPT instruction                        = <span class="literal">true</span></span><br><span class="line">      XSAVEC instruction                          = <span class="literal">false</span></span><br><span class="line">      XSAVES/XRSTORS instructions                 = <span class="literal">false</span></span><br><span class="line">      64-byte alignment <span class="keyword">in</span> compacted XSAVE     = <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h2 id="homerl-homerl-To-be-filled-by-O-E-M-Dropbox-blog-hexo-source-posts-cat-cpu-md"><a href="#homerl-homerl-To-be-filled-by-O-E-M-Dropbox-blog-hexo-source-posts-cat-cpu-md" class="headerlink" title="homerl@homerl-To-be-filled-by-O-E-M:~/Dropbox/blog/hexo/source/_posts$ cat cpu.md "></a>homerl@homerl-To-be-filled-by-O-E-M:~/Dropbox/blog/hexo/source/_posts$ cat cpu.md </h2><p>title: cpu tips<br>date: 2018-05-27 15:35:59<br>tags: [cpu,driver]<br>categories: Linux configuration</p>
<hr>
<h4 id="E3-v6-Turbo-not-work-in-CentOS6-1"><a href="#E3-v6-Turbo-not-work-in-CentOS6-1" class="headerlink" title="E3 v6 Turbo not work in CentOS6"></a>E3 v6 Turbo not work in CentOS6</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpupower frequency-set --governor ondemand/powersave/performance</span><br><span class="line"></span><br><span class="line">CentOS 6</span><br><span class="line">[root@cngb-compute-f9-15 0]<span class="comment"># cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span></span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">[root@cngb-compute-f9-15 0]<span class="comment"># cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span></span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line">3701000</span><br><span class="line"></span><br><span class="line">CentOS 7</span><br><span class="line">[root@e3-test /sys/devices/cpu/power]<span class="comment"># cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span></span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">[root@e3-test /sys/devices/cpu/power]<span class="comment"># cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span></span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br><span class="line">3899859</span><br></pre></td></tr></table></figure>
<h5 id="Check-cpu-freq-instead-of-cat-proc-cpuinfo-grep-MHz-1"><a href="#Check-cpu-freq-instead-of-cat-proc-cpuinfo-grep-MHz-1" class="headerlink" title="Check cpu freq, instead of cat /proc/cpuinfo | grep MHz"></a>Check cpu freq, instead of cat /proc/cpuinfo | grep MHz</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># if you could not get the cpu freq from &quot;cpupower frequency-info&quot;</span><br><span class="line"># use the command </span><br><span class="line">cpupower monitor -m Mperf</span><br><span class="line"></span><br><span class="line">cpupower frequency-set -g performance</span><br><span class="line">cpupower idle-set -d 0</span><br><span class="line">cpupower idle-set -d 1</span><br><span class="line">cpupower idle-set -d 2</span><br><span class="line">tuned-adm profile throughput-performance</span><br><span class="line">tuned-adm active</span><br></pre></td></tr></table></figure>

<h4 id="check-driver-1"><a href="#check-driver-1" class="headerlink" title="check driver"></a>check driver</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpupower frequency-info</span><br><span class="line">analyzing CPU 0:</span><br><span class="line">  driver: acpi-cpufreq</span><br><span class="line">  CPUs <span class="built_in">which</span> run at the same hardware frequency: 0 1 2 3 4 5 6 7</span><br><span class="line">  CPUs <span class="built_in">which</span> need to have their frequency coordinated by software: 0</span><br><span class="line">  maximum transition latency: 10.0 us</span><br><span class="line">  hardware limits: 800 MHz - 3.70 GHz</span><br><span class="line">  available frequency steps:  3.70 GHz, 3.70 GHz, 3.50 GHz, 3.30 GHz, 3.10 GHz, 2.90 GHz, 2.70 GHz, 2.50 GHz, 2.20 GHz, 2.00 GHz, 1.80 GHz, 1.60 GHz, 1.40 GHz, 1.20 GHz, 1000 MHz, 800 MHz</span><br><span class="line">  available cpufreq governors: ondemand userspace performance</span><br><span class="line">  current policy: frequency should be within 800 MHz and 3.70 GHz.</span><br><span class="line">                  The governor <span class="string">"ondemand"</span> may decide <span class="built_in">which</span> speed to use</span><br><span class="line">                  within this range.</span><br><span class="line">  current CPU frequency: 800 MHz (asserted by call to hardware)</span><br><span class="line">  boost state support:</span><br><span class="line">    Supported: yes</span><br><span class="line">    Active: yes</span><br></pre></td></tr></table></figure>
<p>It ‘s looks like it cound not work in 3.9GHz or 4.1GHz<br>Replace driver to intel_pstate</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">modprobe -r acpi_cpufreq</span><br><span class="line">modprobe intel_pstate</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'install acpi_cpufreq /sbin/modprobe intel_pstate'</span> &gt;/etc/modprobe.d/cpu.conf</span><br><span class="line">dracut -f --add-drivers intel_pstate</span><br><span class="line"></span><br><span class="line">cpupower frequency-info</span><br><span class="line">analyzing CPU 0:</span><br><span class="line">  driver: intel_pstate</span><br><span class="line">  CPUs <span class="built_in">which</span> run at the same hardware frequency: 0</span><br><span class="line">  CPUs <span class="built_in">which</span> need to have their frequency coordinated by software: 0</span><br><span class="line">  maximum transition latency:  Cannot determine or is not supported.</span><br><span class="line">  hardware limits: 800 MHz - 4.10 GHz</span><br><span class="line">  available cpufreq governors: performance powersave</span><br><span class="line">  current policy: frequency should be within 800 MHz and 4.10 GHz.</span><br><span class="line">                  The governor <span class="string">"powersave"</span> may decide <span class="built_in">which</span> speed to use</span><br><span class="line">                  within this range.</span><br><span class="line">  current CPU frequency: 4.00 GHz (asserted by call to hardware)</span><br><span class="line">  boost state support:</span><br><span class="line">    Supported: yes</span><br><span class="line">    Active: yes</span><br><span class="line"></span><br><span class="line">cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">3899453</span><br><span class="line">[root@cngb-compute-f9-15 cpu]<span class="comment"># cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span></span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line">4100000</span><br><span class="line"></span><br><span class="line"><span class="comment">#The intel_pstate driver was added later during RHEL6 life cycle and is available in our kernel since RHEL6.5, but Red Hat decided to keep acpi_cpufreq as default to prevent any potential compatibility issues.</span></span><br></pre></td></tr></table></figure>

<h4 id="When-I-run-pigz-to-benmark-1"><a href="#When-I-run-pigz-to-benmark-1" class="headerlink" title="When I run pigz to benmark"></a>When I run pigz to benmark</h4><p>Enable Turbo, acpi-cpufreq(show 3.7GHz) and intel_pstate(3.89GHz) has the same performance (608s finish the benchmark), I can ‘t believe. I test opnessl, Intel® Math Kernel Library , all test result is same. that means in CentOS 6.9 with acpi-cpufreq, just show 3.7GHz, the cpu work in 3.9GHz<br>Disable Turbo in BIOS, acpi-cpufreq(show 3.7GHz) and intel_pstate(3.7GHz) has the same performance too (640s finish the benchmark)<br>About 5% raise, same with cpu frequency distance</p>
<h3 id="Isolated-CPUs-example-1"><a href="#Isolated-CPUs-example-1" class="headerlink" title="Isolated CPUs example"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/isolating_cpus_using_tuned-profiles-realtime" target="_blank" rel="noopener">Isolated CPUs example</a></h3><p>Isolating CPUs generally involves:<br>removing all user-space threads;<br>removing any unbound kernel threads (bound kernel threads are tied to a specific CPU and may not be moved);<br>removing interrupts by modifying the /proc/irq/N/smp_affinity property of each Interrupt Request (IRQ) number N in the system.</p>
<p>Plan out CPU allocation, e.g.<br>    * CPUs should NOT be used for more than one task.<br>    * CPUs for host OS<br>    * CPUs for XVIO<br>    * CPUs for VMs<br>        * CPUs for guest OS<br>        * CPUs for VNF or application</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install tuned tuned-profiles-realtime</span><br><span class="line">$ cat /etc/tuned/realtime-variables.conf</span><br><span class="line">isolated_cores=0-3,5,7</span><br><span class="line">$ tuned-adm profile realtime</span><br><span class="line">$ grep isolcpus /proc/cmdline</span><br><span class="line">$</span><br></pre></td></tr></table></figure>
<p><img src="/img/cpu-Isolated-334379.png" alt=""></p>
<h3 id="kernel-parameter-1"><a href="#kernel-parameter-1" class="headerlink" title="kernel parameter"></a>kernel parameter</h3><p>○ intel_idle.max_cstate=0 processor.max_cstate=0 idle=mwait<br>    ○ idle=mwait C1 will be entered whenever the core is idle irrespective of other settings<br>    ○ idle=poll keeps the processing cores in C0 state when used in conjunction with intel_idle.max_cstate=0</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">grubby --update-kernel=ALL --args=<span class="string">"isolcpus=32-47,48-63 default_hugepagesz=1GB hugepagesz=1G hugepages=12 intel_idle.max_cstate=0 processor.max_cstate=0 idle=mwait"</span></span><br></pre></td></tr></table></figure>

<h4 id="tuna-1"><a href="#tuna-1" class="headerlink" title="tuna"></a><a href="https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/performance_tuning_guide/sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-CPU-Configuration_suggestions#sect-Red_Hat_Enterprise_Linux-Performance_Tuning_Guide-Configuration_suggestions-Configuring_CPU_thread_and_interrupt_affinity_with_Tuna" target="_blank" rel="noopener">tuna</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tuna --cpus CPUs --isolate</span><br><span class="line">$ tuna --cpus CPUs --include</span><br><span class="line">$ tuna --irqs IRQs --cpus CPU --move</span><br><span class="line">$ tuna --irqs sfc1* -c7 -m -x</span><br><span class="line">$ tuna --irqs <span class="string">'enp1s0f*'</span> --socket 0 --spread --show_irqs</span><br><span class="line">$ tuna --threads thread --priority policy:level  <span class="comment">#level:0-99</span></span><br><span class="line">$ tuna --cpus=0,1 --threads=ssh* --move --cpus=2,3 --threads=http* --move</span><br><span class="line">$ tuna --threads 7861 --show_threads</span><br><span class="line">$ tuna --threads 7861 --priority=RR:40</span><br><span class="line">$ tuna -t qemu* -P -c0 -mP -c1 -mP -c+0 -mP</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0 0xff,0xffffffff    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0 0xff,0xffffffff   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0 0xff,0xffffffff   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0 0xff,0xffffffff  54315196        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0 0xff,0xffffffff  42864592         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0        0    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0        0   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0        0   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0        0  54315198        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0        0  42864593         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0        1    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0        1   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0        1   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0        1  54315200        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0        1  42864595         6630        qemu-kvm</span><br><span class="line">                      thread       ctxt_switches</span><br><span class="line">    pid SCHED_ rtpri affinity voluntary nonvoluntary             cmd</span><br><span class="line">  153687  OTHER     0      0,1    173264           51        qemu-kvm</span><br><span class="line">  212178  OTHER     0      0,1   1820409          376        qemu-kvm</span><br><span class="line">  376594  OTHER     0      0,1   2480482         1780        qemu-kvm</span><br><span class="line">  380332  OTHER     0      0,1  54315202        41275        qemu-kvm</span><br><span class="line">  382034  OTHER     0      0,1  42864597         6630        qemu-kvm</span><br></pre></td></tr></table></figure>

<h3 id="cpu-IPC-Instruction-Per-Clock-cycle-1"><a href="#cpu-IPC-Instruction-Per-Clock-cycle-1" class="headerlink" title="cpu IPC (Instruction Per Clock cycle)"></a>cpu IPC (Instruction Per Clock cycle)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cpu FSB x multiplier = frequency</span><br><span class="line">cpu frequency x IPC</span><br></pre></td></tr></table></figure>

<h3 id="Benchmark-CPU-1"><a href="#Benchmark-CPU-1" class="headerlink" title="Benchmark CPU"></a>Benchmark CPU</h3><h4 id="gcc-x86-options-1"><a href="#gcc-x86-options-1" class="headerlink" title="gcc_x86_options"></a><a href="https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html" target="_blank" rel="noopener">gcc_x86_options</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-march=cpu-type</span><br><span class="line">Generate instructions <span class="keyword">for</span> the machine <span class="built_in">type</span> cpu-type. </span><br><span class="line"></span><br><span class="line">-mtune=cpu-type ; only generic and intel</span><br><span class="line">Tune to cpu-type everything applicable about the generated code, except <span class="keyword">for</span> the ABI and the <span class="built_in">set</span> of available instructions. </span><br><span class="line"></span><br><span class="line">-mincoming-stack-boundary=num</span><br><span class="line">Assume the incoming stack is aligned to a 2 raised to num byte boundary. If -mincoming-stack-boundary is not specified, the one specified by -mpreferred-stack-boundary is used.</span><br><span class="line">-mmmx</span><br><span class="line">-msse</span><br><span class="line">-msse2</span><br><span class="line">-msse3</span><br><span class="line">-mssse3</span><br><span class="line">-msse4</span><br><span class="line">-msse4a</span><br><span class="line">-msse4.1</span><br><span class="line">-msse4.2</span><br><span class="line">-mavx</span><br><span class="line">-mavx2</span><br><span class="line">-mavx512f</span><br><span class="line">-mavx512pf</span><br><span class="line">-mavx512er</span><br><span class="line">-mavx512cd</span><br><span class="line">-mavx512vl</span><br><span class="line">-mavx512bw</span><br><span class="line">-mavx512dq</span><br><span class="line">-mavx512ifma</span><br><span class="line">-mavx512vbmi</span><br><span class="line">-mavx512vbmi2</span><br><span class="line">-mavx512bf16</span><br><span class="line">-mavx512vpopcntdq</span><br><span class="line">-mavx512vp2intersect</span><br><span class="line">-mavx5124fmaps</span><br><span class="line">-mavx512vnni</span><br><span class="line">-mavx5124vnniw</span><br><span class="line">-mcldemote</span><br><span class="line">.... <span class="comment">## too many</span></span><br><span class="line"></span><br><span class="line">-mprefer-vector-width=opt</span><br><span class="line">This option instructs GCC to use opt-bit vector width <span class="keyword">in</span> instructions instead of default on the selected platform.</span><br><span class="line">none/128/256/512</span><br></pre></td></tr></table></figure>
<p>Check gcc parameters</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ gcc -march&#x3D;native -Q --help&#x3D;target</span><br><span class="line"></span><br><span class="line">$ gcc -march&#x3D;native -Q --help&#x3D;target|grep march</span><br><span class="line">-march&#x3D;                               broadwell</span><br></pre></td></tr></table></figure>

<h3 id="Disable-AVX-1"><a href="#Disable-AVX-1" class="headerlink" title="Disable AVX"></a><a href="https://software.intel.com/en-us/articles/introduction-to-intel-advanced-vector-extensions" target="_blank" rel="noopener">Disable AVX</a></h3><p>To use the Intel AVX extensions reliably in most settings, the operating system must support saving and loading the new registers (with XSAVE/XRSTOR) on thread context switches to prevent data corruption. To help avoid such errors, operating systems supporting Intel AVX-aware context switches explicitly set a CPU bit enabling the new instructions; otherwise, an undefined opcode (#UD) exception is generated when Intel AVX instructions are used.</p>
<p>Add kernel parameter to disable AVX </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">noxsave         [BUGS=X86] Disables x86 extended register state save</span><br><span class="line">                and restore using xsave. The kernel will fallback to</span><br><span class="line">                enabling legacy floating-point and sse state.</span><br><span class="line"></span><br><span class="line">noxsaveopt      [X86] Disables xsaveopt used <span class="keyword">in</span> saving x86 extended</span><br><span class="line">                register states. The kernel will fall back to use</span><br><span class="line">                xsave to save the states. By using this parameter,</span><br><span class="line">                performance of saving the states is degraded because</span><br><span class="line">                xsave doesn<span class="string">'t support modified optimization while</span></span><br><span class="line"><span class="string">                xsaveopt supports it on xsaveopt enabled systems.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">noxsaves        [X86] Disables xsaves and xrstors used in saving and</span></span><br><span class="line"><span class="string">                restoring x86 extended register state in compacted</span></span><br><span class="line"><span class="string">                form of xsave area. The kernel will fall back to use</span></span><br><span class="line"><span class="string">                xsaveopt and xrstor to save and restore the states</span></span><br><span class="line"><span class="string">                in standard form of xsave area. By using this</span></span><br><span class="line"><span class="string">                parameter, xsave area per process might occupy more</span></span><br><span class="line"><span class="string">                memory on xsaves enabled systems.</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cpuid | grep XSAVE -i | head</span><br><span class="line">Disclaimer: cpuid may not support decoding of all cpuid registers.</span><br><span class="line">      FXSAVE/FXRSTOR                         = <span class="literal">true</span></span><br><span class="line">      XSAVE/XSTOR states                      = <span class="literal">true</span></span><br><span class="line">      OS-enabled XSAVE/XSTOR                  = <span class="literal">true</span></span><br><span class="line">   XSAVE features (0xd/0):</span><br><span class="line">      bytes required by XSAVE/XRSTOR area     = 0x00000340 (832)</span><br><span class="line">   XSAVE features (0xd/1):</span><br><span class="line">      XSAVEOPT instruction                        = <span class="literal">true</span></span><br><span class="line">      XSAVEC instruction                          = <span class="literal">false</span></span><br><span class="line">      XSAVES/XRSTORS instructions                 = <span class="literal">false</span></span><br><span class="line">      64-byte alignment <span class="keyword">in</span> compacted XSAVE     = <span class="literal">false</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Linux configuration</category>
      </categories>
      <tags>
        <tag>cpu</tag>
        <tag>driver</tag>
      </tags>
  </entry>
  <entry>
    <title>ipmitool</title>
    <url>/2018/05/27/ipmi/</url>
    <content><![CDATA[<h4 id="set-boot-dev"><a href="#set-boot-dev" class="headerlink" title="set boot dev"></a>set boot dev</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Ipmitool -I lanplus -H x.x.x.x -U USER -P PASSWORD chassis bootdev bios [ pxe | cdrom ] options=persistent</span><br><span class="line">Ipmitool -I lanplus -H x.x.x.x -U USER -P PASSWORD chassis bootdev bios [ pxe | cdrom ]</span><br><span class="line">ipmitool chassis bootdev none options=<span class="built_in">help</span></span><br><span class="line">Legal options settings are:</span><br><span class="line">	<span class="built_in">help</span>:	<span class="built_in">print</span> this message</span><br><span class="line">	valid:	Boot flags valid</span><br><span class="line">	persistent:	Changes are persistent <span class="keyword">for</span> all future boots</span><br><span class="line">	efiboot:	Extensible Firmware Interface Boot (EFI)</span><br><span class="line">	clear-cmos:	CMOS clear</span><br><span class="line">	lockkbd:	Lock Keyboard</span><br><span class="line">	screenblank:	Screen Blank</span><br><span class="line">	lockoutreset:	Lock out Resetbuttons</span><br><span class="line">	lockout_power:	Lock out (power off/sleep request) via Power Button</span><br><span class="line">	verbose=default:	Request quiet BIOS display</span><br><span class="line">	verbose=no:	Request quiet BIOS display</span><br><span class="line">	verbose=yes:	Request verbose BIOS display</span><br><span class="line">	force_pet:	Force progress event traps</span><br><span class="line">	upw_bypass:	User password bypass</span><br><span class="line">	lockout_sleep:	Log Out Sleep Button</span><br><span class="line">	cons_redirect=default:	Console redirection occurs per BIOS configuration setting</span><br><span class="line">	cons_redirect=skip:	Suppress (skip) console redirection <span class="keyword">if</span> enabled</span><br><span class="line">	cons_redirect=<span class="built_in">enable</span>:	Suppress (skip) console redirection <span class="keyword">if</span> enabled</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h4 id="debug-info"><a href="#debug-info" class="headerlink" title="debug info"></a>debug info</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ipmitool lan <span class="built_in">print</span></span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; shell <span class="comment"># get ipmitool shell, type 'help'</span></span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; sel list <span class="comment"># Show system event log</span></span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; sdr <span class="comment"># List sensor data</span></span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; sdr elist</span><br><span class="line">ipmitool event <span class="string">"CPU 1 Temp"</span> <span class="string">"lnc : Lower Non-Critical"</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#filter string</span></span><br><span class="line">ipmitool sdr list </span><br><span class="line">ipmitool sdr <span class="built_in">type</span> list </span><br><span class="line">ipmitool sdr <span class="built_in">type</span> Temperature </span><br><span class="line">ipmitool sdr <span class="built_in">type</span> Fan </span><br><span class="line">ipmitool sdr <span class="built_in">type</span> <span class="string">'Power Supply'</span></span><br><span class="line"></span><br><span class="line">ipmitool sel elist</span><br><span class="line">ipmitool sel time get</span><br></pre></td></tr></table></figure>

<h4 id="system-status"><a href="#system-status" class="headerlink" title="system status"></a>system status</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$ipmitool</span> sdr elist full | grep Avg</span><br><span class="line">Avg Power        | 2Eh | ok  | 21.0 | 200 Watts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ ipmitool sdr <span class="built_in">type</span> <span class="string">'Current'</span></span><br><span class="line">System Level     | 16h | ok  |  7.1 | 268 Watts</span><br><span class="line">Current Latch    | 09h | ns  |  7.1 | Disabled</span><br><span class="line"></span><br><span class="line">$ ipmitool sdr <span class="built_in">type</span> <span class="string">"CPU "</span></span><br><span class="line">Sensor Type <span class="string">"CPU "</span> not found.</span><br><span class="line">Sensor Types:</span><br><span class="line">	Temperature               (0x01)   Voltage                   (0x02)</span><br><span class="line">	Current                   (0x03)   Fan                       (0x04)</span><br><span class="line">	Physical Security         (0x05)   Platform Security         (0x06)</span><br><span class="line">	Processor                 (0x07)   Power Supply              (0x08)</span><br><span class="line">	Power Unit                (0x09)   Cooling Device            (0x0a)</span><br><span class="line">	Other                     (0x0b)   Memory                    (0x0c)</span><br><span class="line">	Drive Slot / Bay          (0x0d)   POST Memory Resize        (0x0e)</span><br><span class="line">	System Firmwares          (0x0f)   Event Logging Disabled    (0x10)</span><br><span class="line">	Watchdog1                 (0x11)   System Event              (0x12)</span><br><span class="line">	Critical Interrupt        (0x13)   Button                    (0x14)</span><br><span class="line">	Module / Board            (0x15)   Microcontroller           (0x16)</span><br><span class="line">	Add-in Card               (0x17)   Chassis                   (0x18)</span><br><span class="line">	Chip Set                  (0x19)   Other FRU                 (0x1a)</span><br><span class="line">	Cable / Interconnect      (0x1b)   Terminator                (0x1c)</span><br><span class="line">	System Boot Initiated     (0x1d)   Boot Error                (0x1e)</span><br><span class="line">	OS Boot                   (0x1f)   OS Critical Stop          (0x20)</span><br><span class="line">	Slot / Connector          (0x21)   System ACPI Power State   (0x22)</span><br><span class="line">	Watchdog2                 (0x23)   Platform Alert            (0x24)</span><br><span class="line">	Entity Presence           (0x25)   Monitor ASIC              (0x26)</span><br><span class="line">	LAN                       (0x27)   Management Subsys Health  (0x28)</span><br><span class="line">	Battery                   (0x29)   Session Audit             (0x2a)</span><br><span class="line">	Version Change            (0x2b)   FRU State                 (0x2c)</span><br><span class="line">$ ipmitool sdr <span class="built_in">type</span> Temperature</span><br><span class="line">CPU Temp         | 01h | ok  |  3.1 | 83 degrees C</span><br><span class="line">PCH Temp         | 0Ah | ok  |  7.1 | 29 degrees C</span><br><span class="line">System Temp      | 0Bh | ok  |  7.2 | 26 degrees C</span><br><span class="line">Peripheral Temp  | 0Ch | ok  |  7.3 | 30 degrees C</span><br><span class="line">ART1             | 2Bh | ok  |  7.4 | 25 degrees C</span><br><span class="line">ART2             | 2Ch | ok  |  7.5 | 29 degrees C</span><br><span class="line">VcpuVRM Temp     | 10h | ns  |  7.16 | No Reading</span><br><span class="line">DIMMA1 Temp      | B0h | ok  | 32.0 | 28 degrees C</span><br><span class="line">DIMMA2 Temp      | B1h | ok  | 32.1 | 28 degrees C</span><br><span class="line">DIMMB1 Temp      | B2h | ok  | 32.2 | 26 degrees C</span><br><span class="line">DIMMB2 Temp      | B3h | ok  | 32.3 | 27 degrees C</span><br></pre></td></tr></table></figure>

<h4 id="User-and-network"><a href="#User-and-network" class="headerlink" title="User and network"></a>User and network</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ipmitool -I lanplus </span><br><span class="line">ipmitool lan <span class="built_in">set</span> 1 ipsrc [ static | dhcp ] </span><br><span class="line">ipmitool lan <span class="built_in">set</span> 1 ipaddr &lt;ip&gt;</span><br><span class="line">ipmitool lan <span class="built_in">set</span> 1 netmask 255.255.255.0</span><br><span class="line">ipmitool -I lanplus -H &lt;ip&gt; -U &lt;user&gt; -P &lt;user&gt; lan <span class="built_in">set</span> 1 defgw ipaddr &lt;ip&gt;</span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; -P &lt;pass&gt; user <span class="built_in">set</span> name 5 admin</span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; -P &lt;pass&gt; user list</span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; -P &lt;pass&gt; user <span class="built_in">set</span> password 5 &lt;pass&gt; </span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; -P &lt;pass&gt; channel setaccess 1 5 link=on ipmi=on callin=on privilege=4</span><br><span class="line">ipmitool -H &lt;ip&gt; -U &lt;user&gt; -P &lt;pass&gt; user <span class="built_in">enable</span> 5</span><br></pre></td></tr></table></figure>

<h4 id="Get-NIC-mac-from-supermicro-server"><a href="#Get-NIC-mac-from-supermicro-server" class="headerlink" title="Get NIC mac from supermicro server"></a>Get NIC mac from supermicro server</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">You can find the MAC address <span class="keyword">for</span> LAN1/eth0 (not the BMC MAC) via the SuperMicro IPMI interface by running the following <span class="built_in">command</span>:</span><br><span class="line">ipmitool -U <span class="variable">$IPMI_USER</span> -P <span class="variable">$IPMI_PASS</span> -H <span class="variable">$IPMI_HOST</span> raw 0x30 0x21 | tail -c 18</span><br><span class="line">The eth0 MAC address will be output <span class="keyword">in</span> this format:</span><br><span class="line">00 25 90 f0 be ef</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">#### reset mc</span></span><br><span class="line">```bash</span><br><span class="line">ipmitool bmc reset cold</span><br></pre></td></tr></table></figure>

<h4 id="get-SOL-console"><a href="#get-SOL-console" class="headerlink" title="get SOL console"></a>get SOL console</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">modprobe lanplus <span class="comment"># If not yet loaded</span></span><br><span class="line">ipmitool -H &lt;IP&gt; -U &lt;user&gt; -I lanplus sol activate</span><br></pre></td></tr></table></figure>

<h4 id="Set-power-policy"><a href="#Set-power-policy" class="headerlink" title="Set power policy"></a>Set power policy</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ipmitool -H $JBOD_BMCIP -U $user -P $your_password chassis policy previous</span><br><span class="line">    always-on   : turn on when power is restored</span><br><span class="line">    previous    : return to previous state when power is restored</span><br><span class="line">    always-off  : stay off after power is restored</span><br></pre></td></tr></table></figure>

<h3 id="IPMI-log-issues"><a href="#IPMI-log-issues" class="headerlink" title="IPMI log issues"></a>IPMI log issues</h3><p>Pass the “ipmitool sel elist”, I found some interesting things</p>
<p>case1:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">5de1 | 02/27/2018 | 00:22:39 | Voltage Vcpu | Lower Non-recoverable going low  | Deasserted | Reading 1.22 &lt; Threshold 0.49 Volts</span><br><span class="line">5de2 | 02/27/2018 | 00:22:39 | Voltage Vcpu | Lower Critical going low  | Deasserted | Reading 1.22 &lt; Threshold 0.49 Volts</span><br></pre></td></tr></table></figure>


<p><a href="https://www.intel.co.uk/content/www/uk/en/support/articles/000024276/server-products/intel-data-center-blocks-intel-dcb.html" target="_blank" rel="noopener">case2:</a><br>Here is the issues:<br>In most cases, the event points to the Alternating Current (AC) power (electrical outlet power) spiking under the minimum warning and fault thresholds for over 20 milliseconds while the system powered on. Therefore, check the power source.</p>
<p>Other steps to consider if necessary:</p>
<p>Make sure that the firmware is updated to the latest version.<br>Check for foreign objects inside the chassis such as screws that could grind the board. Reset the power supplies.<br>Correctly attach the Intel® Server Board to the chassis base with the spacers/stand-offs. Refer to the board service guide if in doubt and make sure not to tighten the screws too much.<br>Check that the cables connecting the chassis front panel to the Intel® Server Board are plugged in properly to the front panel header.<br>Make sure the drive ribbon cables inside the computer are attached correctly and securely.<br>If you’re using a UPS, see to it that it’s compliant. Access Frequently Asked Questions About UPS Support for more details.</p>
<p>If, despite these above recommendations, the situation persists, contact Support with the SysInfo event logs for further diagnosis.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Warning event: PS2 Status reports a predictive failure due to under-voltage that has been detected. This state may result <span class="keyword">in</span> power supply failure.</span><br><span class="line">Warning event: PS1 Status reports a predictive failure due to under-voltage that has been detected. This state may result <span class="keyword">in</span> power supply failure.</span><br><span class="line">Warning event: PS2 Status reports the power supply<span class="string">'s input (AC/DC) has been lost.</span></span><br><span class="line"><span class="string">CRITICAL event: Pwr Unit Status reports that the power unit has suffered a failure (power supply failure).</span></span><br></pre></td></tr></table></figure>

<p>Here is the production env<br>At 2019-03-10 the power supply has the jitter in Power Supply, I get this error in a lot of servers at the same time, and they are in the same data center and in diff rack</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1a | 03/10/2019 | 16:04:21 | Power Supply Status | Power Supply AC lost | Deasserted</span><br><span class="line">1b | 03/10/2019 | 16:04:31 | Power Supply PS Redundancy | Fully Redundant | Asserted</span><br></pre></td></tr></table></figure>

<h3 id="IPMI-hex-to-ipaddr"><a href="#IPMI-hex-to-ipaddr" class="headerlink" title="IPMI hex to ipaddr"></a>IPMI hex to ipaddr</h3><p>C0 A8 30 AF<br>16<em>12(c)+0=192<br>16</em>10(a)+8=168<br>16<em>3+0=48<br>16</em>10(a)+16=176</p>
<p>192.168.48.176</p>
<p><a href="https://discuss.pivotal.io/hc/en-us/articles/206396927-How-to-work-on-IPMI-and-IPMITOOL" target="_blank" rel="noopener">reference</a><br><a href="https://www.intel.co.uk/content/www/uk/en/support/articles/000024276/server-products/intel-data-center-blocks-intel-dcb.html" target="_blank" rel="noopener">reference2</a></p>
]]></content>
      <categories>
        <category>ipmi</category>
      </categories>
      <tags>
        <tag>ipmi</tag>
      </tags>
  </entry>
  <entry>
    <title>BIOS</title>
    <url>/2018/05/27/bios/</url>
    <content><![CDATA[<ul>
<li>C-state disabled. Ensure that the CPUs are always in the C0 (fully operational) state. Each CPU has several power modes and they are collectively called “C-states” or “C-modes.”.<br><del>~ *P-state disabled. The processor P-state is the capability of running the processor at different voltage and/or frequency levels. Generally, P0 is the highest state resulting in maximum performance, while P1, P2, and so on, will save power but at some penalty to CPU performance. ~</del><ul>
<li>I can ‘t agree it, I will active the P-state</li>
</ul>
</li>
<li>Hyper-threading enabled/disabled<ul>
<li>it ‘s depends your application<br><del>~ * Turbo Boost Disabled. Turbo Boost has a series of dynamic frequency scaling technologies enabling the processor to run above its base operating frequency via dynamic control. Although Turbo Boost (overclocking) would increase the performance, not all cores would be running at the turbo frequency and it would impact the consistency of the performance tests, thus it would not be a real representation of the platform performance. ~</del></li>
<li>I can ‘t agree it, From Xeon E5 v2 to Scalable Xeon, intel just write single core turbo in ark, but it ‘s not means all cores could bot be turbo.  Eg: I enabled all core turbo for E5 2630v4, the performance same with not enable Turbo from E5 2640v4. I have test E7-4890v2, default cpu frequency only 2.8GHz, All cores turbo could reach 3.2GHz</li>
</ul>
</li>
</ul>
<a id="more"></a>

<ul>
<li><p>System Management Interrupts disabled. Disabling the Processor Power and Utilization Monitoring SMI has a great effect because it generates a processor interrupt eight times a second in G6 and later servers. Disabling the Memory Pre-Failure Notification SMI has a smaller effect because it generates an interrupt at a lower frequency</p>
</li>
<li><p>Correct setting of BIOS for DUT (w/ SmartNIC) is important, to address:</p>
<ul>
<li>Throughput variations due to CPU power state changes (C-state; P-state)</li>
<li>Blips (short duration packet drops) due to interrupts (SMI)</li>
</ul>
</li>
<li><p>Power Management</p>
<ul>
<li>Power Regulator = [Static High Performance] (was “dynamic power<br>savings”)</li>
<li>Minimum Processor Core Idle C-state = [No C-states] (was C-6)</li>
<li>Minimum Package Idle C-state = [ No package state] (was C-6)</li>
<li>Power Profile = [ Max Performance ] (was “customer”) this ended up setting the above anyway (and graying them out)</li>
</ul>
</li>
<li><p>Prevent CPUs from entering power savings modes as transitions cause OS and VM drivers to drop packets.</p>
<ul>
<li>Yes, I have the same issues.</li>
</ul>
</li>
</ul>
<p>Example settings (HP) changes (cont.)</p>
<ul>
<li>SMI - System Management Interrupts<ul>
<li>Memory pre-failure notification = [disabled] (was enabled)</li>
</ul>
</li>
<li>When enabled, CPU is periodically interrupted, resulting in packet drops when close to throughput<br>capacity.<ul>
<li>Disable all non-essential SMIs</li>
<li>Frequency of SMI varies<ul>
<li>On an HP DL380 gen9 it is every 60s</li>
<li>On gen6,7 it is every hour</li>
<li>On gen8 every 5 minutes</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>System management interrupts (SMIs) are used to offer extended functionality, such as legacy hardware device emulation. They can also be used for system management tasks. SMIs are similar to NMIs in that they use a special electrical signalling line directly into the CPU, and are generally not able to be masked.<br>When an SMI is received, the CPU will enter System Management Mode (SMM). In this mode, a very low-level handler routine is run to handle the SMIs. The SMM is typically provided directly from the system management firmware, often the BIOS or the EFI.<br>SMIs are most often used to provide legacy hardware emulation. A common example is to emulate a diskette drive. If there is no diskette attached to the system, a virtualized network-managed emulation can be used instead. When the operating system attempts to access the diskette, an SMI is triggered and a handler provides the operating system with an emulated device instead. The operating system then treats the emulation as though it were the legacy device itself.<br>Red Hat Enterprise Linux for Real Time can be adversely affected by SMIs because they take place without the direct involvement of the operating system. A poorly written SMI handling routine may consume many milliseconds of CPU time, and the operating system is not able to preempt the handler if it needs to. This situation creates periodic high latencies in an otherwise well-tuned, highly responsive system. Unfortunately, because SMI handlers can be used by a vendor to manage CPU temperature and fan control, it is not possible to disable them. Instead, it is recommended that you notify the vendor of the problem.<br>You can attempt to isolate SMIs on a Red Hat Enterprise Linux for Real Time system using the hwlatdetect utility, which is available in the rt-tests package. This utility is designed to measure periods of time during which the CPU has been stolen by an SMI handling routine.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">processor	: 119</span><br><span class="line">vendor_id	: GenuineIntel</span><br><span class="line">cpu family	: 6</span><br><span class="line">model		: 62</span><br><span class="line">model name	: Intel(R) Xeon(R) CPU E7-4890 v2 @ 2.80GHz</span><br><span class="line">stepping	: 7</span><br><span class="line">microcode	: 0x713</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cache size	: 38400 KB</span><br><span class="line">physical id	: 3</span><br><span class="line">siblings	: 30</span><br><span class="line">core id		: 14</span><br><span class="line">cpu cores	: 15</span><br><span class="line">apicid		: 125</span><br><span class="line">initial apicid	: 125</span><br><span class="line">fpu		: yes</span><br><span class="line">fpu_exception	: yes</span><br><span class="line">cpuid level	: 13</span><br><span class="line">wp		: yes</span><br><span class="line">flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm epb intel_ppin ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms xsaveopt dtherm ida arat pln pts spec_ctrl intel_stibp</span><br><span class="line">bogomips	: 5638.89</span><br><span class="line">clflush size	: 64</span><br><span class="line">cache_alignment	: 64</span><br><span class="line">address sizes	: 46 bits physical, 48 bits virtual</span><br><span class="line">power management:</span><br><span class="line"></span><br><span class="line">cat /proc/cpuinfo | grep MHz</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">cpu MHz		: 3199.902</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<h3 id="mpt3sas-mpt2sas-HBA-boot-with-high-density-JBOD"><a href="#mpt3sas-mpt2sas-HBA-boot-with-high-density-JBOD" class="headerlink" title="mpt3sas/mpt2sas HBA boot with high density JBOD"></a>mpt3sas/mpt2sas HBA boot with high density JBOD</h3><h4 id="Set-quick-boot"><a href="#Set-quick-boot" class="headerlink" title="Set quick boot"></a>Set quick boot</h4><p>Under BIOS boot mode and enable HBA ctrl + C option<br><img src="/img/mpt3sas-no-boot.jpg" alt=""></p>
<h3 id="Power-setting"><a href="#Power-setting" class="headerlink" title="Power setting"></a>Power setting</h3><h4 id="OS-control"><a href="#OS-control" class="headerlink" title="OS control"></a>OS control</h4><p><img src="/img/power-policy1.png" alt=""></p>
<h4 id="1-1-loading-balance-mode-for-power-supply"><a href="#1-1-loading-balance-mode-for-power-supply" class="headerlink" title="1+1 loading balance mode for power supply"></a>1+1 loading balance mode for power supply</h4><p><img src="/img/power-policy2.png" alt=""></p>
<p><a href="https://open-nfp.org/m/documents/Measuring_a_25_and_40Gbps_Data_Plane.pdf" target="_blank" rel="noopener">reference</a><br><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/system_management_interrupts" target="_blank" rel="noopener">reference2</a></p>
]]></content>
      <categories>
        <category>bios</category>
      </categories>
      <tags>
        <tag>bios</tag>
      </tags>
  </entry>
  <entry>
    <title>Ansible</title>
    <url>/2018/04/12/ansible/</url>
    <content><![CDATA[<h3 id="Continue-run"><a href="#Continue-run" class="headerlink" title="Continue run"></a>Continue run</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ansible-playbook playbook.yml --start-at=<span class="string">"install packages"</span></span><br><span class="line">ansible-playbook playbook.yml --step <span class="comment">## every step will ask you</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>

<h3 id="Custom-install-data"><a href="#Custom-install-data" class="headerlink" title="Custom install data"></a>Custom install data</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat group_vars/all</span><br><span class="line"></span><br><span class="line">ieel_ver: lustre2_102</span><br><span class="line">intel_ieel:</span><br><span class="line">  ieel310:</span><br><span class="line">    lustre:</span><br><span class="line">      - ieel310/lustre-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-modules-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-debuginfo-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-osd-ldiskfs-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-osd-ldiskfs-mount-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-osd-zfs-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-osd-zfs-mount-2.7.19.8-3.10.0_514.2.2.el7_lustre.x86_64.x86_64.rpm</span><br><span class="line">      - ieel310/lustre-iokit-2.7.19.8-3.10.0_514.2.2.el7.x86_64.x86_64.rpm</span><br><span class="line">    zfs:</span><br><span class="line">      - ieel310/spl-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">      - ieel310/spl-dkms-0.6.5.7-1.el7.noarch.rpm</span><br><span class="line">      - ieel310/zfs-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libzpool2-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libzfs2-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libuutil1-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libnvpair1-0.6.5.7-1.el7.x86_64.rpm</span><br><span class="line">    e2fsprogs:</span><br><span class="line">      - ieel310/e2fsprogs-1.42.13.wc5-7.el7.x86_64.rpm</span><br><span class="line">      - ieel310/e2fsprogs-libs-1.42.13.wc5-7.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libcom_err-1.42.13.wc5-7.el7.x86_64.rpm</span><br><span class="line">      - ieel310/libss-1.42.13.wc5-7.el7.x86_64.rpm</span><br><span class="line">    kernel:</span><br><span class="line">      - ieel310/kernel-3.10.0-514.2.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - ieel310/kernel-debuginfo-3.10.0-514.2.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - ieel310/kernel-debuginfo-common-x86_64-3.10.0-514.2.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - ieel310/kernel-devel-3.10.0-514.2.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - ieel310/kernel-headers-3.10.0-514.2.2.el7_lustre.x86_64.rpm</span><br><span class="line">    dkms:</span><br><span class="line">      - ieel310/dkms-2.3-1.20161202gitde1dca9.el7.noarch.rpm</span><br><span class="line">      - ieel310/lustre-dkms-2.7.19.8-1.el7.noarch.rpm</span><br><span class="line">      - ieel310/zfs-dkms-0.6.5.7-1.el7.noarch.rpm</span><br><span class="line">  lustre2_102:</span><br><span class="line">    e2fsprogs:</span><br><span class="line">      - lustre2_102/e2fsprogs-1.42.13.wc6-7.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/e2fsprogs-libs-1.42.13.wc6-7.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/e2fsprogs-static-1.42.13.wc6-7.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libcom_err-1.42.13.wc6-7.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libss-1.42.13.wc6-7.el7.x86_64.rpm</span><br><span class="line">    lustre:</span><br><span class="line">      - lustre2_102/lustre-2.10.2-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/lustre-osd-zfs-mount-2.10.2-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/lustre-osd-ldiskfs-mount-2.10.2-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/kmod-lustre-osd-ldiskfs-2.10.2-1.el7.x86_64.rpm</span><br><span class="line">    zfs:</span><br><span class="line">      - lustre2_102/ksh-20120801-34.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libnvpair1-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libuutil1-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libzfs2-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/libzpool2-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/zfs-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">      - lustre2_102/spl-0.7.3-1.el7.x86_64.rpm</span><br><span class="line">    dkms:</span><br><span class="line">      - lustre2_102/lustre-dkms-2.10.2-1.el7.noarch.rpm</span><br><span class="line">      - lustre2_102/zfs-dkms-0.7.3-1.el7.noarch.rpm</span><br><span class="line">      - lustre2_102/dkms-2.3-5.20170523git8c3065c.el7.noarch.rpm</span><br><span class="line">      - lustre2_102/spl-dkms-0.7.3-1.el7.noarch.rpm</span><br><span class="line">    kernel:</span><br><span class="line">      - lustre2_102/kernel-3.10.0-693.5.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - lustre2_102/kernel-devel-3.10.0-693.5.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - lustre2_102/kernel-headers-3.10.0-693.5.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - lustre2_102/kernel-tools-3.10.0-693.5.2.el7_lustre.x86_64.rpm</span><br><span class="line">      - lustre2_102/kernel-tools-libs-3.10.0-693.5.2.el7_lustre.x86_64.rpm</span><br><span class="line">    sasdriver:</span><br><span class="line">      - lustre2_102/kmod-mpt3sas-26.00.00.00-1.x86_64.rpm</span><br><span class="line"></span><br><span class="line">- name: check lustre <span class="built_in">type</span></span><br><span class="line">  shell: bash -c <span class="string">'hostname | grep -Ei "(mds|oss)"'</span></span><br><span class="line">  register: lustre_type</span><br><span class="line">  ignore_errors: True</span><br><span class="line"></span><br><span class="line">- debug:</span><br><span class="line">   var: lustre_type.stdout</span><br><span class="line"></span><br><span class="line">- name: copy mds.conf to modprobe.d</span><br><span class="line">  copy: src=&#123;&#123; item &#125;&#125; dest=/etc/modprobe.d/lustre.conf</span><br><span class="line">  with_items:</span><br><span class="line">   - mds.conf</span><br><span class="line">  when: <span class="string">'"mds" in hostvars[inventory_hostname].lustre_type.stdout'</span></span><br><span class="line"></span><br><span class="line">- name: copy oss.conf to modprobe.d</span><br><span class="line">  copy: src=&#123;&#123; item &#125;&#125; dest=/etc/modprobe.d/lustre.conf</span><br><span class="line">  with_items:</span><br><span class="line">   - oss.conf</span><br><span class="line">  when: <span class="string">'"oss" in hostvars[inventory_hostname].lustre_type.stdout'</span></span><br><span class="line"></span><br><span class="line">- debug: var=intel_ieel.&#123;&#123; ieel_ver &#125;&#125;.kernel</span><br><span class="line"></span><br><span class="line">- name: Copy kernel packages</span><br><span class="line">  copy: src=&#123;&#123; item &#125;&#125; dest=&#123;&#123; templfs &#125;&#125;</span><br><span class="line">  with_items:</span><br><span class="line">   - <span class="string">"&#123;&#123; intel_ieel[ieel_ver].kernel &#125;&#125;"</span></span><br><span class="line"></span><br><span class="line">- name: Install lustre kernel</span><br><span class="line">  shell: <span class="built_in">cd</span> &#123;&#123; templfs &#125;&#125; &amp;&amp; rpm -Uvh --force kernel-*.rpm &amp;&amp; sleep 5</span><br></pre></td></tr></table></figure>

<h3 id="localintall"><a href="#localintall" class="headerlink" title="localintall"></a>localintall</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">- name: Copy dell racadm</span><br><span class="line">  copy: src=&#123;&#123; item &#125;&#125; dest=/dev/shm</span><br><span class="line">  with_items:</span><br><span class="line">   - dell_racadm/libsmbios-2.3.1-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/libsmbiosc++-2.3.1-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/smbios-utils-bin-2.3.1-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-argtable2-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-deng-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-deng-snmp-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-hapi-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idrac7-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idracadm7-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idracadm-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idrac-ivmcli-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idrac-snmp-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-idrac-vmcli-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-isvc-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-isvc-snmp-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-omacs-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-omcommon-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-omilcore-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-racadm4-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-racadm5-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-rac-components-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line">   - dell_racadm/srvadmin-racdrsc-9.1.2-2965.12849.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">- name: find templfs all lustre releated rpm, using <span class="keyword">for</span> yum localinstall</span><br><span class="line">  find:</span><br><span class="line">    paths: <span class="string">"/dev/shm"</span></span><br><span class="line">    patterns: <span class="string">'*.rpm'</span></span><br><span class="line">  register: dellrpm</span><br><span class="line"></span><br><span class="line">- name: Install racadm rpm</span><br><span class="line">  yum:</span><br><span class="line">    name: <span class="string">"&#123;&#123; dellrpm.files | map(attribute='path') | list &#125;&#125;"</span></span><br><span class="line">    state: present</span><br></pre></td></tr></table></figure>

<h3 id="reboot"><a href="#reboot" class="headerlink" title="reboot"></a>reboot</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">##reboot</span></span><br><span class="line">- name: first restart server <span class="keyword">for</span> lutre kernel</span><br><span class="line">  shell: bash -c <span class="string">'(sleep 2 &amp;&amp; reboot -f)'</span></span><br><span class="line">  async: 1</span><br><span class="line">  poll: 0</span><br><span class="line"></span><br><span class="line">- name: waiting <span class="keyword">for</span> server to come back after reboot</span><br><span class="line">  local_action: wait_for host=&#123;&#123; inventory_hostname &#125;&#125; state=started timeout=600 delay=10 connect_timeout=600 port=22</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>ansible</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title>openzfs issues</title>
    <url>/2017/09/26/openzfs-issues/</url>
    <content><![CDATA[<p>OpenZFS on linux issues record</p>
<a id="more"></a>

<h3 id="About-SMR-HDD"><a href="#About-SMR-HDD" class="headerlink" title="About SMR HDD"></a><a href="https://github.com/zfsonlinux/zfs/issues/4877" target="_blank" rel="noopener">About SMR HDD</a></h3><p>Another thing you might try is to leave the block size set at 1M but increase the zfs_vdev_aggregation_limit to 16M. This way as long as your doing 1M aligned IO you should never write partial blocks and leave holes. ZFS will aggregate these 1M blocks in to larger 16M IOs to the disk.</p>
<h3 id="I-don-‘t-think-it-‘s-the-MMP-issue-because-the-same-issue-in-OpenZFS-0-6-5-7"><a href="#I-don-‘t-think-it-‘s-the-MMP-issue-because-the-same-issue-in-OpenZFS-0-6-5-7" class="headerlink" title="I don ‘t think it ‘s the MMP issue, because the same issue in OpenZFS 0.6.5.7"></a>I don ‘t think it ‘s the MMP issue, because the same issue in OpenZFS 0.6.5.7</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"> state: DEGRADED</span><br><span class="line">status: One or more devices could not be used because the label is missing or</span><br><span class="line">        invalid.  Sufficient replicas exist <span class="keyword">for</span> the pool to <span class="built_in">continue</span></span><br><span class="line">        functioning <span class="keyword">in</span> a degraded state.</span><br></pre></td></tr></table></figure>

<h4 id="Reproduce-the-issue-and-my-work-experiences"><a href="#Reproduce-the-issue-and-my-work-experiences" class="headerlink" title="Reproduce the issue and my work experiences."></a>Reproduce the issue and my work experiences.</h4><ol>
<li>offline 4 disks at the single zpool<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdc/device/delete</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdd/device/delete</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sde/device/delete</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdf/device/delete</span><br></pre></td></tr></table></figure></li>
<li>Plug out SAS expander or SAS Adapter or SAS cable you could reproduce it</li>
<li>Bad SAS cable, replace cable</li>
<li>SAS HBA,Expander,device Firmware update could reslove it<br> especially some of 10TB HDD old firmware, the hdd could be idle, hang, cause the system timeout<br><img src="http://homerl.github.io/2018/06/05/SAS-JBOD-issues/" alt="For more info"></li>
</ol>
<h5 id="Looks-like-MMP-will-cause-more-issues-on-zfs-0-7-9"><a href="#Looks-like-MMP-will-cause-more-issues-on-zfs-0-7-9" class="headerlink" title="Looks like MMP will cause more issues on zfs 0.7.9"></a>Looks like MMP will cause more issues on zfs 0.7.9</h5><p><a href="https://github.com/zfsonlinux/zfs/issues/745" target="_blank" rel="noopener">745</a>, The origin of Evil, because lustre, lustre will destory everything ,haha<br><a href="https://github.com/zfsonlinux/zfs/issues/7045" target="_blank" rel="noopener">7045</a>, because some of sas dev too busy, improve the check time<br><a href="https://github.com/zfsonlinux/zfs/issues/7118" target="_blank" rel="noopener">7118</a>, MMP cause more issues, if your JBOD/HBD/HDD has not return immediately<br><a href="https://github.com/zfsonlinux/zfs/issues/7709" target="_blank" rel="noopener">7709</a></p>
<p>zfs_multihost_fail_intervals:Max allowed period without a successful mmp write<br>zfs_multihost_import_intervals:Number of zfs_multihost_interval periods to wait for activity<br>zfs_multihost_interval:Milliseconds between mmp writes to each leaf</p>
<p>echo 100 &gt;  /sys/module/zfs/parameters/zfs_multihost_fail_intervals;<br>echo 200 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals<br>echo 6000 &gt; /sys/module/zfs/parameters/zfs_multihost_interval<br>100 zfs_multihost_fail_intervals * 6000 zfs_multihost_interval = 600,000 ms</p>
<p>When  zfs_multihost_fail_intervals &gt; 0 then sequential mul‐tihost write failures will cause the pool to be  suspended.<br>      This occurs when zfs_multihost_fail_intervals * zfs_multi‐host_interval milliseconds have passed since the last successful<br>      multihost write.  This guarantees the activity test will see multihost writes if the pool is imported.</p>
<p>zfs_multihost_fail_intervals &gt;= zfs_multihost_import_intervals which could allow a pool to be imported on two nodes without any warnings.<br>MMP writes are done in the context of the sync thread</p>
<p>Looks like you have to reduce block timeout to avoid zfs MMP timeout, that too bad</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">  pool: zpool</span><br><span class="line"> state: ONLINE</span><br><span class="line">status: The pool is suspended because multihost writes failed or were delayed;</span><br><span class="line">	another system could import the pool undetected</span><br></pre></td></tr></table></figure>

<h4 id="Openzfs-issue"><a href="#Openzfs-issue" class="headerlink" title="Openzfs issue"></a>Openzfs issue</h4><p><img src="https://github.com/zfsonlinux/zfs/issues/7118" alt="7118"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 100 &gt;/sys/module/zfs/parameters/zfs_multihost_history</span><br><span class="line">&lt;run <span class="built_in">test</span>&gt;</span><br><span class="line">cat /proc/spl/kstat/zfs/&lt;pool&gt;/multihost</span><br><span class="line"></span><br><span class="line">or you could increase the timeout intervals at zfs_multihost_fail_intervals </span><br><span class="line">my production env zfs version is 0.7.3</span><br><span class="line"></span><br><span class="line"><span class="comment">## get zpool log</span></span><br><span class="line">zpool events -v</span><br></pre></td></tr></table></figure>

<h3 id="zpool-could-not-mount"><a href="#zpool-could-not-mount" class="headerlink" title="[zpool could not mount]"></a>[zpool could not mount]</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zpool import -o <span class="built_in">readonly</span>=on -o cachefile=none tank</span><br><span class="line">$ zpool import -Nm tank</span><br></pre></td></tr></table></figure>

<h3 id="Replace-FX-parameter"><a href="#Replace-FX-parameter" class="headerlink" title="[Replace FX parameter]"></a>[Replace FX parameter]</h3><h4 id="very-dangerours"><a href="#very-dangerours" class="headerlink" title="very dangerours"></a>very dangerours</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">### -F will cause unrecoverable result, don 't use it</span></span><br><span class="line">$ zpool import -fFX -R /tmp/tank ost_11</span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">### Just check, not take action</span></span><br><span class="line">pool=<span class="string">"your_pool_name"</span></span><br><span class="line"><span class="comment">#find a vdev of pool</span></span><br><span class="line">vdev=<span class="string">"/dev/mapper/35000cca2515fc6ac"</span></span><br><span class="line">zdb -lu <span class="variable">$vdev</span> |grep <span class="string">"txg "</span> |cut -d <span class="string">" "</span> -f 3 &gt; /tmp/txg</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> LINE;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">zdb -e <span class="variable">$pool</span> -d -t <span class="variable">$LINE</span></span><br><span class="line"><span class="keyword">done</span> &lt; /tmp/txg</span><br></pre></td></tr></table></figure>


<h4 id="Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang"><a href="#Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang" class="headerlink" title="Import zpool hang, single dev just respond very slow cause all zpool command hang"></a>Import zpool hang, single dev just respond very slow cause all zpool command hang</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/block/sddh/device/delete</span><br><span class="line"><span class="comment">### remove the dev, all has been imported</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## issue log</span></span><br><span class="line">$ lsscsi -git  | grep sg118</span><br><span class="line">[17:0:116:0] disk    sas:0x5000c500a65b61f1          /dev/sddh  35000c500a65b61f3  /dev/sg118</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: Attached scsi generic sg178 <span class="built_in">type</span> 0</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Enabling DIF Type 2 protection</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] 19134414848 512-byte logical blocks: (9.79 TB/8.91 TiB)</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] 4096-byte physical blocks</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Write Protect is off</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Mode Sense: d7 00 10 08</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Write cache: enabled, <span class="built_in">read</span> cache: enabled, supports DPO and FUA</span><br><span class="line">[Sun Sep 23 11:03:43 2018]  sdfm: sdfm1 sdfm9</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Attached SCSI disk</span><br><span class="line">[Sun Sep 23 11:03:44 2018] scsi 17:0:177:0: Attached scsi generic sg179 <span class="built_in">type</span> 3</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: attempting task abort! scmd(ffff88100abf2f40)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: [sg118] CDB: Inquiry 12 01 dc 01 52 00</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: handle(0x0087), sas_address(0x5000c500a65b61f1), phy(34)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: enclosure_logical_id(0x50050cc11ac01572), slot(56)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: task abort: SUCCESS scmd(ffff88100abf2f40)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: attempting task abort! scmd(ffff880fe9784ec0)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: [sg118] CDB: Inquiry 12 01 dc 01 52 00</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: handle(0x0087), sas_address(0x5000c500a65b61f1), phy(34)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: enclosure_logical_id(0x50050cc11ac01572), slot(56)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: task abort: SUCCESS scmd(ffff880fe9784ec0)</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3155 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3155   3152 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267ce0 0000000000000082 ffff881025d0cf10 ffff881011267fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267fd8 ffff881011267fd8 ffff881025d0cf10 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881025d0cf10 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d0744&gt;] spa_import+0x54/0x730 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc062cfc2&gt;] ? nvlist_lookup_common.part.71+0xa2/0xb0 [znvpair]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090bc37&gt;] zfs_ioc_pool_import+0x147/0x160 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3157 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3157   3153 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011283c60 0000000000000086 ffff88103b1fdee0 ffff881011283fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011283fd8 ffff881011283fd8 ffff88103b1fdee0 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff88103b1fdee0 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cee41&gt;] spa_open_common+0x61/0x4d0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff8118cb26&gt;] ? __alloc_pages_nodemask+0x176/0x420</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cf334&gt;] spa_get_stats+0x54/0x550 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc06021a0&gt;] ? spl_kmem_zalloc+0xc0/0x170 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff811ddbee&gt;] ? __kmalloc_node+0x27e/0x2b0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090c019&gt;] zfs_ioc_pool_stats+0x39/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3159 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3159   3156 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88101128bc60 0000000000000086 ffff88102fb3af70 ffff88101128bfd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88101128bfd8 ffff88101128bfd8 ffff88102fb3af70 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff88102fb3af70 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cee41&gt;] spa_open_common+0x61/0x4d0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff8118cb26&gt;] ? __alloc_pages_nodemask+0x176/0x420</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cf334&gt;] spa_get_stats+0x54/0x550 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc06021a0&gt;] ? spl_kmem_zalloc+0xc0/0x170 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff811ddbee&gt;] ? __kmalloc_node+0x27e/0x2b0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090c019&gt;] zfs_ioc_pool_stats+0x39/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task z_zvol:5492 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] z_zvol          D ffffffffc0ade588     0  5492      2 0x00000080</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88100ef0f9a0 0000000000000046 ffff881033e46eb0 ffff88100ef0ffd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88100ef0ffd8 ffff88100ef0ffd8 ffff881033e46eb0 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881033e46eb0 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cee41&gt;] spa_open_common+0x61/0x4d0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08761d6&gt;] ? dbuf_rele+0x36/0x40 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cf2d3&gt;] spa_open+0x13/0x20 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08ad4f3&gt;] dsl_pool_hold+0x33/0x80 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0885935&gt;] dmu_objset_hold+0x35/0xd0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08af0d4&gt;] dsl_prop_get+0x44/0xa0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08af14e&gt;] dsl_prop_get_integer+0x1e/0x20 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc094269b&gt;] zvol_create_minors_cb+0x3b/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08c0880&gt;] ? rrw_exit+0x60/0x170 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0884692&gt;] dmu_objset_find_impl+0x112/0x430 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc088476e&gt;] dmu_objset_find_impl+0x1ee/0x430 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0884e58&gt;] dmu_objset_find+0x58/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0944a7b&gt;] zvol_task_cb+0x52b/0x5a0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a902d&gt;] ? __schedule+0x39d/0x8b0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0604ed6&gt;] taskq_thread+0x246/0x470 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810c4820&gt;] ? wake_up_state+0x20/0x20</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0604c90&gt;] ? taskq_thread_spawn+0x60/0x60 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b099f&gt;] kthread+0xcf/0xe0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b4fd8&gt;] ret_from_fork+0x58/0x90</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task spa_async:6766 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] spa_async       D ffffffffc0ade588     0  6766      2 0x00000080</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff880fd3d3bdc0 0000000000000046 ffff881033689fa0 ffff880fd3d3bfd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff880fd3d3bfd8 ffff880fd3d3bfd8 ffff881033689fa0 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881033689fa0 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2d00&gt;] ? spa_vdev_resilver_done+0x140/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2f44&gt;] spa_async_thread+0x244/0x300 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff811dee33&gt;] ? kfree+0x103/0x140</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2d00&gt;] ? spa_vdev_resilver_done+0x140/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0603fa1&gt;] thread_generic_wrapper+0x71/0x80 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0603f30&gt;] ? __thread_exit+0x20/0x20 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b099f&gt;] kthread+0xcf/0xe0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b4fd8&gt;] ret_from_fork+0x58/0x90</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task z_zvol:6806 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] z_zvol          D ffffffffc0ade588     0  6806      2 0x00000080</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff8810104cb9a0 0000000000000046 ffff881010b0af70 ffff8810104cbfd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff8810104cbfd8 ffff8810104cbfd8 ffff881010b0af70 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881010b0af70 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cee41&gt;] spa_open_common+0x61/0x4d0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08761d6&gt;] ? dbuf_rele+0x36/0x40 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cf2d3&gt;] spa_open+0x13/0x20 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08ad4f3&gt;] dsl_pool_hold+0x33/0x80 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0885935&gt;] dmu_objset_hold+0x35/0xd0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08af0d4&gt;] dsl_prop_get+0x44/0xa0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08af14e&gt;] dsl_prop_get_integer+0x1e/0x20 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc094269b&gt;] zvol_create_minors_cb+0x3b/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08c0880&gt;] ? rrw_exit+0x60/0x170 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0884692&gt;] dmu_objset_find_impl+0x112/0x430 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc088476e&gt;] dmu_objset_find_impl+0x1ee/0x430 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0942660&gt;] ? zvol_prefetch_minors_impl+0x90/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0884e58&gt;] dmu_objset_find+0x58/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0944a7b&gt;] zvol_task_cb+0x52b/0x5a0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a902d&gt;] ? __schedule+0x39d/0x8b0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0604ed6&gt;] taskq_thread+0x246/0x470 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810c4820&gt;] ? wake_up_state+0x20/0x20</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0604c90&gt;] ? taskq_thread_spawn+0x60/0x60 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b099f&gt;] kthread+0xcf/0xe0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b4fd8&gt;] ret_from_fork+0x58/0x90</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task spa_async:7267 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">"echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs"</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] spa_async       D ffffffffc0ade588     0  7267      2 0x00000080</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88107d987dc0 0000000000000046 ffff8810113fcf10 ffff88107d987fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88107d987fd8 ffff88107d987fd8 ffff8810113fcf10 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff8810113fcf10 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2d00&gt;] ? spa_vdev_resilver_done+0x140/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2f44&gt;] spa_async_thread+0x244/0x300 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff811dee33&gt;] ? kfree+0x103/0x140</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d2d00&gt;] ? spa_vdev_resilver_done+0x140/0x140 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0603fa1&gt;] thread_generic_wrapper+0x71/0x80 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0603f30&gt;] ? __thread_exit+0x20/0x20 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b099f&gt;] kthread+0xcf/0xe0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b4fd8&gt;] ret_from_fork+0x58/0x90</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff810b08d0&gt;] ? insert_kthread_work+0x40/0x40</span><br></pre></td></tr></table></figure>


<h3 id="OpenZFS-multihost-writes-failed-about-Invalid-DWORD-count"><a href="#OpenZFS-multihost-writes-failed-about-Invalid-DWORD-count" class="headerlink" title="OpenZFS multihost writes failed (about Invalid DWORD count)"></a>OpenZFS multihost writes failed (about Invalid DWORD count)</h3><pre><code>Invalid DWORD count = 124
Loss of DWORD synchronization = 31</code></pre><p><a href="//www.c0t0d0s0.org/archives/7601-Check-both-sides-or-Errors-on-the-SAN.html" target="_blank" rel="noopener">The interesting part is the highlighted one. A massive increase in invalid tx word count. When you see an massively increased counter here, your HBA just received rubbish from the storage. I never saw a different reason for this than a problem between the interface to the optics on the HBA and the interface to the optics on the switch ranging from not properly seated transceivers to blatant cases of ignoring the minimum bending radius of the fibre optic cables.</a></p>
<p>Suggestions to the customer based on the rule “check cheapest solution first”:<br>reseat cables<br>reseat transceivers<br>use a new cable<br>use new transceivers</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">status: The pool is suspended because multihost writes failed or were delayed;</span><br><span class="line">	another system could import the pool undetected.</span><br><span class="line">action: Make sure the pool<span class="string">'s devices are connected, then reboot your system and</span></span><br><span class="line"><span class="string">	import the pool.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: attempting task abort! scmd(ffff9285b2736a00)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#10 CDB: Read(16) 88 00 00 00 00 02 45 fb ce 48 00 00 00 81 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10) </span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosure level(0x0000), connector name(     )</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: task abort: SUCCESS scmd(ffff9285b2736a00)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#10 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#10 CDB: Read(16) 88 00 00 00 00 02 45 fb ce 48 00 00 00 81 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] blk_update_request: I/O error, dev sdff, sector 9764064840</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: attempting task abort! scmd(ffff9285af232f40)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#6 CDB: Read(16) 88 00 00 00 00 02 45 fb cd 2f 00 00 00 ed 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10) </span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosure level(0x0000), connector name(     )</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: task abort: SUCCESS scmd(ffff9285af232f40)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#6 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#6 CDB: Read(16) 88 00 00 00 00 02 45 fb cd 2f 00 00 00 ed 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] blk_update_request: I/O error, dev sdff, sector 9764064559</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: attempting task abort! scmd(ffff926d3ab36d80)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#5 CDB: Read(16) 88 00 00 00 00 02 45 fb cc 41 00 00 00 d8 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10) </span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosure level(0x0000), connector name(     )</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: task abort: SUCCESS scmd(ffff926d3ab36d80)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#5 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#5 CDB: Read(16) 88 00 00 00 00 02 45 fb cc 41 00 00 00 d8 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] blk_update_request: I/O error, dev sdff, sector 9764064321</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: attempting task abort! scmd(ffff9267c58cea00)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#4 CDB: Read(16) 88 00 00 00 00 02 45 fb cb 69 00 00 00 97 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10) </span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] scsi target13:0:10: enclosure level(0x0000), connector name(     )</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: task abort: SUCCESS scmd(ffff9267c58cea00)</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#4 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] sd 13:0:10:0: [sdff] tag#4 CDB: Read(16) 88 00 00 00 00 02 45 fb cb 69 00 00 00 97 00 00</span></span><br><span class="line"><span class="string">[Tue May 14 15:12:58 2019] blk_update_request: I/O error, dev sdff, sector 9764064105</span></span><br><span class="line"><span class="string">[Tue May 14 15:14:14 2019] WARNING: MMP writes to pool '</span>ost_64<span class="string">' have not succeeded in over 20s; suspending pool</span></span><br><span class="line"><span class="string">[Tue May 14 15:14:14 2019] WARNING: Pool '</span>ost_64<span class="string">' has encountered an uncorrectable I/O failure and has been suspended.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">sdff smart info:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Error counter log:</span></span><br><span class="line"><span class="string">           Errors Corrected by           Total   Correction     Gigabytes    Total</span></span><br><span class="line"><span class="string">               ECC          rereads/    errors   algorithm      processed    uncorrected</span></span><br><span class="line"><span class="string">           fast | delayed   rewrites  corrected  invocations   [10^9 bytes]  errors</span></span><br><span class="line"><span class="string">read:          0   493902         0    493902   12129373      97757.816           0</span></span><br><span class="line"><span class="string">write:         0        1         0         1     654716      29124.879           0</span></span><br><span class="line"><span class="string">verify:        0        0         0         0    7052436          0.000           0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Non-medium error count:        0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">No self-tests have been logged</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Background scan results log</span></span><br><span class="line"><span class="string">  Status: scan is active</span></span><br><span class="line"><span class="string">    Accumulated power on time, hours:minutes 17258:47 [1035527 minutes]</span></span><br><span class="line"><span class="string">    Number of background scans performed: 63,  scan progress: 64.66%</span></span><br><span class="line"><span class="string">    Number of background medium scans performed: 63</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Protocol Specific port log page for SAS SSP</span></span><br><span class="line"><span class="string">relative target port id = 1</span></span><br><span class="line"><span class="string">  generation code = 49</span></span><br><span class="line"><span class="string">  number of phys = 1</span></span><br><span class="line"><span class="string">  phy identifier = 0</span></span><br><span class="line"><span class="string">    attached device type: expander device</span></span><br><span class="line"><span class="string">    attached reason: hard reset</span></span><br><span class="line"><span class="string">    reason: unknown</span></span><br><span class="line"><span class="string">    negotiated logical link rate: reserved [11]</span></span><br><span class="line"><span class="string">    attached initiator port: ssp=0 stp=0 smp=0</span></span><br><span class="line"><span class="string">    attached target port: ssp=0 stp=0 smp=1</span></span><br><span class="line"><span class="string">    SAS address = 0x5000cca251b4462d</span></span><br><span class="line"><span class="string">    attached SAS address = 0x500304800928dc3f</span></span><br><span class="line"><span class="string">    attached phy identifier = 37</span></span><br><span class="line"><span class="string">    Invalid DWORD count = 56</span></span><br><span class="line"><span class="string">    Running disparity error count = 0</span></span><br><span class="line"><span class="string">    Loss of DWORD synchronization = 14</span></span><br><span class="line"><span class="string">    Phy reset problem = 0</span></span><br><span class="line"><span class="string">    Phy event descriptors:</span></span><br><span class="line"><span class="string">     Invalid word count: 56</span></span><br><span class="line"><span class="string">     Running disparity error count: 0</span></span><br><span class="line"><span class="string">     Loss of dword synchronization count: 14</span></span><br><span class="line"><span class="string">     Phy reset problem count: 0</span></span><br><span class="line"><span class="string">relative target port id = 2</span></span><br><span class="line"><span class="string">  generation code = 49</span></span><br><span class="line"><span class="string">  number of phys = 1</span></span><br><span class="line"><span class="string">  phy identifier = 1</span></span><br><span class="line"><span class="string">    attached device type: expander device</span></span><br><span class="line"><span class="string">    attached reason: hard reset</span></span><br><span class="line"><span class="string">    reason: unknown</span></span><br><span class="line"><span class="string">    negotiated logical link rate: reserved [11]</span></span><br><span class="line"><span class="string">    attached initiator port: ssp=0 stp=0 smp=0</span></span><br><span class="line"><span class="string">    attached target port: ssp=0 stp=0 smp=1</span></span><br><span class="line"><span class="string">    SAS address = 0x5000cca251b4462e</span></span><br><span class="line"><span class="string">    attached SAS address = 0x500304800928dd3f</span></span><br><span class="line"><span class="string">    attached phy identifier = 37</span></span><br><span class="line"><span class="string">    Invalid DWORD count = 124</span></span><br><span class="line"><span class="string">    Running disparity error count = 0</span></span><br><span class="line"><span class="string">    Loss of DWORD synchronization = 31</span></span><br><span class="line"><span class="string">    Phy reset problem = 0</span></span><br><span class="line"><span class="string">    Phy event descriptors:</span></span><br><span class="line"><span class="string">     Invalid word count: 124</span></span><br><span class="line"><span class="string">     Running disparity error count: 0</span></span><br><span class="line"><span class="string">     Loss of dword synchronization count: 31</span></span><br><span class="line"><span class="string">     Phy reset problem count: 0</span></span><br></pre></td></tr></table></figure>


<h3 id="For-Low-memroy-server"><a href="#For-Low-memroy-server" class="headerlink" title="For Low memroy server"></a>For Low memroy server</h3><p>I attacht 168 HDDs with 32GB memroy server</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kernel: mpt2sas1: iomem(0x0000000091c40000), mapped(0xffffc90003760000), size(65536)</span><br><span class="line">kernel: mpt2sas1: ioport(0x0000000000002000), size(256)</span><br><span class="line">kernel: mpt2sas1: Allocated physical memory: size(17329 kB)</span><br><span class="line">kernel: mpt2sas1: Current Controller Queue Depth(9979), Max Controller Queue Depth(10240)</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">kernel: Out of memory: Kill process 7795 (collectl) score 0 or sacrifice child</span><br><span class="line">kernel: Killed process 7795 (collectl) total-vm:177612kB, anon-rss:21400kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">kernel: collectl: page allocation failure: order:0, mode:0x2015a</span><br><span class="line">...</span><br><span class="line">kernel: Out of memory: Kill process 4013 (tuned) score 0 or sacrifice child</span><br><span class="line">kernel: Killed process 4013 (tuned) total-vm:573848kB, anon-rss:13472kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">systemd: tuned.service: main process exited, code=killed, status=9/KILL</span><br><span class="line">systemd: Unit tuned.service entered failed state</span><br><span class="line"></span><br><span class="line">CONFIG_SCSI_MPT2SAS_MAX_SGE: This option allows you to specify the</span><br><span class="line"> maximum number of scatter-gather entries per I/O. The driver default</span><br><span class="line"> is 128, <span class="built_in">which</span> matches MAX_HW_SEGMENTS. However, it may decreased</span><br><span class="line"> down to 16. Decreasing this parameter will reduce memory requirements</span><br><span class="line"> on a per controller instance.</span><br><span class="line"></span><br><span class="line">$ cat /etc/modprobe.d/mpt2sas.conf</span><br><span class="line">options mpt2sas max_msix_vectors=4</span><br><span class="line">options mpt2sas command_retry_count=32 <span class="comment">#if you have long timeout and much more retry count will cause the issue HDD retry many times</span></span><br><span class="line"></span><br><span class="line">options mpt2sas max_sgl_entries=32 <span class="comment">#for low memory system</span></span><br><span class="line">options mpt2sas max_queue_depth=300</span><br><span class="line"></span><br><span class="line">options mpt3sas max_queue_depth=512 <span class="comment">#</span></span><br><span class="line">options mpt3sas max_sgl_entries=256 <span class="comment"># enough memroy</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 5000 &gt; /sys/module/zfs/parameters/zfs_multihost_interval</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> /sys/class/block/*/device/timeout; <span class="keyword">do</span> <span class="built_in">echo</span> 90 &gt; <span class="variable">$i</span>; <span class="keyword">done</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> /sys/block/*/device/queue_depth; <span class="keyword">do</span> <span class="built_in">echo</span> 254 &gt; <span class="variable">$i</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="zpool-could-not-be-import-because-single-HDD-failure-in-raidz3"><a href="#zpool-could-not-be-import-because-single-HDD-failure-in-raidz3" class="headerlink" title="zpool could not be import because single HDD failure in raidz3"></a>zpool could not be import because single HDD failure in raidz3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zpool import -f -o  cachefile&#x3D;none -o readonly&#x3D;on ost_61</span><br><span class="line">cannot import &#39;ost_61&#39;: I&#x2F;O error Destroy and re-create the pool from a backup source.</span><br><span class="line"></span><br><span class="line">------------------------oh shit--------------------</span><br><span class="line"></span><br><span class="line">zpool import show </span><br><span class="line"></span><br><span class="line">     pool: ost_61</span><br><span class="line">     id: 15498746923132605816</span><br><span class="line">  state: FAULTED</span><br><span class="line"> status: The pool metadata is corrupted.</span><br><span class="line"> action: The pool cannot be imported due to damaged devices or data.</span><br><span class="line">        The pool may be active on another system, but can be imported using</span><br><span class="line">        the &#39;-f&#39; flag.</span><br><span class="line">   see: http:&#x2F;&#x2F;zfsonlinux.org&#x2F;msg&#x2F;ZFS-8000-72</span><br><span class="line"> config:        ost_61                      FAULTED  corrupted data</span><br><span class="line">          raidz3-0                  ONLINE</span><br><span class="line">            sddd                    ONLINE</span><br><span class="line">            sdde                    ONLINE</span><br><span class="line">            sddf                    ONLINE</span><br><span class="line">            sddg                    ONLINE</span><br><span class="line">            sddh                    ONLINE</span><br><span class="line">            sddi                    ONLINE</span><br><span class="line">            scsi-35000c500a66c86f7  ONLINE</span><br><span class="line">            scsi-35000c500a675b907  ONLINE</span><br><span class="line">            scsi-35000c500a665afe7  ONLINE</span><br><span class="line">            scsi-35000c500a670c7a3  ONLINE</span><br><span class="line">            scsi-35000c500a66d665f  ONLINE</span><br><span class="line">            scsi-35000c500a65a86b7  ONLINE</span><br><span class="line">            scsi-35000c500a664a617  ONLINE</span><br><span class="line">            scsi-35000c500a665b353  ONLINE</span><br><span class="line">            scsi-35000c500a670c9fb  ONLINE</span><br><span class="line">            scsi-35000c500a670b10f  ONLINE</span><br><span class="line">            scsi-35000c500a65a9027  ONLINE</span><br><span class="line">            scsi-35000c500a65a02af  ONLINE</span><br><span class="line">            scsi-35000c500a670b213  ONLINE</span><br><span class="line">            scsi-35000c500a670b157  ONLINE</span><br><span class="line">            scsi-35000c500a665b1b3  ONLINE</span><br><span class="line">            scsi-35000c500a670c5fb  ONLINE</span><br><span class="line"></span><br><span class="line">$ zpool import -f -o  cachefile&#x3D;none -F -n ost_61</span><br><span class="line">$ echo $?</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ zdb -e ost_61 </span><br><span class="line">Configuration for import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 15498746923132605816</span><br><span class="line">        name: &#39;ost_61&#39;</span><br><span class="line">        state: 0</span><br><span class="line">        hostid: 3219179115</span><br><span class="line">        hostname: &#39;cngb-oss-c25-4.cngb.sz.hpc&#39;</span><br><span class="line">        vdev_tree:</span><br><span class="line">            type: &#39;root&#39;</span><br><span class="line">            id: 0</span><br><span class="line">            guid: 15498746923132605816</span><br><span class="line">            children[0]:</span><br><span class="line">                type: &#39;raidz&#39;</span><br><span class="line">                id: 0</span><br><span class="line">                guid: 18120440563767675975</span><br><span class="line">                nparity: 3</span><br><span class="line">                metaslab_array: 65</span><br><span class="line">                metaslab_shift: 40</span><br><span class="line">                ashift: 9</span><br><span class="line">                asize: 215529714352128</span><br><span class="line">                is_log: 0</span><br><span class="line">                create_txg: 4</span><br><span class="line">                children[0]:</span><br><span class="line">                    type: &#39;disk&#39;</span><br><span class="line">                    id: 0</span><br><span class="line">                    guid: 691489041564037318</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 277</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: &#39;&#x2F;dev&#x2F;sddd1&#39;</span><br><span class="line">                    devid: &#39;scsi-35000c500a670b7bf-part1&#39;</span><br><span class="line">                    phys_path: &#39;pci-0000:b3:00.0-sas-0x5000c500a670b7bd-lun-0&#39;</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"> children[20]:</span><br><span class="line">                    type: &#39;disk&#39;</span><br><span class="line">                    id: 20</span><br><span class="line">                    guid: 3278057294213455851</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 257</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: &#39;&#x2F;dev&#x2F;disk&#x2F;by-id&#x2F;scsi-35000c500a665b1b3-part1&#39;</span><br><span class="line">                    devid: &#39;scsi-35000c500a665b1b3-part1&#39;</span><br><span class="line">                    phys_path: &#39;pci-0000:b3:00.0-sas-0x5000c500a665b1b1-lun-0&#39;</span><br><span class="line">                children[21]:</span><br><span class="line">                    type: &#39;disk&#39;</span><br><span class="line">                    id: 21</span><br><span class="line">                    guid: 10749955888611927037</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 256</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: &#39;&#x2F;dev&#x2F;disk&#x2F;by-id&#x2F;scsi-35000c500a670c5fb-part1&#39;</span><br><span class="line">                    devid: &#39;scsi-35000c500a670c5fb-part1&#39;</span><br><span class="line">                    phys_path: &#39;pci-0000:b3:00.0-sas-0x5000c500a670c5f9-lun-0&#39;</span><br><span class="line">        rewind-policy:</span><br><span class="line">            rewind-request-txg: 18446744073709551615</span><br><span class="line">            rewind-request: 2</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br></pre></td></tr></table></figure>


<h4 id="I-guess-there-are-4-x-HDDs-could-read-zfs-metadata-in-this-zpool"><a href="#I-guess-there-are-4-x-HDDs-could-read-zfs-metadata-in-this-zpool" class="headerlink" title="I guess there are 4 x HDDs could read zfs metadata in this zpool"></a>I guess there are 4 x HDDs could read zfs metadata in this zpool</h4><p>Try it one by one</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ zdb -e ost_61 -d scsi-35000c500a665afe7</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 509M, 284 objects</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type</span><br><span class="line">         0    2   128K    16K   736K     512   592K   23.99  DMU dnode</span><br><span class="line"></span><br><span class="line">$ zdb -e ost_61 -d sddi</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error </span><br><span class="line"></span><br><span class="line">$ zdb -e ost_61 -d -t 1274825</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset ost_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">----sddh--finish-----</span><br><span class="line">----sddi-----</span><br><span class="line">zdb -e ost_61 -d -t 1414018</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414019</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414020</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414021</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414022</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414023</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">zdb -e ost_61 -d -t 1414025</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">zdb -e ost_61 -d -t 1274813</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset ost_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e ost_61 -d -t 1274814</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset ost_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e ost_61 -d -t 1414007</span><br><span class="line">zdb: can&#39;t open &#39;ost_61&#39;: Input&#x2F;output error</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">----sddi--finish-----</span><br><span class="line">----scsi-35000c500a66c86f7-----</span><br><span class="line">zdb -e ost_61 -d -t 1274826</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset ost_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e ost_61 -d -t 1274827</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset ost_61&#x2F;ost_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset ost_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br></pre></td></tr></table></figure>
<p>Only sddi show input/output error</p>
<h4 id="Ban-the-sddi"><a href="#Ban-the-sddi" class="headerlink" title="Ban the sddi"></a>Ban the sddi</h4><p>This is the <a href="https://github.com/zfsonlinux/zfs/issues/7184" target="_blank" rel="noopener">zfs bug</a>, the easy way is use zdb -lu /dev/sd(all zpool devices), you could compare txg number, single device has the wrong txg number with the others. </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@cngb-oss-c25-3.cngb.sz.hpc &#x2F;tmp&#x2F;zdb]# diff sdcu sdcv # it &#39;s ok</span><br><span class="line">[root@cngb-oss-c25-3.cngb.sz.hpc &#x2F;tmp&#x2F;zdb]# diff sdcu sddi # found the wrong device</span><br><span class="line">25,37c25,37</span><br><span class="line">&lt; 	txg &#x3D; 2674112</span><br><span class="line">&lt; 	txg &#x3D; 2673986</span><br><span class="line">&lt; 	txg &#x3D; 2674114</span><br><span class="line">&lt; 	txg &#x3D; 2674115</span><br><span class="line">&lt; 	txg &#x3D; 2674116</span><br><span class="line">&lt; 	txg &#x3D; 2674117</span><br><span class="line">&lt; 	txg &#x3D; 2674118</span><br><span class="line">&lt; 	txg &#x3D; 2674119</span><br><span class="line">&lt; 	txg &#x3D; 2674120</span><br><span class="line">&lt; 	txg &#x3D; 2674121</span><br><span class="line">&lt; 	txg &#x3D; 2674122</span><br><span class="line">&lt; 	txg &#x3D; 2674123</span><br><span class="line">&lt; 	txg &#x3D; 2674124</span><br><span class="line">---</span><br><span class="line">&gt; 	txg &#x3D; 7727315</span><br><span class="line">&gt; 	txg &#x3D; 7727443</span><br><span class="line">&gt; 	txg &#x3D; 7727317</span><br><span class="line">&gt; 	txg &#x3D; 7727318</span><br><span class="line">&gt; 	txg &#x3D; 7727319</span><br><span class="line">&gt; 	txg &#x3D; 7727320</span><br><span class="line">&gt; 	txg &#x3D; 7727321</span><br><span class="line">&gt; 	txg &#x3D; 7727322</span><br><span class="line">&gt; 	txg &#x3D; 7727323</span><br><span class="line">&gt; 	txg &#x3D; 7727324</span><br><span class="line">&gt; 	txg &#x3D; 7727325</span><br><span class="line">&gt; 	txg &#x3D; 7727453</span><br><span class="line"></span><br><span class="line">echo 1 &gt; &#x2F;sys&#x2F;class&#x2F;block&#x2F;sddi&#x2F;device&#x2F;delete</span><br></pre></td></tr></table></figure>
<p>After ban the sddi, the zpool could be imported……wipe the sweat, wipe the sweat……<br>The issue because this device (sddi) has been in two diff zpool(server A and server B) at the same time</p>
<h3 id="Single-HDD-hang-cause-LSI-HBA-reset-self"><a href="#Single-HDD-hang-cause-LSI-HBA-reset-self" class="headerlink" title="Single HDD hang cause LSI HBA reset self"></a>Single HDD hang cause LSI HBA reset self</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Mar 13 17:47:16 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">Mar 13 17:47:16 your_hostname kernel: mf:<span class="comment">#012#011000000a3 00000000 00000000 00000000 00000000 00000018 00000000 0000012c #012#01100000000 00000006 00000000 00000000 00000000 00000000 00000000 02000000 #012#01100000012 0000002c 00000000 00000000 00000000 00000000 00000000 00000000</span></span><br><span class="line">Mar 13 17:47:16 your_hostname kernel: mpt3sas_cm0: issue target reset: handle = (0x00a3)</span><br><span class="line">Mar 13 17:47:44 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">Mar 13 17:47:44 your_hostname kernel: mf:<span class="comment">#012#011000000a3 00000000 00000000 00000000 00000000 00000018 00000000 0000012c #012#01100000000 00000006 00000000 00000000 00000000 00000000 00000000 02000000 #012#01100000012 0000002c 00000000 00000000 00000000 00000000 00000000 00000000</span></span><br><span class="line">Mar 13 17:47:44 your_hostname kernel: mpt3sas_cm0: issue target reset: handle = (0x00a3)</span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f4c57e31340)</span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: sd 17:0:144:0: [sg145] tag<span class="comment">#13 CDB: Test Unit Ready 00 00 00 00 00 00</span></span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: scsi target17:0:144: _scsih_tm_display_info: handle(0x00a3), sas_address(0x5000c500a6753d51), phy(20)</span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: scsi target17:0:144: enclosurelogical id(0x50050cc11ac01694), slot(1)</span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: scsi target17:0:144: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Mar 13 17:47:52 your_hostname kernel: sd 17:0:144:0: task abort: SUCCESS scmd(ffff9f4c57e31340)</span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f4e947adb00)</span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: sd 17:0:144:0: [sg145] tag<span class="comment">#172 CDB: Log Sense 4d 00 6f 00 00 00 00 10 00 00</span></span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: scsi target17:0:144: _scsih_tm_display_info: handle(0x00a3), sas_address(0x5000c500a6753d51), phy(20)</span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: scsi target17:0:144: enclosurelogical id(0x50050cc11ac01694), slot(1)</span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: scsi target17:0:144: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Mar 13 17:47:58 your_hostname kernel: sd 17:0:144:0: task abort: SUCCESS scmd(ffff9f4e947adb00)</span><br><span class="line">.......</span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f4e21ff0e00)</span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: sd 17:0:144:0: tag<span class="comment">#0 CDB: Read capacity(16) 9e 10 00 00 00 00 00 00 00 00 00 00 00 20 00 00</span></span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: scsi target17:0:144: _scsih_tm_display_info: handle(0x00a3), sas_address(0x5000c500a6753d51), phy(20)</span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: scsi target17:0:144: enclosurelogical id(0x50050cc11ac01694), slot(1)</span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: scsi target17:0:144: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Mar 13 17:51:40 your_hostname kernel: sd 17:0:144:0: task abort: SUCCESS scmd(ffff9f4e21ff0e00)</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f480939bf00)</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: sd 17:0:144:0: tag<span class="comment">#0 CDB: Read capacity(16) 9e 10 00 00 00 00 00 00 00 00 00 00 00 20 00 00</span></span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: scsi target17:0:144: _scsih_tm_display_info: handle(0x00a3), sas_address(0x5000c500a6753d51), phy(20)</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: scsi target17:0:144: enclosurelogical id(0x50050cc11ac01694), slot(1)</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: scsi target17:0:144: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: sd 17:0:144:0: task abort: SUCCESS scmd(ffff9f480939bf00)</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: sd 17:0:144:0: [sdeh] Read Capacity(16) failed: Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span><br><span class="line">Mar 13 17:52:11 your_hostname kernel: sd 17:0:144:0: [sdeh] Sense not available.</span><br><span class="line">Mar 13 17:52:42 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f491c216c80)</span><br><span class="line">Mar 13 17:52:42 your_hostname kernel: sd 17:0:144:0: tag<span class="comment">#0 CDB: Read Capacity(10) 25 00 00 00 00 00 00 00 00 00</span></span><br><span class="line">......</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: sd 17:0:144:0: attempting task abort! scmd(ffff9f43a0ff6580)</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: sd 17:0:144:0: [sg145] tag<span class="comment">#33 CDB: Test Unit Ready 00 00 00 00 00 00</span></span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: scsi target17:0:144: _scsih_tm_display_info: handle(0x00a3), sas_address(0x5000c500a6753d51), phy(20)</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: scsi target17:0:144: enclosurelogical id(0x50050cc11ac01694), slot(1)</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: scsi target17:0:144: enclosure level(0x0000), connector name( 1   )</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: sd 17:0:144:0: task abort: SUCCESS scmd(ffff9f43a0ff6580)</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: mpt3sas_cm0: attempting host reset! scmd(ffff9f43a0ff6580)</span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: sd 17:0:144:0: [sg145] tag<span class="comment">#33 CDB: Inquiry 12 01 83 00 f0 00</span></span><br><span class="line">Mar 13 19:26:20 your_hostname kernel: mpt3sas_cm0: sending diag reset !!</span><br><span class="line">Mar 13 19:26:21 your_hostname kernel: mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: IOC Number : 0</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: CurrentHostPageSize is 0: Setting default host page size to 4k</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: FW Package Version(16.17.00.03)</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: LSISAS3008: FWVersion(16.00.04.00), ChipRevision(0x02), BiosVersion(18.00.00.00)</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: Dell 12Gbps SAS HBA</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">Mar 13 19:26:22 your_hostname kernel: mpt3sas_cm0: sending port <span class="built_in">enable</span> !!</span><br><span class="line">Mar 13 19:26:24 your_hostname kernel: WARNING: MMP writes to pool <span class="string">'ost_6'</span> have not succeeded <span class="keyword">in</span> over 25s; suspending pool</span><br><span class="line">Mar 13 19:26:24 your_hostname kernel: WARNING: Pool <span class="string">'ost_6'</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">line number, message</span><br><span class="line">27567:Mar 13 17:47:16 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">27570:Mar 13 17:47:44 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">28676:Mar 13 17:49:36 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">29851:Mar 13 17:58:36 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31211:Mar 13 17:59:51 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31817:Mar 13 18:00:26 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31852:Mar 13 18:01:47 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31855:Mar 13 18:02:13 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31878:Mar 13 18:03:33 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31881:Mar 13 18:03:57 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31910:Mar 13 18:05:13 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31913:Mar 13 18:05:37 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31931:Mar 13 18:06:48 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31940:Mar 13 18:07:17 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31961:Mar 13 18:08:28 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31964:Mar 13 18:08:52 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">31996:Mar 13 18:10:12 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32012:Mar 13 18:10:47 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32057:Mar 13 18:12:07 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32060:Mar 13 18:12:32 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32101:Mar 13 18:13:53 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32116:Mar 13 18:14:37 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32162:Mar 13 18:15:57 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32165:Mar 13 18:16:22 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32182:Mar 13 18:17:33 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32191:Mar 13 18:18:02 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32218:Mar 13 18:19:16 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32233:Mar 13 18:19:57 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32250:Mar 13 18:20:57 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32265:Mar 13 18:21:55 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32280:Mar 13 18:22:42 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32309:Mar 13 18:23:40 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32324:Mar 13 18:24:16 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32339:Mar 13 18:25:28 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32355:Mar 13 18:26:15 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32370:Mar 13 18:27:02 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32387:Mar 13 18:27:49 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32402:Mar 13 18:28:40 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32417:Mar 13 18:29:29 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32435:Mar 13 18:30:13 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32453:Mar 13 18:31:07 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32468:Mar 13 18:32:00 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32495:Mar 13 18:33:27 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32498:Mar 13 18:34:18 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32525:Mar 13 18:35:34 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32543:Mar 13 18:36:46 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32570:Mar 13 18:38:09 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32585:Mar 13 18:38:58 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32600:Mar 13 18:39:57 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32621:Mar 13 18:41:19 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32642:Mar 13 18:42:31 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32657:Mar 13 18:43:26 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32672:Mar 13 18:44:39 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32690:Mar 13 18:45:51 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32705:Mar 13 18:46:58 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32735:Mar 13 18:48:03 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32752:Mar 13 18:48:52 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32770:Mar 13 18:50:23 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32794:Mar 13 18:51:15 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32809:Mar 13 18:52:25 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32818:Mar 13 18:53:03 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32857:Mar 13 18:55:09 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32863:Mar 13 18:55:52 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32884:Mar 13 18:57:12 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32887:Mar 13 18:58:01 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32914:Mar 13 18:59:44 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32920:Mar 13 19:00:21 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32965:Mar 13 19:01:55 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">32968:Mar 13 19:02:33 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33007:Mar 13 19:04:28 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33010:Mar 13 19:05:14 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33028:Mar 13 19:06:14 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33055:Mar 13 19:07:24 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33070:Mar 13 19:08:35 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33091:Mar 13 19:09:52 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33097:Mar 13 19:10:32 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33115:Mar 13 19:11:45 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33130:Mar 13 19:12:45 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33157:Mar 13 19:13:59 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33160:Mar 13 19:14:39 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33190:Mar 13 19:16:11 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33205:Mar 13 19:17:05 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33232:Mar 13 19:18:27 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33235:Mar 13 19:19:06 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33253:Mar 13 19:20:24 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33283:Mar 13 19:21:38 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33298:Mar 13 19:22:32 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33313:Mar 13 19:23:34 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33328:Mar 13 19:24:30 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line">33364:Mar 13 19:25:59 your_hostname kernel: mpt3sas_cm0: Command Timeout</span><br><span class="line"></span><br><span class="line">after 90 <span class="built_in">times</span> timeout, the HBA reset itself</span><br><span class="line"></span><br><span class="line">33400:Mar 13 19:26:21 your_hostname kernel: mpt3sas_cm0: diag reset: SUCCESS</span><br><span class="line"></span><br><span class="line">$ zpool status your-pool</span><br><span class="line">  pool: your-pool</span><br><span class="line"> state: ONLINE</span><br><span class="line">  scan: resilvered 510K <span class="keyword">in</span> 0h0m with 0 errors on Sat Jan 26 17:05:55 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                        STATE     READ WRITE CKSUM</span><br><span class="line">        ost_84                      ONLINE       0     0     0</span><br><span class="line">          raidz3-0                  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670c507  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a65b41f7  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a6753d4b  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670ccf3  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670b117  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a675a767  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a6754883  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500957e33f7  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a675a25b  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670c393  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a65a9e53  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a6515413  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a663e253  ONLINE       0     0     0</span><br><span class="line">            scsi-35000cca26c1b754c  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a663b1f7  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670c503  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a6753c5b  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a663b05b  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670b917  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a670c1af  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a65a8e9b  ONLINE       0     0     0</span><br><span class="line">            scsi-35000c500a6756753  ONLINE       0     0     0</span><br><span class="line"></span><br><span class="line">errors: No known data errors</span><br><span class="line"></span><br><span class="line"><span class="comment">## In this raid, only sdap is HGST dev, the others are seagate</span></span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.00    0.00   20.99   47.76    0.00   31.25</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdo               0.00     0.00   10.00   60.00     0.14     5.35   160.51     6.87  105.14   94.95  106.83  14.28  99.95</span><br><span class="line">sdp               0.00     0.00   10.50   41.50     0.15     3.72   152.37     7.65  154.65  151.67  155.41  19.21  99.90</span><br><span class="line">sdr               0.00     0.00   12.50   35.50     0.19     3.16   143.05     7.30  141.66   98.84  156.73  20.81  99.90</span><br><span class="line">sdq               0.00     0.00    9.50   55.00     0.14     4.98   162.49     6.82  111.70   87.21  115.93  15.49  99.90</span><br><span class="line">sdap              0.00     0.00   12.50   71.00     0.19     3.61    93.22     0.23    2.71   14.28    0.67   1.96  16.40</span><br><span class="line">sdad              0.00     0.00   13.50   52.50     0.21     4.69   151.88     6.93  113.20   69.00  124.56  15.14  99.90</span><br><span class="line">sdv               0.00     0.00   10.50   68.50     0.16     5.06   135.30     6.71   78.85  155.86   67.04  12.64  99.85</span><br><span class="line">sdak              0.00     0.00    9.50   85.50     0.19     6.53   144.90     5.53   65.13   99.84   61.27  10.36  98.40</span><br><span class="line">sdal              0.00     0.00   10.50   48.00     0.19     4.27   156.03     6.87  116.79   84.00  123.97  17.07  99.85</span><br><span class="line">sdan              0.00     0.00    9.00   39.50     0.16     3.56   156.88     6.58  125.02   57.61  140.38  20.60  99.90</span><br><span class="line">sdu               0.00     0.00   12.00   61.50     0.16     5.53   158.49     7.11  104.78   95.92  106.50  13.59  99.85</span><br><span class="line">sdaf              0.00     0.00   11.00   38.50     0.19     3.42   149.39     6.93  128.40   79.00  142.52  20.17  99.85</span><br><span class="line">sdac              0.00     0.00   11.00   62.50     0.18     4.27   123.96     5.17   67.90   97.59   62.67  13.28  97.60</span><br><span class="line">sdag              0.00     0.00   10.00   41.00     0.19     3.77   158.72     7.08  139.36  102.00  148.48  19.58  99.85</span><br><span class="line">sdao              0.00     0.00    8.50   54.00     0.14     4.90   165.24     6.91  115.50  100.65  117.84  15.97  99.80</span><br><span class="line">sdaj              0.00     0.00   11.00   66.00     0.20     5.48   151.16     6.83   92.98  115.36   89.25  12.96  99.80</span><br><span class="line">sdai              0.00     0.00   11.00   35.00     0.20     3.13   148.52     6.82  145.83   68.82  170.03  21.70  99.80</span><br><span class="line">sdt               0.00     0.00   13.50   44.00     0.18     3.90   145.22     6.78  117.48   53.89  136.99  17.36  99.80</span><br><span class="line">sdah              0.00     0.00   12.00   53.50     0.20     4.79   156.21     7.39  115.08  110.71  116.06  15.24  99.80</span><br><span class="line">sds               0.00     0.00   10.50   53.00     0.12     4.77   157.54     6.86  111.13   51.00  123.05  15.71  99.75</span><br><span class="line">sdae              0.00     0.00   11.00   42.50     0.19     3.66   147.39     6.85  133.48   73.23  149.07  18.64  99.75</span><br><span class="line">sdam              0.00     0.00    8.00   50.00     0.15     4.61   167.91     6.31  114.64   40.50  126.50  17.19  99.70</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.20    0.00   22.24   34.84    0.00   42.71</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdo               0.00     0.00    0.50   86.00     0.02     6.59   156.42     5.86   67.03   20.00   67.31  11.55  99.95</span><br><span class="line">sdp               0.00     0.00    1.00   84.50     0.03     7.16   172.36     7.03   70.34   33.50   70.78  11.70 100.00</span><br><span class="line">sdr               0.00     0.00    1.00   72.50     0.03     6.08   170.44     6.22   90.38   21.00   91.34  13.61 100.00</span><br><span class="line">sdq               0.00     0.00    1.00   84.00     0.03     7.01   169.64     6.08   69.63   13.50   70.30  11.76 100.00</span><br><span class="line">sdap              0.00     0.00    0.50  129.50     0.02     6.47   102.29     0.07    0.53   13.00    0.48   0.32   4.10</span><br><span class="line">sdad              0.00     0.00    1.00   76.50     0.03     6.42   170.63     6.21   77.92  216.00   76.12  12.90 100.00</span><br><span class="line">sdv               0.00     0.00    1.00   84.00     0.03     6.79   164.50     8.07   73.64   73.00   73.65  11.76 100.00</span><br><span class="line">sdak              0.00     0.00    1.00   83.00     0.03     6.34   155.41     5.95   66.38  106.00   65.90  11.83  99.40</span><br><span class="line">sdal              0.00     0.00    1.00   67.50     0.03     5.59   167.98     6.07   92.59   76.50   92.83  14.60 100.00</span><br><span class="line">sdan              0.00     0.00    1.00   73.50     0.03     6.19   171.05     7.69   75.56   41.50   76.02  13.42  99.95</span><br><span class="line">sdu               0.00     0.00    1.00   80.00     0.03     6.40   162.69     6.04   73.44   54.00   73.69  12.35 100.00</span><br><span class="line">sdaf              0.00     0.00    1.00   78.00     0.01     6.69   173.73     6.45   77.08   94.00   76.87  12.66 100.00</span><br><span class="line">sdac              0.00     0.00    1.50   83.00     0.03     6.68   162.79     6.28   74.35  237.33   71.40  11.84 100.05</span><br><span class="line">sdag              0.00     0.00    0.50   83.50     0.01     7.11   173.65     7.13   70.86   77.00   70.82  11.90 100.00</span><br><span class="line">sdao              0.00     0.00    1.00   87.00     0.02     7.19   167.85     6.57   75.49  418.00   71.56  11.37 100.05</span><br><span class="line">sdaj              0.00     0.00    1.00   75.00     0.03     6.16   166.95     6.82   75.19   66.50   75.31  13.16 100.00</span><br><span class="line">sdai              0.00     0.00    1.00   79.00     0.03     6.74   173.41     7.01   75.91   18.00   76.64  12.51 100.05</span><br><span class="line">sdt               0.00     0.00    1.50   78.50     0.03     6.56   168.79     6.58   85.08  239.33   82.13  12.50 100.00</span><br><span class="line">sdah              0.00     0.00    0.50   79.00     0.02     6.69   172.81     7.25   76.71   37.00   76.96  12.58 100.00</span><br><span class="line">sds               0.00     0.00    1.00   81.50     0.03     6.90   172.09     8.08   74.24   80.00   74.17  12.13 100.05</span><br><span class="line">sdae              0.00     0.00    1.00   80.00     0.01     6.77   171.45     6.18   79.00  257.00   76.78  12.35 100.05</span><br><span class="line">sdam              0.00     0.00    1.00   78.50     0.03     6.64   171.82     6.02   79.11   24.50   79.81  12.58 100.05</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.17    0.00   20.85   37.54    0.00   40.44</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdo               0.00     0.00    2.00   85.00     0.00     7.11   167.36     5.37   56.57   74.25   56.15  10.51  91.40</span><br><span class="line">sdp               0.00     0.00    3.00   83.50     0.00     6.95   164.62     5.70   62.18  158.50   58.72  10.81  93.50</span><br><span class="line">sdr               0.00     0.00    2.00   90.50     0.00     7.43   164.43     6.44   59.36  112.25   58.19  10.79  99.85</span><br><span class="line">sdq               0.00     0.00    1.50   91.00     0.00     7.58   167.91     6.70   65.33  330.33   60.96  10.74  99.30</span><br><span class="line">sdap              0.00     0.00    2.50  143.50     0.00     7.53   105.66     0.11    0.76    9.00    0.62   0.42   6.10</span><br><span class="line">sdad              0.00     0.00    2.50   85.00     0.00     7.00   163.93     6.48   64.74  242.60   59.51  10.70  93.60</span><br><span class="line">sdv               0.00     0.00    2.00   74.50     0.00     6.58   176.24     6.50   90.99  229.50   87.27  13.07 100.00</span><br><span class="line">sdak              0.00     0.00    2.00   90.00     0.00     8.06   179.35     6.99   59.90  163.50   57.59  10.87 100.00</span><br><span class="line">sdal              0.00     0.00    4.50   79.50     0.00     6.71   163.76     6.76   82.09  202.89   75.25  11.90 100.00</span><br><span class="line">sdan              0.00     0.00    2.50   87.50     0.00     7.24   164.77     6.21   62.68  115.80   61.17  10.43  93.90</span><br><span class="line">sdu               0.00     0.00    2.00   85.00     0.00     7.61   179.13     6.41   73.32  184.00   70.72  11.49 100.00</span><br><span class="line">sdaf              0.00     0.00    2.00   88.00     0.00     7.82   177.91     6.83   74.74  414.00   67.03  11.11 100.00</span><br><span class="line">sdac              0.00     0.00    1.50   88.50     0.00     7.14   162.38     6.69   66.64  101.33   66.06  10.69  96.25</span><br><span class="line">sdag              0.00     0.00    0.50   87.50     0.00     7.21   167.86     6.95   67.72  240.00   66.73  11.03  97.10</span><br><span class="line">sdao              0.00     0.00    2.50   90.50     0.00     7.24   159.50     5.69   60.65  119.20   59.03  10.36  96.35</span><br><span class="line">sdaj              0.00     0.00    3.00   86.50     0.00     7.37   168.70     7.64   82.65  508.17   67.90  11.17 100.00</span><br><span class="line">sdai              0.00     0.00    1.50   80.50     0.00     6.68   166.99     5.25   62.49   55.67   62.62  11.07  90.75</span><br><span class="line">sdt               0.00     0.00    2.50   88.00     0.00     7.90   178.77     6.47   70.90  183.80   67.69  11.05 100.00</span><br><span class="line">sdah              0.00     0.00    2.50   87.50     0.00     7.79   177.33     6.26   69.06   73.00   68.95  11.11 100.00</span><br><span class="line">sds               0.00     0.00    1.00   94.00     0.00     8.42   181.62     7.84   65.32   35.50   65.63  10.53 100.00</span><br><span class="line">sdae              0.00     0.00    2.00   82.00     0.00     6.92   168.83     6.91   79.28  140.25   77.79  11.90 100.00</span><br><span class="line">sdam              0.00     0.00    3.50   77.50     0.00     6.40   161.80     5.96   73.86  198.14   68.25  11.69  94.70</span><br></pre></td></tr></table></figure>
<p>In this raidz3, only sdap is HGST dev, the others are seagate HDD<br>In a pair of another server not use the HDD, the bug will cause the server HBA reset again because the single dev timeout<br>Bad HDD verdor is seagate, when the HDD use ratio up to 75%, that too terrible, the HDD firmware is TT54, when you disable read buff, seagate HDD read/write jitter is very serious, 100x ~ 400x bad than WD(HGST), I guess it nothing to do with firmware, I don ‘t think TT55 could resolve it.<br>When you reset JBOD encolsure, Out of a week, the issue will show again.</p>
<p>$ sdparm -s RCE=0 /dev/sdxx could reduce the seagate latency</p>
<p>In client, read are very slow</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[&lt;ffffffff811315c0&gt;] sync_page+0x40/0x50</span><br><span class="line">[&lt;ffffffff811315de&gt;] sync_page_killable+0xe/0x40</span><br><span class="line">[&lt;ffffffff811314e7&gt;] __lock_page_killable+0x67/0x70</span><br><span class="line">[&lt;ffffffff81133254&gt;] generic_file_aio_read+0x4c4/0x710</span><br><span class="line">[&lt;ffffffffa0b7f02c&gt;] ll_file_aio_read+0x1ec/0x4c0 [lustre]</span><br><span class="line">[&lt;ffffffffa0b8024a&gt;] ll_file_read+0x13a/0x270 [lustre]</span><br><span class="line">[&lt;ffffffff8119d927&gt;] vfs_read+0xb7/0x1a0</span><br><span class="line">[&lt;ffffffff8119dc71&gt;] sys_read+0x51/0xb0</span><br><span class="line">[&lt;ffffffff8155e375&gt;] system_call_fastpath+0x19/0x34</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br><span class="line">--------</span><br><span class="line">[&lt;ffffffff811315c0&gt;] sync_page+0x40/0x50</span><br><span class="line">[&lt;ffffffff811315de&gt;] sync_page_killable+0xe/0x40</span><br><span class="line">[&lt;ffffffff811314e7&gt;] __lock_page_killable+0x67/0x70</span><br><span class="line">[&lt;ffffffff81133278&gt;] generic_file_aio_read+0x4e8/0x710</span><br><span class="line">[&lt;ffffffffa0bd0d52&gt;] vvp_io_read_start+0x302/0x570 [lustre]</span><br><span class="line">[&lt;ffffffffa073975a&gt;] cl_io_start+0x6a/0x140 [obdclass]</span><br><span class="line">[&lt;ffffffffa073c459&gt;] cl_io_loop+0x129/0xb20 [obdclass]</span><br><span class="line">[&lt;ffffffffa0b7e5d1&gt;] ll_file_io_generic+0x231/0xaa0 [lustre]</span><br><span class="line">[&lt;ffffffffa0b7f1e3&gt;] ll_file_aio_read+0x3a3/0x4c0 [lustre]</span><br><span class="line">[&lt;ffffffffa0b8024a&gt;] ll_file_read+0x13a/0x270 [lustre]</span><br><span class="line">[&lt;ffffffff8119d927&gt;] vfs_read+0xb7/0x1a0</span><br><span class="line">[&lt;ffffffff8119dc71&gt;] sys_read+0x51/0xb0</span><br><span class="line">[&lt;ffffffff8155e375&gt;] system_call_fastpath+0x19/0x34</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br><span class="line">--------</span><br><span class="line">[&lt;ffffffff811315c0&gt;] sync_page+0x40/0x50</span><br><span class="line">[&lt;ffffffff811315de&gt;] sync_page_killable+0xe/0x40</span><br><span class="line">[&lt;ffffffff811314e7&gt;] __lock_page_killable+0x67/0x70</span><br><span class="line">[&lt;ffffffff81133254&gt;] generic_file_aio_read+0x4c4/0x710</span><br><span class="line">[&lt;ffffffffa0b7f02c&gt;] ll_file_aio_read+0x1ec/0x4c0 [lustre]</span><br><span class="line">[&lt;ffffffffa0b8024a&gt;] ll_file_read+0x13a/0x270 [lustre]</span><br><span class="line">[&lt;ffffffff8119d927&gt;] vfs_read+0xb7/0x1a0</span><br><span class="line">[&lt;ffffffff8119dc71&gt;] sys_read+0x51/0xb0</span><br><span class="line">[&lt;ffffffff8155e375&gt;] system_call_fastpath+0x19/0x34</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br><span class="line">--------</span><br><span class="line">[&lt;ffffffffa0739d35&gt;] cl_sync_io_wait+0x2a5/0x380 [obdclass]</span><br><span class="line">[&lt;ffffffffa07386ae&gt;] cl_lock_request+0x1ce/0x200 [obdclass]</span><br><span class="line">[&lt;ffffffffa0bc9d13&gt;] cl_glimpse_lock+0x113/0x2b0 [lustre]</span><br><span class="line">[&lt;ffffffffa0bca097&gt;] cl_glimpse_size0+0x1e7/0x240 [lustre]</span><br><span class="line">[&lt;ffffffffa0b857cc&gt;] ll_getattr+0x31c/0x8a0 [lustre]</span><br><span class="line">[&lt;ffffffff811a2e44&gt;] vfs_getattr+0x54/0x90</span><br><span class="line">[&lt;ffffffff811a2ee4&gt;] vfs_fstatat+0x64/0xa0</span><br><span class="line">[&lt;ffffffff811a304b&gt;] vfs_stat+0x1b/0x20</span><br><span class="line">[&lt;ffffffff811a3074&gt;] sys_newstat+0x24/0x50</span><br><span class="line">[&lt;ffffffff8155e375&gt;] system_call_fastpath+0x19/0x34</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br></pre></td></tr></table></figure>

<h3 id="Error-ICRC-ABRT-at-LBA-in-SATA-device"><a href="#Error-ICRC-ABRT-at-LBA-in-SATA-device" class="headerlink" title="Error: ICRC, ABRT at LBA in SATA device"></a>Error: ICRC, ABRT at LBA in SATA device</h3><p>zfs show a lot of check sum error<br>fix it by upgrade IO expander firmware</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Error 17 occurred at disk power-on lifetime: 1903 hours (79 days + 7 hours)</span><br><span class="line">  When the <span class="built_in">command</span> that caused the error occurred, the device was doing SMART Offline or Self-test.</span><br><span class="line"></span><br><span class="line">  After <span class="built_in">command</span> completion occurred, registers were:</span><br><span class="line">  ER ST SC SN CL CH DH</span><br><span class="line">  -- -- -- -- -- -- --</span><br><span class="line">  84 41 00 00 00 00 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0</span><br><span class="line"></span><br><span class="line">  Commands leading to the <span class="built_in">command</span> that caused the error were:</span><br><span class="line">  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name</span><br><span class="line">  -- -- -- -- -- -- -- --  ----------------  --------------------</span><br><span class="line">  61 05 10 b2 8e 78 40 00  23d+05:55:53.717  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 38 b1 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 30 d9 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 28 d8 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 0a 20 68 ac 53 40 00  23d+05:55:53.715  WRITE FPDMA QUEUED</span><br><span class="line"></span><br><span class="line">  pool: tank</span><br><span class="line"> state: ONLINE</span><br><span class="line">status: One or more devices has experienced an unrecoverable error.  An</span><br><span class="line">	attempt was made to correct the error.  Applications are unaffected.</span><br><span class="line">action: Determine <span class="keyword">if</span> the device needs to be replaced, and clear the errors</span><br><span class="line">	using <span class="string">'zpool clear'</span> or replace the device with <span class="string">'zpool replace'</span>.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-9P</span><br><span class="line">  scan: scrub repaired 55.5K <span class="keyword">in</span> 11h11m with 0 errors on Mon Mar 18 09:57:08 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">	NAME        STATE     READ WRITE CKSUM</span><br><span class="line">	tank        ONLINE       0     0     0</span><br><span class="line">	  raidz3-0  ONLINE       0     0     0</span><br><span class="line">	    sdh     ONLINE       0     0    51</span><br><span class="line">	    sdi     ONLINE       0     0    13</span><br><span class="line">	    sdj     ONLINE       0     0    19</span><br><span class="line">	    sdk     ONLINE       0     0    10</span><br><span class="line">	    sdl     ONLINE       0     0    28</span><br><span class="line">	    sdm     ONLINE       0     0    39</span><br><span class="line">	    sdn     ONLINE       0     0    70</span><br><span class="line">	    sdo     ONLINE       0     0    25</span><br></pre></td></tr></table></figure>

<h3 id="Enable-TLER-ERC-CCTL-function-for-SATA-device"><a href="#Enable-TLER-ERC-CCTL-function-for-SATA-device" class="headerlink" title="Enable TLER/ERC/CCTL function for SATA device"></a>Enable TLER/ERC/CCTL function for SATA device</h3><p>Some vendor default value was disabled</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ smartctl -l scterc,70,70 /dev/sdab</span><br><span class="line">smartctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-957.el7_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">SCT Error Recovery Control <span class="built_in">set</span> to:</span><br><span class="line">           Read:     70 (7.0 seconds)</span><br><span class="line">          Write:     70 (7.0 seconds)</span><br><span class="line"></span><br><span class="line">$ smartctl -l scterc /dev/sdab</span><br><span class="line">smartctl 6.5 2016-05-07 r4318 [x86_64-linux-3.10.0-957.el7_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-16, Bruce Allen, Christian Franke, www.smartmontools.org</span><br><span class="line"></span><br><span class="line">SCT Error Recovery Control:</span><br><span class="line">           Read:     70 (7.0 seconds)</span><br><span class="line">          Write:     70 (7.0 seconds)</span><br></pre></td></tr></table></figure>


<h3 id="SAS-smart-status"><a href="#SAS-smart-status" class="headerlink" title="SAS smart status"></a>SAS smart status</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">20c20</span><br><span class="line">&lt; Local Time is:        Tue Jun  4 19:01:23 2019 CST</span><br><span class="line">---</span><br><span class="line">&gt; Local Time is:        Tue Jun  4 19:00:14 2019 CST</span><br><span class="line">39c39</span><br><span class="line">&lt;   Blocks sent to initiator = 30411897006194688</span><br><span class="line">---</span><br><span class="line">&gt;   Blocks sent to initiator = 30411894288285696</span><br><span class="line">45,46c45,46</span><br><span class="line">&lt; <span class="built_in">read</span>:          0  1074545         0   1813692   56373220      72544.663         180</span><br><span class="line">&lt; write:         0        1         0         3    4864356      42356.396           0</span><br><span class="line">---</span><br><span class="line">&gt; <span class="built_in">read</span>:          0  1074502         0   1813649   56371166      72544.656         180</span><br><span class="line">&gt; write:         0        1         0         3    4864355      42356.390           0</span><br><span class="line"></span><br><span class="line">Error counter <span class="built_in">log</span>:</span><br><span class="line">           Errors Corrected by           Total   Correction     Gigabytes    Total</span><br><span class="line">               ECC          rereads/    errors   algorithm      processed    uncorrected</span><br><span class="line">           fast | delayed   rewrites  corrected  invocations   [10^9 bytes]  errors</span><br><span class="line"><span class="built_in">read</span>:          0  1074545         0   1813692   56373220      72544.663         180</span><br><span class="line">write:         0        1         0         3    4864356      42356.396           0</span><br><span class="line">...</span><br><span class="line"><span class="built_in">read</span>:          0  1074502         0   1813649   56371166      72544.656         180</span><br><span class="line">write:         0        1         0         3    4864355      42356.390           0</span><br></pre></td></tr></table></figure>

<h3 id="replace-sas-devices"><a href="#replace-sas-devices" class="headerlink" title="replace sas devices"></a>replace sas devices</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[Wed Jun  5 11:10:29 2019] sd 13:0:59:0: device_block, handle(0x0047)</span><br><span class="line">[Wed Jun  5 11:10:31 2019] sd 13:0:59:0: device_unblock and setting to running, handle(0x0047)</span><br><span class="line">[Wed Jun  5 11:10:31 2019] sd 13:0:59:0: [sdes] Synchronizing SCSI cache</span><br><span class="line">[Wed Jun  5 11:10:31 2019] sd 13:0:59:0: [sdes] Synchronize Cache(10) failed: Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK</span><br><span class="line"></span><br><span class="line"><span class="comment"># echo 1 &gt; delete</span></span><br><span class="line"></span><br><span class="line">[Wed Jun  5 11:10:31 2019] mpt3sas_cm2: removing handle(0x0047), sas_addr(0x5000cca2516f15ae)</span><br><span class="line">[Wed Jun  5 11:10:31 2019] mpt3sas_cm2: enclosure logical id(0x500304800928a0bf), slot(6)</span><br><span class="line">[Wed Jun  5 11:10:31 2019] mpt3sas_cm2: enclosure level(0x0001),connector name(     )</span><br><span class="line">[Wed Jun  5 11:13:50 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:50 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:50 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:51 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:51 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:51 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:51 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:51 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:52 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:52 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:52 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:52 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:52 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:53 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:53 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:53 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:53 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:53 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:54 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:54 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:54 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:54 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:54 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:55 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:55 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:55 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:55 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:55 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:56 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:56 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:56 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:56 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:56 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:57 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:57 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:57 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:57 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:57 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:58 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:58 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:58 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:58 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:58 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:59 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:13:59 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:13:59 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:59 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:13:59 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:00 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:00 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:00 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:00 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:00 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:01 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:01 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:01 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:01 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:01 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:02 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:02 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:02 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:02 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:02 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:03 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:03 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:03 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:03 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:03 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:04 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:04 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:04 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:04 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:04 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:05 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:05 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:06 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:06 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:06 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:06 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:06 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:07 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:07 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:07 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:07 2019] mpt3sas_cm2: START_UNIT: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:07 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] mpt3sas_cm2: detecting: handle(0x0069), sas_address(0x5000cca2519ae8fe), phy(7)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] mpt3sas_cm2: REPORT_LUNS: handle(0x0069), retries(0)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] mpt3sas_cm2: TEST_UNIT_READY: handle(0x0069), lun(0)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: Direct-Access     HGST     HUH721010AL5200  A21D PQ: 0 ANSI: 6</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: SSP: handle(0x0069), sas_addr(0x5000cca2519ae8fe), phy(7), device_name(0x5000cca2519ae8ff)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: enclosure logical id(0x500304800928a0bf), slot(6)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: enclosure level(0x0001), connector name(     )</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: serial_number(        7PJS61HC)</span><br><span class="line">[Wed Jun  5 11:14:08 2019] scsi 13:0:93:0: qdepth(254), tagged(1), simple(0), ordered(0), scsi_level(7), cmd_que(1)</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: Attached scsi generic sg154 <span class="built_in">type</span> 0</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] 19532873728 512-byte logical blocks: (10.0 TB/9.09 TiB)</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] 4096-byte physical blocks</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] Write Protect is off</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] Mode Sense: f7 00 10 08</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] Write cache: enabled, <span class="built_in">read</span> cache: enabled, supports DPO and FUA</span><br><span class="line">[Wed Jun  5 11:14:09 2019]  sdes:</span><br><span class="line">[Wed Jun  5 11:14:09 2019] sd 13:0:93:0: [sdes] Attached SCSI disk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### zfs cksum error</span></span><br><span class="line"><span class="comment">#### Cache issue</span></span><br><span class="line">Some <span class="built_in">times</span> you could not found any errro from scsi, drvier, cable, hba, IO expander, smart check HDD is ok, no error</span><br><span class="line">Looks like smartctl -t long could not check SAS/SATA device <span class="built_in">read</span>/write cache</span><br><span class="line">If you <span class="built_in">disable</span> the device write cache, the cksum error is missing</span><br></pre></td></tr></table></figure>
<p>Dec 18 2019 03:34:03.733962748 ereport.fs.zfs.checksum<br>        class = “ereport.fs.zfs.checksum”<br>        ena = 0xe4ec5ccfb8a01001<br>        detector = (embedded nvlist)<br>                version = 0x0<br>                scheme = “zfs”<br>                pool = 0x2c8f99507dfa8d55<br>                vdev = 0x43a5020a3f617b3a<br>        (end detector)<br>        pool = “ost_48”<br>        pool_guid = 0x2c8f99507dfa8d55<br>        pool_state = 0x0<br>        pool_context = 0x0<br>        pool_failmode = “wait”<br>        vdev_guid = 0x43a5020a3f617b3a<br>        vdev_type = “disk”<br>        vdev_path = “/dev/disk/by-id/scsi-35000cca251b14e3c”<br>        vdev_ashift = 0xc<br>        vdev_complete_ts = 0x60e4ec5af404e<br>        vdev_delta_ts = 0xf254e8<br>        vdev_read_errors = 0x0<br>        vdev_write_errors = 0x0<br>        vdev_cksum_errors = 0x3865<br>        parent_guid = 0x7ed8241b267c55ce<br>        parent_type = “raidz”<br>        vdev_spare_paths =<br>        vdev_spare_guids =<br>        zio_err = 0x0<br>        zio_flags = 0x1008b0<br>        zio_stage = 0x100000<br>        zio_pipeline = 0xf80000<br>        zio_delay = 0x0<br>        zio_timestamp = 0x0<br>        zio_delta = 0x0<br>        zio_offset = 0x35b7ff04400<br>        zio_size = 0x2a00<br>        zio_objset = 0x2b<br>        zio_object = 0xd988<br>        zio_level = 0x0<br>        zio_blkid = 0x8654<br>        bad_ranges = 0x0 0x2a00<br>        bad_ranges_min_gap = 0x8<br>        bad_range_sets = 0x2956<br>        bad_range_clears = 0x855f<br>        bad_set_histogram = 0x76 0x7e 0xbc 0xc8 0xd6 0x9c 0xcc 0x96 0x6c 0x81 0xb0 0xce 0xd7 0x74 0xcb 0x8b 0x72 0x74 0xbe 0xcd 0xc6 0x92 0xb2 0x96 0x7a 0x93 0xc3 0xe1 0xd7 0x9b 0xce 0x88 0x7c 0x72 0xaa 0xdd 0xbe 0x86 0xbc 0x83 0x6d 0x74 0xcc 0xc2 0xc6 0x86 0xc1 0x87 0x74 0x8b 0xbe 0xc0 0xc2 0xa0 0xce 0x8c 0x73 0x8e 0xb3 0xe4 0xc8 0x8a 0xbc 0x9a<br>        bad_cleared_histogram = 0x253 0x242 0x1ff 0x1d8 0x1e5 0x233 0x1d9 0x22a 0x243 0x24b 0x20b 0x1f9 0x1dc 0x24b 0x1f7 0x23f 0x23c 0x25b 0x1fa 0x1db 0x1e9 0x223 0x1f7 0x20f 0x235 0x224 0x1ff 0x1ee 0x1d3 0x225 0x1fa 0x23b 0x253 0x271 0x20f 0x1ec 0x1e5 0x221 0x1fb 0x248 0x24e 0x228 0x1e7 0x1e0 0x1ed 0x24f 0x1f2 0x24f 0x264 0x22d 0x202 0x1e5 0x1ff 0x236 0x1ca 0x222 0x250 0x230 0x217 0x1e6 0x1d6 0x22b 0x1e7 0x238<br>        time = 0x5df92dab 0x2bbf61fc<br>        eid = 0x6ec1ab2</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">### Recovery zfs partition table</span><br><span class="line">#### backup partition table</span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96; bash</span><br><span class="line"># MBR</span><br><span class="line">sfdisk -d &#x2F;dev&#x2F;sda &gt; &#x2F;tmp&#x2F;sda.bak</span><br><span class="line">sfdisk &#x2F;dev&#x2F;sda &lt; &#x2F;tmp&#x2F;sda.bak</span><br><span class="line"></span><br><span class="line">dd if&#x3D;&#x2F;dev&#x2F;sda of&#x3D;&#x2F;dev&#x2F;sdb bs&#x3D;512 count&#x3D;1</span><br><span class="line"></span><br><span class="line"># GPT</span><br><span class="line">sgdisk --backup&#x3D;table &#x2F;dev&#x2F;sda</span><br><span class="line">sgdisk --load-backup&#x3D;table &#x2F;dev&#x2F;sdb</span><br><span class="line">sgdisk -G &#x2F;dev&#x2F;sdb</span><br><span class="line"></span><br><span class="line">B&#x3D;$(parted -ms &#x2F;dev&#x2F;sda print |tail -1|cut -b1)</span><br><span class="line">A&#x3D;$((128*B)+1024)</span><br><span class="line"></span><br><span class="line">dd if&#x3D;&#x2F;dev&#x2F;sda of&#x3D;GPT_TABLE bs&#x3D;1 count&#x3D;$A</span><br><span class="line">dd if&#x3D;GPT_TABLE of&#x3D;&#x2F;dev&#x2F;sdb</span><br><span class="line">partprobe &#x2F;dev&#x2F;sdb</span><br></pre></td></tr></table></figure>
<p>Backup is very important</p>
<h4 id="Damage-command"><a href="#Damage-command" class="headerlink" title="Damage command"></a>Damage command</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">950  parted /dev/sdb rm 1</span><br><span class="line">951  df -h</span><br><span class="line">952  zpool status</span><br><span class="line">953  zpool destroy tank</span><br><span class="line">954  yum remove $(rpm -qa | grepzfs)</span><br><span class="line">955 parted -a cylinder /dev/sdb mkpart primary 0 100%</span><br><span class="line"><span class="comment"># same parted commmand with created</span></span><br></pre></td></tr></table></figure>
<p>zpool -D improt failed, </p>
<h4 id="Found-uberblock-tag-label"><a href="#Found-uberblock-tag-label" class="headerlink" title="Found uberblock tag label"></a>Found uberblock tag label</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dd if=/dev/sdb1 of=label-2m bs=1M count=2</span></span><br></pre></td></tr></table></figure>

<p>The begin address is 011bc00-128KB, and after 256KB is the uberblock<br>You can use zdb -l 256KB-file, you could get zpool info<br>zfs magic number is 0cb1 ba00<br><img src="/img/uberblock_tag.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># awk 'BEGIN&#123;printf "%x",0x011bc00-0x0020000"\n"&#125;'</span></span><br><span class="line">fbc00</span><br><span class="line"><span class="comment"># awk 'BEGIN&#123;printf "%x",0x011bc00+0x0020000"\n"&#125;'</span></span><br><span class="line">13bc00</span><br></pre></td></tr></table></figure>
<p>The address is from fbc00 to 13bc00</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'BEGIN&#123;print (strtonum("0xfbc00"))/1024&#125;'</span></span><br><span class="line">1007</span><br></pre></td></tr></table></figure>
<p>The begin address in 1007K of the disk (sdb1)</p>
<h4 id="Create-a-new-zpool-in-another-disk"><a href="#Create-a-new-zpool-in-another-disk" class="headerlink" title="Create a new zpool in another disk"></a>Create a new zpool in another disk</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># parted -a cylinder /dev/sdd mkpart primary 0 100%a</span></span><br><span class="line"><span class="comment"># zpool create tank /dev/sdd1</span></span><br><span class="line"><span class="comment"># dd if=/dev/sdd1 of=label-sdd bs=1M count=2</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/uberblock_tag2.png" alt=""><br>The tag at 0021000, and there are nothing in previous part except zero</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># dd if=/dev/zero of=/dev/sdc1 bs=4k count=1</span></span><br><span class="line"><span class="comment"># dd if=/dev/sdb1 of=/dev/sdc1 bs=512 skip=2014 #offset 1007KB</span></span><br></pre></td></tr></table></figure>
<p>You can use zdb get infomation from sdc1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">version: 5000</span><br><span class="line">name: <span class="string">'tank'</span></span><br><span class="line">state: 2</span><br><span class="line">txg: 1256237</span><br><span class="line">pool_guid: 10281825564445304044</span><br><span class="line">errata: 0</span><br><span class="line">hostname: <span class="string">'nas-56-11.local'</span></span><br><span class="line">top_guid: 8832263634429449792</span><br><span class="line">guid: 8832263634429449792</span><br><span class="line">vdev_children: 1</span><br><span class="line">vdev_tree:</span><br><span class="line">    <span class="built_in">type</span>: <span class="string">'disk'</span></span><br><span class="line">    id: 0</span><br><span class="line">    guid: 8832263634429449792</span><br><span class="line">    path: <span class="string">'/dev/sdb1'</span></span><br><span class="line">    whole_disk: 1</span><br><span class="line">    metaslab_array: 33</span><br><span class="line">    metaslab_shift: 37</span><br><span class="line">    ashift: 12</span><br><span class="line">    asize: 17990975750144</span><br><span class="line">    is_log: 0</span><br><span class="line">    create_txg: 4</span><br></pre></td></tr></table></figure>
<p>When you dd complete. Use “zpool -D import tank” import the loss pool.<br>The disk is a hardraid , There are 17TB important data in it. It really scared the heck out of me.<br>Really Thank Javen Wu very much, Because your help these data could be recovery.</p>
<h4 id="About-partition-table"><a href="#About-partition-table" class="headerlink" title="About partition table"></a><a href="https://unix.stackexchange.com/questions/103919/how-do-i-find-the-offset-of-an-ext4-filesystem" target="_blank" rel="noopener">About partition table</a></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">Starting at (512-byte) sector 63. This was the tradition <span class="keyword">for</span> a very long time, and worked until someone came up with 4K disks...</span><br><span class="line">Starting at (512-byte) sector 2048. This is the new tradition, to accommodate 4K disks.</span><br><span class="line">A bonus option! Sarting at sector 56. This is what happens <span class="keyword">if</span> someone moves the 63-start partition to make it align with a 4K sector.</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>zfs</tag>
        <tag>scsi</tag>
      </tags>
  </entry>
  <entry>
    <title>kvm_tips</title>
    <url>/2017/03/09/kvm-command/</url>
    <content><![CDATA[<h3 id="Two-guest-share-a-storage-device"><a href="#Two-guest-share-a-storage-device" class="headerlink" title="Two guest share a storage device"></a>Two guest share a storage device</h3><p>Qcow2 not support, raw is ok</p>
<a id="more"></a>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=share-storage.raw bs=1M count=0 seek=<span class="variable">$the_size</span></span><br><span class="line">bus=scsi</span><br><span class="line">cache=writethrough</span><br><span class="line"></span><br><span class="line">    &lt;disk <span class="built_in">type</span>=<span class="string">'file'</span> device=<span class="string">'disk'</span>&gt;</span><br><span class="line">      &lt;driver name=<span class="string">'qemu'</span> <span class="built_in">type</span>=<span class="string">'raw'</span> cache=<span class="string">'writethrough'</span>/&gt;</span><br><span class="line">      &lt;<span class="built_in">source</span> file=<span class="string">'/tank/test0.raw'</span>/&gt;</span><br><span class="line">      &lt;target dev=<span class="string">'sda'</span> bus=<span class="string">'scsi'</span>/&gt;</span><br><span class="line">      &lt;shareable/&gt;</span><br><span class="line">      &lt;serial&gt;0001&lt;/serial&gt;</span><br><span class="line">      &lt;address <span class="built_in">type</span>=<span class="string">'drive'</span> controller=<span class="string">'0'</span> bus=<span class="string">'0'</span> target=<span class="string">'0'</span> unit=<span class="string">'0'</span>/&gt;</span><br><span class="line">    &lt;/disk&gt;</span><br><span class="line">    &lt;disk <span class="built_in">type</span>=<span class="string">'file'</span> device=<span class="string">'disk'</span>&gt;</span><br><span class="line">      &lt;driver name=<span class="string">'qemu'</span> <span class="built_in">type</span>=<span class="string">'raw'</span> cache=<span class="string">'writethrough'</span>/&gt;</span><br><span class="line">      &lt;<span class="built_in">source</span> file=<span class="string">'/tank/test1.raw'</span>/&gt;</span><br><span class="line">      &lt;target dev=<span class="string">'sdb'</span> bus=<span class="string">'scsi'</span>/&gt;</span><br><span class="line">      &lt;shareable/&gt;</span><br><span class="line">      &lt;serial&gt;0002&lt;/serial&gt;</span><br><span class="line">      &lt;address <span class="built_in">type</span>=<span class="string">'drive'</span> controller=<span class="string">'0'</span> bus=<span class="string">'0'</span> target=<span class="string">'0'</span> unit=<span class="string">'1'</span>/&gt;</span><br><span class="line">    &lt;/disk&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">### Install</span></span><br><span class="line">```bash</span><br><span class="line">yum -y install qemu-kvm libvirt virt-install bridge-utils</span><br></pre></td></tr></table></figure>

<h3 id="KVM-Multipath"><a href="#KVM-Multipath" class="headerlink" title="KVM Multipath"></a>KVM Multipath</h3><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">disk</span> <span class="attr">type</span>=<span class="string">'file'</span> <span class="attr">device</span>=<span class="string">'disk'</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">driver</span> <span class="attr">name</span>=<span class="string">'qemu'</span> <span class="attr">type</span>=<span class="string">'qcow2'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">source</span> <span class="attr">file</span>=<span class="string">'/kvm/fileA.qcow2'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">target</span> <span class="attr">dev</span>=<span class="string">'sda'</span> <span class="attr">bus</span>=<span class="string">'scsi'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">serial</span>&gt;</span>0002<span class="tag">&lt;/<span class="name">serial</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">address</span> <span class="attr">type</span>=<span class="string">'drive'</span> <span class="attr">controller</span>=<span class="string">'0'</span> <span class="attr">bus</span>=<span class="string">'0'</span> <span class="attr">target</span>=<span class="string">'0'</span> <span class="attr">unit</span>=<span class="string">'0'</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">disk</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">disk</span> <span class="attr">type</span>=<span class="string">'file'</span> <span class="attr">device</span>=<span class="string">'disk'</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">driver</span> <span class="attr">name</span>=<span class="string">'qemu'</span> <span class="attr">type</span>=<span class="string">'qcow2'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">source</span> <span class="attr">file</span>=<span class="string">'/kvm/fileA.qcow2'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">target</span> <span class="attr">dev</span>=<span class="string">'sdb'</span> <span class="attr">bus</span>=<span class="string">'scsi'</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">serial</span>&gt;</span>0002<span class="tag">&lt;/<span class="name">serial</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">address</span> <span class="attr">type</span>=<span class="string">'drive'</span> <span class="attr">controller</span>=<span class="string">'0'</span> <span class="attr">bus</span>=<span class="string">'0'</span> <span class="attr">target</span>=<span class="string">'0'</span> <span class="attr">unit</span>=<span class="string">'1'</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">disk</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># list</span></span><br><span class="line">qemu-img snapshot -l /kvm/fileA.qcow2</span><br><span class="line">ID ...</span><br><span class="line">1 ...  </span><br><span class="line"></span><br><span class="line"><span class="comment"># create</span></span><br><span class="line">qemu-img snapshot -c first /kvm/fileA.qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># recovery</span></span><br><span class="line">qemu-img snapshot -a 1 /kvm/fileA.qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Delete</span></span><br><span class="line">qemu-img snapshot -d 1 /kvm/fileA.qcow2</span><br></pre></td></tr></table></figure>

<h3 id="Create-convert-base-image"><a href="#Create-convert-base-image" class="headerlink" title="Create convert base image"></a>Create convert base image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">qemu-img create -f qed vm1.qed 80G</span><br><span class="line">qemu-img convert -f vmdk -O raw centos7.vmdk centos7.img</span><br><span class="line"><span class="comment">#Base images</span></span><br><span class="line">qemu-img create -b centos6-base.qed -f  qed  centos6-tmp.qed</span><br><span class="line">qemu-img rebase  -u -b centos-6.qed ./kvm1.qed</span><br></pre></td></tr></table></figure>

<h3 id="Virt-manager-show-unreadable-code"><a href="#Virt-manager-show-unreadable-code" class="headerlink" title="Virt-manager show unreadable code"></a>Virt-manager show unreadable code</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum -y install dejavu-sans-fonts</span><br><span class="line"><span class="comment"># if you need Chinese</span></span><br><span class="line">yum -y install ghostscript-chinese-zh_CN</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>virtualization</category>
      </categories>
      <tags>
        <tag>multipath</tag>
        <tag>centos</tag>
        <tag>kvm</tag>
        <tag>virtualization</tag>
      </tags>
  </entry>
  <entry>
    <title>A copy about ZFS Internals(p1-p5)</title>
    <url>/2016/12/28/learn-zfs-internals-p1-p5/</url>
    <content><![CDATA[<h3 id="ZFS-Internals"><a href="#ZFS-Internals" class="headerlink" title="ZFS Internals"></a><a href="http://www.eall.com.br/blog/?p=312" target="_blank" rel="noopener">ZFS Internals</a></h3><a id="more"></a>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=disk0 bs=1M count=100</span><br><span class="line">104857600 bytes (105 MB) copied, 0.0582985 s, 1.8 GB/s</span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=disk1 bs=1M count=100</span><br><span class="line">104857600 bytes (105 MB) copied, 0.0587067 s, 1.8 GB/s</span><br><span class="line">$ zpool create <span class="built_in">test</span>-pool /tank/<span class="built_in">test</span>/disk0  /tank/<span class="built_in">test</span>/disk1 </span><br><span class="line"></span><br><span class="line"><span class="comment"># I don 't know why only 128M could be used. loss a lot of capacity</span></span><br><span class="line">$ zpool list <span class="built_in">test</span>-pool</span><br><span class="line">NAME        SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT</span><br><span class="line"><span class="built_in">test</span>-pool   160M   244K   160M         -     1%     0%  1.00x  ONLINE  -</span><br><span class="line"></span><br><span class="line">$ zfs get compression <span class="built_in">test</span>-pool</span><br><span class="line">NAME       PROPERTY     VALUE     SOURCE</span><br><span class="line"><span class="built_in">test</span>-pool  compression  off       default</span><br><span class="line">$ zfs get recordsize <span class="built_in">test</span>-pool</span><br><span class="line">NAME       PROPERTY    VALUE    SOURCE</span><br><span class="line"><span class="built_in">test</span>-pool  recordsize  128K     default</span><br><span class="line"></span><br><span class="line">$ touch /<span class="built_in">test</span>-pool/empty </span><br><span class="line">$ du -hs /<span class="built_in">test</span>-pool/empty</span><br><span class="line">512	/<span class="built_in">test</span>-pool/empty</span><br><span class="line"></span><br><span class="line"><span class="comment"># ashift=0 ?</span></span><br><span class="line"><span class="built_in">test</span>-pool  ashift    0       default</span><br><span class="line"></span><br><span class="line">$ rm -f testfile ;<span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; testfile; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ ls -li</span><br><span class="line">7 -rw-r--r-- 1 root root 382106 12月 28 12:45 testfile</span><br><span class="line"></span><br><span class="line">$ zdb -dddddddd <span class="built_in">test</span>-pool/ 7</span><br><span class="line">Dataset <span class="built_in">test</span>-pool [ZPL], ID 21, cr_txg 1, 404K, 7 objects, rootbp DVA[0]=&lt;0:16600:200&gt; DVA[1]=&lt;1:89200:200&gt; [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=77L/77P fill=7 cksum=b706a1be7:479be3ec2c2:e63cf120702c:1fa949c8882cfd</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">         7    2    16K   128K   385K   384K  100.00  ZFS plain file (K=inherit) (Z=inherit)</span><br><span class="line">                                        168   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 2</span><br><span class="line">	path	/testfile</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Dec 28 12:45:10 2016</span><br><span class="line">	mtime	Wed Dec 28 12:45:12 2016</span><br><span class="line">	ctime	Wed Dec 28 12:45:12 2016</span><br><span class="line">	crtime	Wed Dec 28 12:45:10 2016</span><br><span class="line">	gen	77</span><br><span class="line">	mode	100644</span><br><span class="line">	size	382106</span><br><span class="line">	parent	4</span><br><span class="line">	links	1</span><br><span class="line">	pflags	40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:2400:200 1:2400:200 4000L/200P F=3 B=77/77</span><br><span class="line">               0  L0 1:28000:20000 20000L/20000P F=1 B=77/77</span><br><span class="line">           20000  L0 1:48000:20000 20000L/20000P F=1 B=77/77</span><br><span class="line">           40000  L0 1:68000:20000 20000L/20000P F=1 B=77/77</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000060000) size  384K</span><br><span class="line"></span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print strtonum("0x"20000)&#125;'</span></span><br><span class="line">131072</span><br></pre></td></tr></table></figure>

<ul>
<li><p>0 0 L1  0:2400:200 1:2400:200 4000L/200P F=3 B=77/77</p>
<ul>
<li>L1 means two levels of indirection (number of block pointers which need to be traversed to arrive at this data)</li>
<li>0:2400:200 = disk0 , first raidz volume ?</li>
<li>the offset from the begining of the disk (2400)</li>
<li>and the size of the block (0x200 = 512 bytes)</li>
<li>0:2400 is the Data virtual Address 1 (dva1, Data virtual address )</li>
<li>F=3  is the fill count, and describes the number of non-zero block pointers under this block pointer</li>
<li>B=77/77 is the birth time, what is the same as the txg number, or some body said it ‘s a transaction number ?</li>
</ul>
</li>
<li><p>0  L0 1:28000:20000 20000L/20000P F=1 B=77/77</p>
<ul>
<li>L0 is the block level that holds data (we can have up to six levels)</li>
<li>the fill count has a little different interpretation. Here the F= means if the block has data or not (0 or 1), what is different from the levels 1 and above, where it means “how many” non-zero block pointers under this block pointer. </li>
</ul>
</li>
</ul>
<p>Output first 128K of our file<br>16 bytes per line</p>
<p>-R poolname vdev:offset:size[:flags]</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'BEGIN&#123;print strtonum("0x"20000)&#125;'</span></span><br><span class="line">131072</span><br><span class="line"></span><br><span class="line">$ zdb -R <span class="built_in">test</span>-pool/ 1:28000:20000 | head</span><br><span class="line">1:28000:20000</span><br><span class="line">          0 1 2 3 4 5 6 7   8 9 a b c d e f  0123456789abcdef</span><br><span class="line">000000:  0a330a320a310a30  0a370a360a350a34  0.1.2.3.4.5.6.7.</span><br><span class="line">000010:  310a30310a390a38  0a33310a32310a31  8.9.10.11.12.13.</span><br><span class="line">000020:  36310a35310a3431  310a38310a37310a  14.15.16.17.18.1</span><br><span class="line">000030:  0a31320a30320a39  34320a33320a3232  9.20.21.22.23.24</span><br><span class="line">000040:  320a36320a35320a  0a39320a38320a37  .25.26.27.28.29.</span><br><span class="line">000050:  32330a31330a3033  330a34330a33330a  30.31.32.33.34.3</span><br><span class="line">000060:  0a37330a36330a35  30340a39330a3833  5.36.37.38.39.40</span><br></pre></td></tr></table></figure>


<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zdb -R tank/<span class="built_in">test</span> 1:28000:20000:r | head</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line">$ zdb -R <span class="built_in">test</span>-pool/ 1:28000:20000:r &gt; file1</span><br><span class="line">Found vdev: /tank/<span class="built_in">test</span>/disk1</span><br><span class="line">$ zdb -R <span class="built_in">test</span>-pool/ 1:48000:20000:r &gt; file2</span><br><span class="line">Found vdev: /tank/<span class="built_in">test</span>/disk1</span><br><span class="line">$ zdb -R <span class="built_in">test</span>-pool/ 1:68000:20000:r &gt; file3</span><br><span class="line">Found vdev: /tank/<span class="built_in">test</span>/disk1</span><br><span class="line"></span><br><span class="line">$ ls -lh</span><br><span class="line">-rw-r--r-- 1 root root 128K 12月 28 12:51 file1</span><br><span class="line">-rw-r--r-- 1 root root 128K 12月 28 12:53 file2</span><br><span class="line">-rw-r--r-- 1 root root 128K 12月 28 12:53 file3</span><br><span class="line">-rw-r--r-- 1 root root 374K 12月 28 12:45 testfile</span><br><span class="line"></span><br><span class="line">$ head -n 2 file1 </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">$ head -n 2 testfile </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">$ tail -n 2 file1</span><br><span class="line">23695</span><br><span class="line">23696</span><br><span class="line">$ head -n 2 file2</span><br><span class="line">23697</span><br><span class="line">23698</span><br><span class="line">$ tail -n 2 file2</span><br><span class="line">45541</span><br><span class="line">45$ head -n 2 file3</span><br><span class="line">542</span><br><span class="line">45543</span><br><span class="line">$ tail -n 2 file3</span><br><span class="line">65535</span><br><span class="line"></span><br><span class="line"><span class="comment"># why output single line ?</span></span><br><span class="line">$ tail -n 2 file3 | hexdump -C</span><br><span class="line">00000000  36 35 35 33 35 0a 00 00  00 00 00 00 00 00 00 00  |65535...........|</span><br><span class="line">00000010  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|</span><br><span class="line">*</span><br><span class="line">00002b60  00 00 00 00 00 00 00 00  00 00 00 00              |............|</span><br><span class="line">00002b6c</span><br><span class="line">$ tail -n 2 testfile | hexdump -C</span><br><span class="line">00000000  36 35 35 33 34 0a 36 35  35 33 35 0a              |65534.65535.|</span><br><span class="line">0000000c</span><br><span class="line"></span><br><span class="line"><span class="comment">#delete zero last line in file3, you could get same md5sum</span></span><br><span class="line"></span><br><span class="line">$ md5sum  testfile </span><br><span class="line">43a1b3e2e25c9410daefdaa1d78678bf  testfile</span><br><span class="line">$ cat file1 file2 file3 | md5sum </span><br><span class="line">43a1b3e2e25c9410daefdaa1d78678bf  -</span><br></pre></td></tr></table></figure>
<h1 id="Test-copy-on-write"><a href="#Test-copy-on-write" class="headerlink" title="Test copy on write"></a>Test copy on write</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp testfile testfile-1</span><br><span class="line">$ ls -i testfile-1 </span><br><span class="line">22 testfile-1</span><br><span class="line">$ zdb -dddddddd <span class="built_in">test</span>-pool/ 22</span><br><span class="line">Dataset <span class="built_in">test</span>-pool [ZPL], ID 21, cr_txg 1, 1.14M, 11 objects, rootbp DVA[0]=&lt;0:8e00:200&gt; DVA[1]=&lt;1:14e00:200&gt; [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=631L/631P fill=11 cksum=b211ccbf0:3ff67b6c728:bddd89c4f0bb:183789ced23226</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">        22    2    16K   128K   385K   384K  100.00  ZFS plain file (K=inherit) (Z=inherit)</span><br><span class="line">                                        168   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 2</span><br><span class="line">	path	/testfile-1</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	mtime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	ctime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	crtime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	gen	631</span><br><span class="line">	mode	100644</span><br><span class="line">	size	382106</span><br><span class="line">	parent	4</span><br><span class="line">	links	1</span><br><span class="line">	pflags	40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:2600:200 1:2600:200 4000L/200P F=3 B=631/631</span><br><span class="line">               0  L0 1:adc00:20000 20000L/20000P F=1 B=631/631</span><br><span class="line">           20000  L0 1:cdc00:20000 20000L/20000P F=1 B=631/631</span><br><span class="line">           40000  L0 1:edc00:20000 20000L/20000P F=1 B=631/631</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000060000) size  384K</span><br><span class="line"></span><br><span class="line">$ </span><br><span class="line">$ <span class="built_in">echo</span> haha &gt;&gt; testfile-1 </span><br><span class="line">$ zdb -dddddddd <span class="built_in">test</span>-pool/ 22</span><br><span class="line">Dataset <span class="built_in">test</span>-pool [ZPL], ID 21, cr_txg 1, 1.14M, 11 objects, rootbp DVA[0]=&lt;0:5e00:200&gt; DVA[1]=&lt;1:6a00:200&gt; [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=637L/637P fill=11 cksum=bcaba3f58:44283babf90:cb3dd715909e:1a01d96e52cd6b</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">        22    2    16K   128K   385K   384K  100.00  ZFS plain file (K=inherit) (Z=inherit)</span><br><span class="line">                                        168   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 2</span><br><span class="line">	path	/testfile-1</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	mtime	Wed Dec 28 13:31:49 2016</span><br><span class="line">	ctime	Wed Dec 28 13:31:49 2016</span><br><span class="line">	crtime	Wed Dec 28 13:31:22 2016</span><br><span class="line">	gen	631</span><br><span class="line">	mode	100644</span><br><span class="line">	size	382111</span><br><span class="line">	parent	4</span><br><span class="line">	links	1</span><br><span class="line">	pflags	40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:5800:200 1:15400:200 4000L/200P F=3 B=637/637</span><br><span class="line">               0  L0 1:adc00:20000 20000L/20000P F=1 B=631/631</span><br><span class="line">           20000  L0 1:cdc00:20000 20000L/20000P F=1 B=631/631</span><br><span class="line">           40000  L0 0:7ca00:20000 20000L/20000P F=1 B=637/637 <span class="comment">### copy on write</span></span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000060000) size  384K</span><br></pre></td></tr></table></figure>

<h3 id="Find-the-physical-address"><a href="#Find-the-physical-address" class="headerlink" title="Find the physical address"></a>Find the physical address</h3><p>Because there are some different with solaris zfs, it ‘s not 4MiB in header.<br>I dd first 5MiB data from disk0.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk0  of=/tmp/<span class="built_in">test</span> bs=1M count=5</span><br><span class="line"></span><br><span class="line"><span class="comment"># Found the data in disk0</span></span><br><span class="line">$ hexdump -C /tmp/<span class="built_in">test</span> | more</span><br><span class="line">00418fc0  13 00 00 00 00 00 00 80  00 00 00 00 00 00 2e 66  |...............f|</span><br><span class="line">00418fd0  69 6c 65 33 2e 73 77 70  00 00 00 00 00 00 00 00  |ile3.swp........|</span><br><span class="line">00418fe0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|</span><br><span class="line">*</span><br><span class="line">00419000  30 0a 31 0a 32 0a 33 0a  34 0a 35 0a 36 0a 37 0a  |0.1.2.3.4.5.6.7.|</span><br><span class="line">00419010  38 0a 39 0a 31 30 0a 31  31 0a 31 32 0a 31 33 0a  |8.9.10.11.12.13.|</span><br><span class="line">00419020  31 34 0a 31 35 0a 31 36  0a 31 37 0a 31 38 0a 31  |14.15.16.17.18.1|</span><br><span class="line">00419030  39 0a 32 30 0a 32 31 0a  32 32 0a 32 33 0a 32 34  |9.20.21.22.23.24|</span><br><span class="line">00419040  0a 32 35 0a 32 36 0a 32  37 0a 32 38 0a 32 39 0a  |.25.26.27.28.29.|</span><br><span class="line"></span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print strtonum("0x"419000)&#125;'</span></span><br><span class="line">4296704</span><br><span class="line">$ $ awk <span class="string">'BEGIN&#123;print strtonum("0x"419000)/1024&#125;'</span></span><br><span class="line">4196</span><br><span class="line"></span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print strtonum("0x"28000)&#125;'</span></span><br><span class="line">163840</span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print strtonum("0x"20000)&#125;'</span></span><br><span class="line">131072</span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print (4296704-163840)/1024&#125;'</span></span><br><span class="line">4036</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk0 of=/tmp/<span class="built_in">test</span>-1 bs=1k skip=4196 count=1</span><br><span class="line">$ head -n 2 /tmp/<span class="built_in">test</span>-1 </span><br><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
<p>which means the offset from the begining of the physical vdev (starting after the vdev labels, plus the boot block), 4036KiB total in openzfs on linux<br>4132864 = 3f100 ?</p>
<h3 id="Compare-with-ext3"><a href="#Compare-with-ext3" class="headerlink" title="Compare with ext3"></a>Compare with ext3</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /tank/<span class="built_in">test</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=disk2 bs=1M count=100</span><br><span class="line">104857600 bytes (105 MB) copied, 0.0600031 s, 1.7 GB/s</span><br><span class="line"></span><br><span class="line">$ mkfs.ext3 -b 4096 /tank/<span class="built_in">test</span>/disk2</span><br><span class="line">mke2fs 1.41.12 (17-May-2010)</span><br><span class="line">/tank/<span class="built_in">test</span>/disk2 is not a block special device.</span><br><span class="line">Proceed anyway? (y,n) y</span><br><span class="line">Filesystem label=</span><br><span class="line">OS <span class="built_in">type</span>: Linux</span><br><span class="line">Block size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Fragment size=4096 (<span class="built_in">log</span>=2)</span><br><span class="line">Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">25600 inodes, 25600 blocks</span><br><span class="line">1280 blocks (5.00%) reserved <span class="keyword">for</span> the super user</span><br><span class="line">First data block=0</span><br><span class="line">Maximum filesystem blocks=29360128</span><br><span class="line">1 block group</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">25600 inodes per group</span><br><span class="line"></span><br><span class="line">Writing inode tables: <span class="keyword">done</span>                            </span><br><span class="line">Creating journal (1024 blocks): <span class="keyword">done</span></span><br><span class="line">Writing superblocks and filesystem accounting information: <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">This filesystem will be automatically checked every 24 mounts or</span><br><span class="line">180 days, whichever comes first.  Use tune2fs -c or -i to override.</span><br><span class="line"></span><br><span class="line">$ mount disk2 /mnt -o loop</span><br><span class="line">$ df -h</span><br><span class="line">Filesystem        Size  Used Avail Use% Mounted on</span><br><span class="line">/tank/<span class="built_in">test</span>/disk2   97M  4.5M   88M   5% /mnt</span><br><span class="line"></span><br><span class="line">$ cp -a /<span class="built_in">test</span>-pool/testfile ./</span><br><span class="line">$ filefrag -v /mnt/testfile </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">Filesystem cylinder groups is approximately 1</span><br><span class="line">File size of /mnt/testfile is 382109 (94 blocks, blocksize 4096)</span><br><span class="line"> ext logical physical expected length flags</span><br><span class="line">   0       0     1841              12 merged</span><br><span class="line">   1      12     1854     1853     82 merged,eof</span><br><span class="line">/mnt/testfile: 2 extents found, perfection would be 1 extent</span><br><span class="line"></span><br><span class="line">$ umount /mnt</span><br><span class="line">$ debugfs /tank/<span class="built_in">test</span>/disk2 </span><br><span class="line"> 1.41.12 (17-May-2010)</span><br><span class="line">debugfs:  stats</span><br><span class="line">Filesystem volume name:   &lt;none&gt;</span><br><span class="line">Last mounted on:          &lt;not available&gt;</span><br><span class="line">Filesystem UUID:          d6f9c3d7-e56b-433d-9830-69b15553029b</span><br><span class="line">Filesystem magic number:  0xEF53</span><br><span class="line">Filesystem revision <span class="comment">#:    1 (dynamic)</span></span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype sparse_super large_file</span><br><span class="line">Filesystem flags:         signed_directory_hash </span><br><span class="line">Default mount options:    (none)</span><br><span class="line">Filesystem state:         clean</span><br><span class="line">Errors behavior:          Continue</span><br><span class="line">Filesystem OS <span class="built_in">type</span>:       Linux</span><br><span class="line">Inode count:              25600</span><br><span class="line">Block count:              25600</span><br><span class="line">Reserved block count:     1280</span><br><span class="line">Free blocks:              23664</span><br><span class="line">Free inodes:              25588</span><br><span class="line">First block:              0</span><br><span class="line">Block size:               4096</span><br><span class="line">Fragment size:            4096</span><br><span class="line">Reserved GDT blocks:      6</span><br><span class="line">Blocks per group:         32768</span><br><span class="line">Fragments per group:      32768</span><br><span class="line">Inodes per group:         25600</span><br><span class="line">Inode blocks per group:   800</span><br><span class="line">Filesystem created:       Wed Dec 28 16:03:47 2016</span><br><span class="line">Last mount time:          Wed Dec 28 16:04:10 2016</span><br><span class="line">Last write time:          Wed Dec 28 16:04:52 2016</span><br><span class="line">Mount count:              1</span><br><span class="line">Maximum mount count:      24</span><br><span class="line">Last checked:             Wed Dec 28 16:03:47 2016</span><br><span class="line">Check interval:           15552000 (6 months)</span><br><span class="line">Next check after:         Mon Jun 26 16:03:47 2017</span><br><span class="line">Reserved blocks uid:      0 (user root)</span><br><span class="line">Reserved blocks gid:      0 (group root)</span><br><span class="line">First inode:              11</span><br><span class="line">Inode size:	          128</span><br><span class="line">Journal inode:            8</span><br><span class="line">Default directory <span class="built_in">hash</span>:   half_md4</span><br><span class="line">Directory Hash Seed:      58b4c828-2e74-4add-91bb-4007efd5187f</span><br><span class="line">Journal backup:           inode blocks</span><br><span class="line">Directories:              2</span><br><span class="line"> Group  0: block bitmap at 8, inode bitmap at 9, inode table at 10</span><br><span class="line">           23664 free blocks, 25588 free inodes, 2 used directories</span><br><span class="line"></span><br><span class="line">debugfs:  ls</span><br><span class="line"> 2  (12) .    2  (12) ..    11  (20) lost+found    12  (4052) testfile  </span><br><span class="line"></span><br><span class="line">debugfs:  show_inode_info testfile</span><br><span class="line">Inode: 12   Type: regular    Mode:  0644   Flags: 0x0</span><br><span class="line">Generation: 933995507    Version: 0x00000000</span><br><span class="line">User:     0   Group:     0   Size: 382109</span><br><span class="line">File ACL: 0    Directory ACL: 0</span><br><span class="line">Links: 1   Blockcount: 760</span><br><span class="line">Fragment:  Address: 0    Number: 0    Size: 0</span><br><span class="line">ctime: 0x58637206 -- Wed Dec 28 16:04:22 2016</span><br><span class="line">atime: 0x58637206 -- Wed Dec 28 16:04:22 2016</span><br><span class="line">mtime: 0x58637206 -- Wed Dec 28 16:04:22 2016</span><br><span class="line">BLOCKS:</span><br><span class="line">(0-11):1841-1852, (IND):1853, (12-93):1854-1935</span><br><span class="line">TOTAL: 95</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk2 of=/tmp/file-1 bs=4K count=12 skip=1841</span><br><span class="line"></span><br><span class="line">$ head -n 2 /tmp/file-1 </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ tail -n 2 /tmp/file-1 </span><br><span class="line">10042</span><br><span class="line">1004</span><br><span class="line"></span><br><span class="line"><span class="variable">$if</span>=/tank/<span class="built_in">test</span>/disk2 of=/tmp/file-2 bs=4K count=82 skip=1854</span><br><span class="line">$ head -n 2 /tmp/file-2</span><br><span class="line">3</span><br><span class="line">10044</span><br><span class="line">$ tail -n 2 /tmp/file-2 </span><br><span class="line">12</span><br><span class="line">$ tail -n 3 /tmp/file-2 </span><br><span class="line">65535</span><br><span class="line">12 <span class="comment"># I have echo 12 &gt;&gt; to testfile.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># delete zero line(last line) in file2</span></span><br><span class="line">$ cat /tmp/file-1 /tmp/file-2 | md5sum </span><br><span class="line">48688f1bef04b9b59905409e9226ba65  -</span><br><span class="line">$ md5sum /mnt/testfile </span><br><span class="line">48688f1bef04b9b59905409e9226ba65  /mnt/testfile</span><br><span class="line"></span><br><span class="line"><span class="comment">#Modify file</span></span><br><span class="line"></span><br><span class="line">$ filefrag -v /mnt/testfile </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">Filesystem cylinder groups is approximately 1</span><br><span class="line">File size of /mnt/testfile is 382109 (94 blocks, blocksize 4096)</span><br><span class="line"> ext logical physical expected length flags</span><br><span class="line">   0       0     4104              12 merged</span><br><span class="line">   1      12     4117     4116     82 merged,eof</span><br><span class="line">/mnt/testfile: 2 extents found, perfection would be 1 extent</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk2 of=/tmp/disk-2-1 bs=4k count=4105</span><br><span class="line">4105+0 records <span class="keyword">in</span></span><br><span class="line">4105+0 records out</span><br><span class="line">16814080 bytes (17 MB) copied, 0.161878 s, 104 MB/s</span><br><span class="line">You have new mail <span class="keyword">in</span> /var/spool/mail/root</span><br><span class="line">$ tail /tmp/disk-2-1 </span><br><span class="line">1032</span><br><span class="line">1033</span><br><span class="line">1034</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line"></span><br><span class="line">1<span class="variable">$dd</span> <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk2 of=/tmp/disk-2-2 bs=4k skip=4105 </span><br><span class="line">21495+0 records <span class="keyword">in</span></span><br><span class="line">21495+0 records out</span><br><span class="line">88043520 bytes (88 MB) copied, 0.66258 s, 133 MB/s</span><br><span class="line">$ head /tmp/disk-2-2 </span><br><span class="line">041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 1111111111 &gt;&gt; /tmp/disk-2-1</span><br><span class="line"></span><br><span class="line">$ cat /tmp/disk-2-1 &gt; /tank/<span class="built_in">test</span>/disk2</span><br><span class="line">$ du -hs /tank/<span class="built_in">test</span>/disk2</span><br><span class="line">17M	/tank/<span class="built_in">test</span>/disk2</span><br><span class="line">$ cat /tmp/disk-2-2 &gt;&gt; /tank/<span class="built_in">test</span>/disk2</span><br><span class="line">$ du -hs /tank/<span class="built_in">test</span>/disk2 </span><br><span class="line">100M	/tank/<span class="built_in">test</span>/disk2</span><br><span class="line">$ fsck /tank/<span class="built_in">test</span>/disk2</span><br><span class="line">fsck from util-linux-ng 2.17.2</span><br><span class="line">e2fsck 1.41.12 (17-May-2010)</span><br><span class="line">/tank/<span class="built_in">test</span>/disk2: clean, 12/25600 files, 1936/25600 blocks</span><br><span class="line"></span><br><span class="line">$ mount /tank/<span class="built_in">test</span>/disk2 /mnt -o loop</span><br><span class="line"></span><br><span class="line">$ head -n 1042 /mnt/testfile | tail -n 2</span><br><span class="line">1040</span><br><span class="line">11111111111</span><br><span class="line"><span class="comment"># got it , file has changed.</span></span><br></pre></td></tr></table></figure>

<h3 id="Same-with-openzfs"><a href="#Same-with-openzfs" class="headerlink" title="Same with openzfs"></a>Same with openzfs</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-bash-4.1<span class="comment"># zpool destroy test-pool</span></span><br><span class="line">$ zpool create <span class="built_in">test</span>-pool /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">$ zpool create <span class="built_in">test</span>-pool -o ashift=9 /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">$ rm -f testfile ;<span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; testfile; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">0040f6a0  ff ff ff a6 50 00 00 00  00 00 00 00 00 00 00 00  |....P...........|</span><br><span class="line">0040f6b0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|</span><br><span class="line">*</span><br><span class="line">0040f800  30 0a 31 0a 32 0a 33 0a  34 0a 35 0a 36 0a 37 0a  |0.1.2.3.4.5.6.7.|</span><br><span class="line">0040f810  38 0a 39 0a 31 30 0a 31  31 0a 31 32 0a 31 33 0a  |8.9.10.11.12.13.|</span><br><span class="line">0040f820  31 34 0a 31 35 0a 31 36  0a 31 37 0a 31 38 0a 31  |14.15.16.17.18.1|</span><br><span class="line">0040f830  39 0a 32 30 0a 32 31 0a  32 32 0a 32 33 0a 32 34  |9.20.21.22.23.24|</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ ls -li /<span class="built_in">test</span>-pool/testfile </span><br><span class="line">7 -rw-r--r-- 1 root root 382106 12月 28 17:31 /<span class="built_in">test</span>-pool/testfile</span><br><span class="line"></span><br><span class="line">$ zdb -dddddd <span class="built_in">test</span>-pool/ 7</span><br><span class="line">Dataset <span class="built_in">test</span>-pool [ZPL], ID 21, cr_txg 1, 404K, 7 objects, rootbp DVA[0]=&lt;0:71c00:200&gt; DVA[1]=&lt;0:71e00:200&gt; [L0 DMU objset] fletcher4 lz4 LE contiguous unique double size=800L/200P birth=7L/7P fill=7 cksum=d25a82905:4fd05e01a15:f9b25d4668de:21890ed460fd11</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">         7    2    16K   128K   385K   384K  100.00  ZFS plain file (K=inherit) (Z=inherit)</span><br><span class="line">                                        168   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 2</span><br><span class="line">	path	/testfile</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Wed Dec 28 17:31:11 2016</span><br><span class="line">	mtime	Wed Dec 28 17:31:13 2016</span><br><span class="line">	ctime	Wed Dec 28 17:31:13 2016</span><br><span class="line">	crtime	Wed Dec 28 17:31:11 2016</span><br><span class="line">	gen	7</span><br><span class="line">	mode	100644</span><br><span class="line">	size	382106</span><br><span class="line">	parent	4</span><br><span class="line">	links	1</span><br><span class="line">	pflags	40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:4800:200 0:4a00:200 4000L/200P F=3 B=7/7</span><br><span class="line">               0  L0 0:f800:20000 20000L/20000P F=1 B=7/7</span><br><span class="line">           20000  L0 0:2f800:20000 20000L/20000P F=1 B=7/7</span><br><span class="line">           40000  L0 0:4f800:20000 20000L/20000P F=1 B=7/7</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000060000) size  384K</span><br><span class="line"></span><br><span class="line">$ 40f800=4257792</span><br><span class="line">$ f800=63488</span><br><span class="line">$ awk <span class="string">'BEGIN&#123;print (4257792-63488)/1024&#125;'</span></span><br><span class="line">4096</span><br></pre></td></tr></table></figure>
<p>Header is 4MiB, and offset 63488 begin to write real data</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'BEGIN&#123;print strtonum("0x40f800")/1024&#125;'</span></span><br><span class="line">4158</span><br><span class="line"></span><br><span class="line">$ zdb -R <span class="built_in">test</span>-pool 0:f800:20000:r &gt; /tmp/file1</span><br><span class="line">-bash-4.1<span class="comment"># head /tmp/file1 </span></span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line"><span class="comment"># modify /tmp/file1</span></span><br><span class="line">$ head -n 4 /tmp/file1</span><br><span class="line"><span class="comment">#0</span></span><br><span class="line"><span class="comment">#1</span></span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk0 of=/tmp/disk0-1 bs=1k count=4158</span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk0 of=/tmp/disk0-2 bs=1k skip=4158 count=1</span><br><span class="line">$ dd <span class="keyword">if</span>=/tank/<span class="built_in">test</span>/disk0 of=/tmp/disk0-3 bs=1k skip=4159</span><br><span class="line"></span><br><span class="line">$ head /tmp/disk0-2 </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line">-bash-4.1<span class="comment"># tail /tmp/disk0-2 </span></span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">28</span><br><span class="line"></span><br><span class="line">$ ls -l /tmp/disk0-*</span><br><span class="line">-rw-r--r-- 1 root root   4257792 12月 28 19:59 /tmp/disk0-1</span><br><span class="line">-rw-r--r-- 1 root root      1024 12月 28 19:59 /tmp/disk0-2</span><br><span class="line">-rw-r--r-- 1 root root 100598784 12月 28 20:03 /tmp/disk0-3</span><br><span class="line"></span><br><span class="line"><span class="comment"># modify disk0-2</span></span><br><span class="line">$ vi /tmp/disk0-2 </span><br><span class="line">-bash-4.1<span class="comment"># head -n 2 /tmp/disk0-2 </span></span><br><span class="line"><span class="comment">#0</span></span><br><span class="line"><span class="comment">#1</span></span><br><span class="line"></span><br><span class="line">$ cp /tmp/disk0-1 /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">$ cat /tmp/disk0-2 &gt;&gt; /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">$ cat /tmp/disk0-3 &gt;&gt; /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">$ zpool import -d /tank/<span class="built_in">test</span>/ <span class="built_in">test</span>-pool</span><br><span class="line">cannot import <span class="string">'test-pool'</span>: I/O error</span><br><span class="line">	Destroy and re-create the pool from</span><br><span class="line">	a backup <span class="built_in">source</span>.</span><br><span class="line"></span><br><span class="line">$ zdb -l /tank/<span class="built_in">test</span>/disk0 </span><br><span class="line">zdb -l /tank/<span class="built_in">test</span>/disk0</span><br><span class="line">--------------------------------------------</span><br><span class="line">LABEL 0</span><br><span class="line">--------------------------------------------</span><br><span class="line">    version: 5000</span><br><span class="line">    name: <span class="string">'test-pool'</span></span><br><span class="line">    state: 0</span><br><span class="line">    txg: 1277</span><br><span class="line">    pool_guid: 2913619627256210242</span><br><span class="line">    errata: 0</span><br><span class="line">    hostname: <span class="string">'nas-57-11.local'</span></span><br><span class="line">    top_guid: 14729225611287826913</span><br><span class="line">    guid: 14729225611287826913</span><br><span class="line">    vdev_children: 1</span><br><span class="line">    vdev_tree:</span><br><span class="line">        <span class="built_in">type</span>: <span class="string">'file'</span></span><br><span class="line">        id: 0</span><br><span class="line">        guid: 14729225611287826913</span><br><span class="line">        path: <span class="string">'/tank/test/disk0'</span></span><br><span class="line">        metaslab_array: 35</span><br><span class="line">        metaslab_shift: 24</span><br><span class="line">        ashift: 9</span><br><span class="line">        asize: 100139008</span><br><span class="line">        is_log: 0</span><br><span class="line">        create_txg: 4</span><br><span class="line">    features_for_read:</span><br><span class="line">        com.delphix:hole_birth</span><br><span class="line">        com.delphix:embedded_data</span><br><span class="line">--------------------------------------------</span><br><span class="line">LABEL 1</span><br><span class="line">--------------------------------------------</span><br><span class="line">    version: 5000</span><br><span class="line">    name: <span class="string">'test-pool'</span></span><br><span class="line">    state: 0</span><br><span class="line">    txg: 1277</span><br><span class="line">    pool_guid: 2913619627256210242</span><br><span class="line">    errata: 0</span><br><span class="line">    hostname: <span class="string">'nas-57-11.local'</span></span><br><span class="line">    top_guid: 14729225611287826913</span><br><span class="line">    guid: 14729225611287826913</span><br><span class="line">    vdev_children: 1</span><br><span class="line">    vdev_tree:</span><br><span class="line">        <span class="built_in">type</span>: <span class="string">'file'</span></span><br><span class="line">        id: 0</span><br><span class="line">        guid: 14729225611287826913</span><br><span class="line">        path: <span class="string">'/tank/test/disk0'</span></span><br><span class="line">        metaslab_array: 35</span><br><span class="line">        metaslab_shift: 24</span><br><span class="line">        ashift: 9</span><br><span class="line">        asize: 100139008</span><br><span class="line">        is_log: 0</span><br><span class="line">        create_txg: 4</span><br><span class="line">    features_for_read:</span><br><span class="line">        com.delphix:hole_birth</span><br><span class="line">        com.delphix:embedded_data</span><br><span class="line">--------------------------------------------</span><br><span class="line">LABEL 2</span><br><span class="line">--------------------------------------------</span><br><span class="line">failed to unpack label 2</span><br><span class="line">--------------------------------------------</span><br><span class="line">LABEL 3</span><br><span class="line">--------------------------------------------</span><br><span class="line">failed to unpack label 3</span><br><span class="line"></span><br><span class="line">$ zpool import -N -o <span class="built_in">readonly</span>=on -f -d /tank/<span class="built_in">test</span>/ -F -T 1277</span><br><span class="line">   pool: <span class="built_in">test</span>-pool</span><br><span class="line">     id: 2913619627256210242</span><br><span class="line">  state: FAULTED</span><br><span class="line"> status: The pool metadata is corrupted.</span><br><span class="line"> action: The pool cannot be imported due to damaged devices or data.</span><br><span class="line">	The pool may be active on another system, but can be imported using</span><br><span class="line">	the <span class="string">'-f'</span> flag.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-72</span><br><span class="line"> config:</span><br><span class="line"></span><br><span class="line">	<span class="built_in">test</span>-pool           FAULTED  corrupted data</span><br><span class="line">	  /tank/<span class="built_in">test</span>/disk0  ONLINE</span><br></pre></td></tr></table></figure>

<h3 id="Raidz-size"><a href="#Raidz-size" class="headerlink" title="Raidz size"></a><a href="http://blog.gjpvanwesten.nl/2014/08/part-iv-how-much-space-do-you-lose-with.html" target="_blank" rel="noopener">Raidz size</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'BEGIN&#123;print 128/7&#125;'</span></span><br><span class="line">18.2857</span><br><span class="line">awk <span class="string">'BEGIN&#123;print 18.2857/20&#125;'</span></span><br><span class="line">0.914285</span><br><span class="line">awk <span class="string">'BEGIN&#123;print 18.2857/18.5&#125;'</span></span><br><span class="line">0.988416</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>zfs</tag>
      </tags>
  </entry>
  <entry>
    <title>Dump sparse file data segment</title>
    <url>/2016/12/18/sparse/</url>
    <content><![CDATA[<h3 id="Test-file"><a href="#Test-file" class="headerlink" title="Test file"></a>Test file</h3><p>ubuntu 16.04<br>ext4 file system</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rm data-A</span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..1500&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; data-A ; <span class="keyword">done</span></span><br><span class="line">$ filefrag -v data-A </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of data-A is 6394 (2 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:        0..       0:    3464588..   3464588:      1:            </span><br><span class="line">   1:        1..       1:          0..         0:      1:    3464589: last,unknown_loc,delalloc,eof</span><br><span class="line">$ dd <span class="keyword">if</span>=data-A of=data-A-1 bs=4096 count=1</span><br><span class="line">$ dd <span class="keyword">if</span>=data-A of=data-A-2 bs=4096 count=1 skip=1</span><br><span class="line">$ head data-A-2</span><br><span class="line">1</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ cat data-A-1 data-A-2 | md5sum</span><br><span class="line">30ef7f11d29b8ad47542e97ed9d2a398  -</span><br><span class="line">$ cat data-A | md5sum</span><br><span class="line">30ef7f11d29b8ad47542e97ed9d2a398  -</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="Test-sparse-file"><a href="#Test-sparse-file" class="headerlink" title="Test sparse file"></a>Test sparse file</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rm ./data-A;<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..1500&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; data-A ; <span class="keyword">done</span></span><br><span class="line">$ filefrag -v data-A </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of data-A is 6393 (2 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:        0..       1:          0..         1:      2:             last,unknown_loc,delalloc,eof</span><br><span class="line">data-A: 1 extent found</span><br><span class="line"></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=<span class="built_in">test</span> bs=4096 count=0 seek=25 <span class="comment">#write 100K sparse file</span></span><br><span class="line">$ cat data-A &gt;&gt; <span class="built_in">test</span></span><br><span class="line">$ ls -lhs </span><br><span class="line">total 16K</span><br><span class="line">8.0K -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 6.3K 12月 19 14:04 data-A</span><br><span class="line">8.0K -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 107K 12月 19 14:06 <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">$ df -h / </span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda2       101G   35G   61G  37% /</span><br><span class="line"></span><br><span class="line">$ sudo dumpe2fs /dev/sda2 | grep -i <span class="string">"Block size"</span></span><br><span class="line">dumpe2fs 1.42.13 (17-May-2015)</span><br><span class="line">Block size:               4096</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ filefrag -v data-A </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of data-A is 6393 (2 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:        0..       1:    3713508..   3713509:      2:             last,eof</span><br><span class="line">data-A: 1 extent found</span><br><span class="line"></span><br><span class="line">$ sudo dd <span class="keyword">if</span>=/dev/sda2 of=<span class="built_in">test</span>-1 bs=4096 count=1 skip=3713508</span><br><span class="line">$ head <span class="built_in">test</span>-1 </span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line"></span><br><span class="line">$ tail <span class="built_in">test</span>-1</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">104</span><br><span class="line"></span><br><span class="line">$ cat <span class="built_in">test</span>-1 | md5sum</span><br><span class="line">27260c41d34d5a01f5fba073f9059a90  -</span><br><span class="line"></span><br><span class="line">$ filefrag -v <span class="built_in">test</span></span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span> is 108793 (27 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:       25..      26:    3713510..   3713511:      2:             last,eof</span><br><span class="line"><span class="built_in">test</span>: 1 extent found</span><br><span class="line"></span><br><span class="line">$ sudo dd <span class="keyword">if</span>=/dev/sda2 of=<span class="built_in">test</span><span class="_">-s</span>-1 bs=4096 count=1 skip=3713510</span><br><span class="line"></span><br><span class="line">$ md5sum <span class="built_in">test</span>-*</span><br><span class="line">27260c41d34d5a01f5fba073f9059a90  <span class="built_in">test</span>-1</span><br><span class="line">27260c41d34d5a01f5fba073f9059a90  <span class="built_in">test</span><span class="_">-s</span>-1</span><br><span class="line"></span><br><span class="line">$ sudo dd <span class="keyword">if</span>=/dev/sda2 of=<span class="built_in">test</span><span class="_">-s</span>-2 bs=4096 count=1 skip=3713511</span><br><span class="line"></span><br><span class="line"><span class="comment"># the checksum difference between test-s-2 and test</span></span><br><span class="line">$ hexdump <span class="built_in">test</span><span class="_">-s</span>-2 | tail</span><br><span class="line">0000890 3431 3038 310a 3834 0a31 3431 3238 310a</span><br><span class="line">00008a0 3834 0a33 3431 3438 310a 3834 0a35 3431</span><br><span class="line">00008b0 3638 310a 3834 0a37 3431 3838 310a 3834</span><br><span class="line">00008c0 0a39 3431 3039 310a 3934 0a31 3431 3239</span><br><span class="line">00008d0 310a 3934 0a33 3431 3439 310a 3934 0a35</span><br><span class="line">00008e0 3431 3639 310a 3934 0a37 3431 3839 310a</span><br><span class="line">00008f0 3934 0a39 3531 3030 000a 0000 0000 0000</span><br><span class="line">0000900 0000 0000 0000 0000 0000 0000 0000 0000</span><br><span class="line">*</span><br><span class="line">0001000</span><br><span class="line"></span><br><span class="line">$ hexdump <span class="built_in">test</span> | tail</span><br><span class="line">001a870 0a33 3431 3437 310a 3734 0a35 3431 3637</span><br><span class="line">001a880 310a 3734 0a37 3431 3837 310a 3734 0a39</span><br><span class="line">001a890 3431 3038 310a 3834 0a31 3431 3238 310a</span><br><span class="line">001a8a0 3834 0a33 3431 3438 310a 3834 0a35 3431</span><br><span class="line">001a8b0 3638 310a 3834 0a37 3431 3838 310a 3834</span><br><span class="line">001a8c0 0a39 3431 3039 310a 3934 0a31 3431 3239</span><br><span class="line">001a8d0 310a 3934 0a33 3431 3439 310a 3934 0a35</span><br><span class="line">001a8e0 3431 3639 310a 3934 0a37 3431 3839 310a</span><br><span class="line">001a8f0 3934 0a39 3531 3030 000a               </span><br><span class="line">001a8f9</span><br></pre></td></tr></table></figure>
<p>There are some of zero fill in the file bottom.<br>astersik (*) !(means same as the line above)[<a href="http://superuser.com/questions/494245/what-does-an-asterisk-mean-in-hexdump-output]" target="_blank" rel="noopener">http://superuser.com/questions/494245/what-does-an-asterisk-mean-in-hexdump-output]</a></p>
<h3 id="dd-update"><a href="#dd-update" class="headerlink" title="dd update"></a>dd update</h3><p>operate single byte</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=data-A of=data-A-1Byte bs=1  count=1 ibs=1 obs=1</span><br><span class="line">$ dd <span class="keyword">if</span>=data-A of=data-A-skip-1byte bs=1  skip=2 count=1 ibs=1 obs=1 iflag=count_bytes iflag=skip_bytes</span><br><span class="line"></span><br><span class="line">$ ls -ls | grep -i by</span><br><span class="line"></span><br><span class="line"><span class="comment">#first column is KiB , 4 is 4KiB</span></span><br><span class="line"><span class="comment">#sixth column is byte , 1 byte</span></span><br><span class="line"></span><br><span class="line">  4 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>      1 12月 19 15:23 data-A-1Byte</span><br><span class="line">  4 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>      1 12月 19 15:22 data-A-skip-1byte</span><br><span class="line"></span><br><span class="line">$ cat data-A-1Byte</span><br><span class="line">1 $ cat data-A-skip-1byte</span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">$ ls -ls <span class="built_in">test</span> data-A </span><br><span class="line">8 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>   6393 12月 19 14:54 data-A</span><br><span class="line">8 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 108793 12月 19 15:30 <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">$  dd <span class="keyword">if</span>=<span class="built_in">test</span> of=<span class="built_in">test</span>-data bs=1 count=6393 skip=102400 iflag=skip_bytes ibs=1 obs=1</span><br><span class="line">$ $ sha1sum   data-A <span class="built_in">test</span>-data <span class="built_in">test</span></span><br><span class="line">8f8374832cb6536f580eef3b316ad08293a47617  data-A</span><br><span class="line">8f8374832cb6536f580eef3b316ad08293a47617  <span class="built_in">test</span>-data</span><br><span class="line">03e31af4c89c9003ee0f43e49a65ad2e24367024  <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line">$ hexdump data-A | tail</span><br><span class="line">0001870 0a33 3431 3437 310a 3734 0a35 3431 3637</span><br><span class="line">0001880 310a 3734 0a37 3431 3837 310a 3734 0a39</span><br><span class="line">0001890 3431 3038 310a 3834 0a31 3431 3238 310a</span><br><span class="line">00018a0 3834 0a33 3431 3438 310a 3834 0a35 3431</span><br><span class="line">00018b0 3638 310a 3834 0a37 3431 3838 310a 3834</span><br><span class="line">00018c0 0a39 3431 3039 310a 3934 0a31 3431 3239</span><br><span class="line">00018d0 310a 3934 0a33 3431 3439 310a 3934 0a35</span><br><span class="line">00018e0 3431 3639 310a 3934 0a37 3431 3839 310a</span><br><span class="line">00018f0 3934 0a39 3531 3030 000a               </span><br><span class="line">00018f9</span><br><span class="line">$ hexdump <span class="built_in">test</span>-data | tail</span><br><span class="line">0001870 0a33 3431 3437 310a 3734 0a35 3431 3637</span><br><span class="line">0001880 310a 3734 0a37 3431 3837 310a 3734 0a39</span><br><span class="line">0001890 3431 3038 310a 3834 0a31 3431 3238 310a</span><br><span class="line">00018a0 3834 0a33 3431 3438 310a 3834 0a35 3431</span><br><span class="line">00018b0 3638 310a 3834 0a37 3431 3838 310a 3834</span><br><span class="line">00018c0 0a39 3431 3039 310a 3934 0a31 3431 3239</span><br><span class="line">00018d0 310a 3934 0a33 3431 3439 310a 3934 0a35</span><br><span class="line">00018e0 3431 3639 310a 3934 0a37 3431 3839 310a</span><br><span class="line">00018f0 3934 0a39 3531 3030 000a               </span><br><span class="line">00018f9</span><br><span class="line">$ hexdump <span class="built_in">test</span> | tail</span><br><span class="line">001a870 0a33 3431 3437 310a 3734 0a35 3431 3637</span><br><span class="line">001a880 310a 3734 0a37 3431 3837 310a 3734 0a39</span><br><span class="line">001a890 3431 3038 310a 3834 0a31 3431 3238 310a</span><br><span class="line">001a8a0 3834 0a33 3431 3438 310a 3834 0a35 3431</span><br><span class="line">001a8b0 3638 310a 3834 0a37 3431 3838 310a 3834</span><br><span class="line">001a8c0 0a39 3431 3039 310a 3934 0a31 3431 3239</span><br><span class="line">001a8d0 310a 3934 0a33 3431 3439 310a 3934 0a35</span><br><span class="line">001a8e0 3431 3639 310a 3934 0a37 3431 3839 310a</span><br><span class="line">001a8f0 3934 0a39 3531 3030 000a               </span><br><span class="line">001a8f9</span><br></pre></td></tr></table></figure>

<h3 id="Expand-sparse-file"><a href="#Expand-sparse-file" class="headerlink" title="Expand sparse file"></a>Expand sparse file</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ truncate -s +256 test_bak</span><br><span class="line">$ ls -ls  test_bak </span><br><span class="line">108 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 109049 12月 19 16:36 test_bak</span><br><span class="line">$ ls -ls  <span class="built_in">test</span></span><br><span class="line">8 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 108793 12月 19 15:30 <span class="built_in">test</span></span><br><span class="line">$ <span class="built_in">echo</span> $((109049-108793))</span><br><span class="line">256</span><br><span class="line"></span><br><span class="line"><span class="comment"># not sparse</span></span><br><span class="line"><span class="variable">$fallocate</span> -o 1M -l 1M 1M-file</span><br></pre></td></tr></table></figure>

<h3 id="About-fallocate"><a href="#About-fallocate" class="headerlink" title="About fallocate"></a>About <a href="https://pelican.craoc.fr/sparse-file.html" target="_blank" rel="noopener">fallocate</a></h3><h4 id="collapse-range"><a href="#collapse-range" class="headerlink" title="collapse-range"></a>collapse-range</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ rm <span class="built_in">test</span>;<span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..60000&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span> &gt;&gt; <span class="built_in">test</span>; <span class="keyword">done</span></span><br><span class="line">$ ls -ls <span class="built_in">test</span></span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 348896 12月 19 17:03 <span class="built_in">test</span></span><br><span class="line">$ cp <span class="built_in">test</span> test2</span><br><span class="line">$ fallocate --collapse-range --offset 4096 --length 8192  test2 <span class="comment"># From 4096, delete length from 4096 delete 8192 length bytes</span></span><br><span class="line">$ dd <span class="keyword">if</span>=<span class="built_in">test</span> of=<span class="built_in">test</span>-dd-4096-8192 bs=4096 count=2 skip=4096 iflag=skip_bytes <span class="comment"># make sure delete length</span></span><br><span class="line"></span><br><span class="line">head <span class="built_in">test</span>-dd-4096-8192 </span><br><span class="line">041</span><br><span class="line">1042</span><br><span class="line">1043</span><br><span class="line">1044</span><br><span class="line">1045</span><br><span class="line">1046</span><br><span class="line">1047</span><br><span class="line">1048</span><br><span class="line">1049</span><br><span class="line">1050</span><br><span class="line"></span><br><span class="line">$ tail <span class="built_in">test</span>-dd-4096-8192</span><br><span class="line">2670</span><br><span class="line">2671</span><br><span class="line">2672</span><br><span class="line">2673</span><br><span class="line">2674</span><br><span class="line">2675</span><br><span class="line">2676</span><br><span class="line">2677</span><br><span class="line">2678</span><br><span class="line">267</span><br><span class="line"></span><br><span class="line">$ head -n 1045 test2 | tail</span><br><span class="line">1035</span><br><span class="line">1036</span><br><span class="line">1037</span><br><span class="line">1038</span><br><span class="line">1039</span><br><span class="line">1040</span><br><span class="line">19   <span class="comment"># 1 041 and 267 9</span></span><br><span class="line">2680</span><br><span class="line">2681</span><br><span class="line">2682</span><br><span class="line"></span><br><span class="line"><span class="comment"># test in sparse file</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=<span class="built_in">test</span>-sparse bs=1M count=0 seek=1</span><br><span class="line">$ ls -lsh</span><br><span class="line">total 344K</span><br><span class="line">344K -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 341K 12月 19 17:03 <span class="built_in">test</span></span><br><span class="line">   0 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 1.0M 12月 19 17:23 <span class="built_in">test</span>-sparse</span><br><span class="line">$ cat <span class="built_in">test</span> &gt;&gt; <span class="built_in">test</span>-sparse</span><br><span class="line">$ ls -ls</span><br><span class="line">total 688</span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>  348896 12月 19 17:03 <span class="built_in">test</span></span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 1397472 12月 19 17:23 <span class="built_in">test</span>-sparse</span><br><span class="line"></span><br><span class="line">$ fallocate --collapse-range --offset 4096 --length 8192 <span class="built_in">test</span>-sparse </span><br><span class="line">$ ls -ls</span><br><span class="line">total 688</span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>  348896 12月 19 17:03 <span class="built_in">test</span></span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 1389280 12月 19 17:24 <span class="built_in">test</span>-sparse</span><br><span class="line"></span><br><span class="line">$ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1397472 (342 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:      254..     341:    3803866..   3803953:     88:             last,eof</span><br><span class="line"></span><br><span class="line">$ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1389280 (340 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:      252..     339:    3803866..   3803953:     88:             last,eof</span><br><span class="line"><span class="built_in">test</span>-sparse: 1 extent found</span><br></pre></td></tr></table></figure>

<h4 id="zero-range"><a href="#zero-range" class="headerlink" title="zero-range"></a>zero-range</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cp  <span class="built_in">test</span> <span class="built_in">test</span>-sparse </span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=<span class="built_in">test</span>-sparse bs=1M count=0 seek=1</span><br><span class="line">0+0 records <span class="keyword">in</span></span><br><span class="line">0+0 records out</span><br><span class="line">0 bytes copied, 0.000132755 s, 0.0 kB/s</span><br><span class="line">1d [<span class="built_in">test</span>:/tmp/<span class="built_in">test</span>/test2] 130 $ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1048576 (256 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:        0..       2:     589896..    589898:      3:             last,unwritten</span><br><span class="line"><span class="built_in">test</span>-sparse: 1 extent found</span><br><span class="line">1d [<span class="built_in">test</span>:/tmp/<span class="built_in">test</span>/test2] $ cat <span class="built_in">test</span> &gt;&gt; <span class="built_in">test</span>-sparse </span><br><span class="line">1d [<span class="built_in">test</span>:/tmp/<span class="built_in">test</span>/test2] $ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1397472 (342 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:        0..       2:     589896..    589898:      3:             unwritten</span><br><span class="line">   1:      256..     341:          0..        85:     86:     589899: last,unknown_loc,delalloc,eof</span><br><span class="line"><span class="built_in">test</span>-sparse: 2 extents found</span><br><span class="line">$  head <span class="built_in">test</span>-sparse </span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line">$ fallocate --zero-range --offset 1048576 --length 128 <span class="built_in">test</span>-sparse </span><br><span class="line">$ head <span class="built_in">test</span>-sparse </span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line"></span><br><span class="line">$ fallocate --zero-range --offset 1048576 --length 348896  <span class="built_in">test</span>-sparse</span><br><span class="line"><span class="comment"># clear all</span></span><br></pre></td></tr></table></figure>

<h4 id="punch-hole"><a href="#punch-hole" class="headerlink" title="punch-hole"></a>punch-hole</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat <span class="built_in">test</span> &gt;&gt; <span class="built_in">test</span>-sparse </span><br><span class="line">$ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1397472 (342 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:      256..     341:          0..        85:     86:             last,unknown_loc,delalloc,eof</span><br><span class="line"><span class="built_in">test</span>-sparse: 1 extent found</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="variable">$cat</span> <span class="built_in">test</span> &gt;&gt; <span class="built_in">test</span>-sparse </span><br><span class="line">$ ls -ls</span><br><span class="line">total 688</span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span>  348896 12月 19 17:29 <span class="built_in">test</span></span><br><span class="line">344 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 1397472 12月 19 17:53 <span class="built_in">test</span>-sparse</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> ((4096*256)=1048576 ;((4096*84))=344064</span><br><span class="line"></span><br><span class="line"><span class="variable">$fallocate</span> --punch-hole --offset 1048576 --length 344064  <span class="built_in">test</span>-sparse </span><br><span class="line">1d [<span class="built_in">test</span>:/tmp/<span class="built_in">test</span>/test2] $ filefrag -v <span class="built_in">test</span>-sparse </span><br><span class="line">Filesystem <span class="built_in">type</span> is: ef53</span><br><span class="line">File size of <span class="built_in">test</span>-sparse is 1397472 (342 blocks of 4096 bytes)</span><br><span class="line"> ext:     logical_offset:        physical_offset: length:   expected: flags:</span><br><span class="line">   0:      340..     341:    3802396..   3802397:      2:             last,eof</span><br><span class="line"><span class="built_in">test</span>-sparse: 1 extent found</span><br><span class="line"></span><br><span class="line">$ ls -ls <span class="built_in">test</span>-sparse </span><br><span class="line">8 -rw-rw-r-- 1 <span class="built_in">test</span> <span class="built_in">test</span> 1397472 12月 19 17:54 <span class="built_in">test</span>-sparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># space has recycled</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>file</category>
      </categories>
      <tags>
        <tag>sparse file</tag>
      </tags>
  </entry>
  <entry>
    <title>openzfs tips</title>
    <url>/2016/12/04/openzfs-tips/</url>
    <content><![CDATA[<h3 id="How-many-devices-in-single-raidz-zpool"><a href="#How-many-devices-in-single-raidz-zpool" class="headerlink" title="How many devices in single raidz zpool"></a>How many devices in single raidz zpool</h3><p>2^n + parity<br>raidz2 2^3+2=10 2^4+2=18 , we have 14 x HDDs in single raidz2 zpool in production<br>raidz3 2^3+3=11 2^4+3=19 , next we will choice 19 x HDDs in single raidz3</p>
<a id="more"></a>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10TBxHDD  ashift12    ashift9(TiB)  a12&#x2F;a9   raw(TB)    raw compare with 9   raw compare with 12</span><br><span class="line">8+2         68           71         95.77%   100           71%  (recommand)      68%</span><br><span class="line">10+2        81           88         92%      120           73.3%                 68.5%</span><br><span class="line">12+2       102          106         96.22%   140           75.7%a                72.8%</span><br><span class="line">14+2       116          123         94.3%    160           76.8%                 72.5%</span><br><span class="line">16+2       141          141         100%     180           78.3% (recommand)     78.3%</span><br><span class="line">8+3         71           71         100%     110           64.5%                 64.5%</span><br><span class="line">11+3        90           97         92.78%   140           69.2%                 64.2%</span><br><span class="line">12+3        97          105         92.38%   150           70%                   64.6%</span><br><span class="line">16+3       134          141         95%      190           74.2% (recommand)     70.5%</span><br><span class="line">17+3       141          149         94.6%    200           74.5%                 70.5%</span><br><span class="line">18+3       148          156         94.8%    210           74.2%                 70.47%</span><br><span class="line">19+3       155          166         93.3%    220           75.4%                 70.45%</span><br><span class="line">20+3       162          175         92.5%    230           76%                   70.43%</span><br><span class="line"></span><br><span class="line">12TBxHDD  ashift12    ashift9(TiB)  a12&#x2F;a9   raw(TB)   raw compare with 9       raw compare with 12</span><br><span class="line">5+3         49	         53         92.4%    96          55.2%                   51%</span><br><span class="line">6+2         61           64         92.4%    96          66.6%                   63.5%</span><br><span class="line">8+2         81           85         95.2%    120         70.8%                   67.5%</span><br><span class="line">16+2       170          170         100%     216         78.7%  (recommand)    78.7%</span><br><span class="line">8+3         85           85         100%     132         64.3%                 64.3%</span><br><span class="line">11+3       108          116         93.1%    168         69%                   64.2%</span><br><span class="line">12+3       116          126         92%      180         70%                   64.4%</span><br><span class="line">15+3       139          157         88.5%    216         72.6%                 64.3%</span><br><span class="line">16+3       161          170         94.7%    228         74.5% (recommand)     70.6%</span><br><span class="line">20+3       195          210         91.42%   276         76%                   70.6%</span><br><span class="line">21+3       203          220         92.2%    288         76.3%                 70.4%</span><br><span class="line"></span><br><span class="line">12TB SAS device</span><br><span class="line">5U 84 bay 12TB</span><br><span class="line">(16+2)*4+(6+2)+4 hs &#x3D; 170*4+61 &#x3D; 741TiB&#x2F;1008TB &#x3D; 73.4% (TiB&#x2F;TB) (ashfit 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*4+(8+2) &#x3D; 161*4 + 61 &#x3D; 705TiB&#x2F;1008TB &#x3D; 69.9% (ashfit 12, raidz3)</span><br><span class="line">(16+3)*4+(8+2) &#x3D; 170*4 + 64 &#x3D; 744TiB&#x2F;1008TB &#x3D; 73.8% (ashfit 9, raidz3)</span><br><span class="line"></span><br><span class="line">4U 60 bay 12TB</span><br><span class="line">(16+2)*3+6hs&#x3D;170*3&#x2F;720&#x3D;70.8% (ashfit 12,raidz2,6 hot spare)</span><br><span class="line">(16+3)*3+3hs&#x3D;161*3&#x2F;720&#x3D;67%   (ashfit 12, raidz3, 3 hot spare)</span><br><span class="line">(16+3)*3+3hs&#x3D;170*3&#x2F;720&#x3D;70.8% (ashfit 9, raidz3, 3 hot spare)</span><br><span class="line"></span><br><span class="line">4U 90 bay 12TB</span><br><span class="line">(16+2)*4+(12+2)+4 hs&#x3D;(170*4+102)&#x2F;1080 &#x3D; 72.4% (ahsift 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*4+(8+2)&#x3D;170*4+81&#x2F;1080 &#x3D; 70.4% (ahsift 12, raidz3)</span><br><span class="line">(16+3)*4+(8+2)&#x3D;170*4+85&#x2F;1080 &#x3D; 70.8% (ahsift 9, raidz3)</span><br><span class="line"></span><br><span class="line">4U 102 bay</span><br><span class="line">(16+2)*5+(6+2)+4hs &#x3D;(170*5+61)&#x2F;1224 &#x3D; 74.4% (ashfit 12, raidz2, 4 hot spare)</span><br><span class="line">(16+3)*5+ 43(4+2)+1hs &#x3D;((161*5)+43)&#x2F;1224 &#x3D; 69.2% (ashfit 12, raidz3, 1 hot spare)</span><br><span class="line">(16+3)*5+ 43(4+2)+1hs &#x3D;((170*5)+43)&#x2F;1224 &#x3D; 72.9% (ashfit 9, raidz3, 1 hot spare)</span><br></pre></td></tr></table></figure>

<p>not make sure</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[381558.364210] sd 17:0:132:0: [sg134] CDB: Test Unit Ready 00 00 00 00 00 00</span><br><span class="line">[381558.365100] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381558.365961] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381558.368328] sd 17:0:132:0: task abort: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381558.369172] scsi target17:0:132: attempting target reset! scmd(ffff8803abf18a80)</span><br><span class="line">[381558.370012] sd 17:0:132:0: [sg134] CDB: Inquiry 12 01 83 00 f0 00</span><br><span class="line">[381558.370845] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381558.371660] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381558.631612] scsi target17:0:132: target reset: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381568.608006] sd 17:0:132:0: attempting task abort! scmd(ffff8803abf18a80)</span><br><span class="line">[381568.608802] sd 17:0:132:0: [sg134] CDB: Test Unit Ready 00 00 00 00 00 00</span><br><span class="line">[381568.609600] scsi target17:0:132: handle(0x0097), sas_address(0x5000c500955e5855), phy(23)</span><br><span class="line">[381568.610369] scsi target17:0:132: enclosure_logical_id(0x50050cc11ac01572), slot(45)</span><br><span class="line">[381568.612506] sd 17:0:132:0: task abort: SUCCESS scmd(ffff8803abf18a80)</span><br><span class="line">[381568.613253] mpt2sas0: attempting host reset! scmd(ffff8803abf18a80)</span><br><span class="line">[381568.613989] sd 17:0:132:0: [sg134] CDB: Inquiry 12 01 83 00 f0 00</span><br><span class="line">[381568.614761] mpt2sas0: sending diag reset !!</span><br><span class="line">[381569.611617] mpt2sas0: diag reset: SUCCESS</span><br><span class="line">[381569.747649] mpt2sas0: LSISAS2308: FWVersion(20.00.07.00), ChipRevision(0x05), BiosVersion(07.27.01.01)</span><br><span class="line">[381569.748356] mpt2sas0: Protocol=(Initiator,Target), Capabilities=(TLR,EEDP,Snapshot Buffer,Diag Trace Buffer,Task Set Full,NCQ)</span><br><span class="line">[381569.749818] mpt2sas0: sending port <span class="built_in">enable</span> !!</span><br><span class="line">[381573.276922] WARNING: Pool <span class="string">'ost_81'</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.392645] WARNING: Pool <span class="string">'ost_85'</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.420579] WARNING: Pool <span class="string">'ost_83'</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381573.445523] WARNING: Pool <span class="string">'ost_87'</span> has encountered an uncorrectable I/O failure and has been suspended.</span><br><span class="line"></span><br><span class="line">[381576.820367] mpt2sas0: port <span class="built_in">enable</span>: SUCCESS</span><br><span class="line">[381576.821174] mpt2sas0: search <span class="keyword">for</span> end-devices: start</span><br><span class="line">[381576.823066] scsi target17:0:0: handle(0x000e), sas_addr(0x50050cc11938283e), enclosure logical id(0x50050cc11ac018e1), slot(0)</span><br><span class="line">[381576.824595] scsi target17:0:1: handle(0x000f), sas_addr(0x5000cca2735b5bd5), enclosure logical id(0x50050cc11ac018e1), slot(76)</span><br><span class="line">[381576.826169] scsi target17:0:2: handle(0x0010), sas_addr(0x5000cca2735d80cd), enclosure logical id(0x50050cc11ac018e1), slot(79)a</span><br><span class="line">[381576.827819] scsi target17:0:3: handle(0x0011), sas_addr(0x5000cca2732f2521), enclosure logical id(0x50050cc11ac018e1), slot(82)</span><br><span class="line">[381576.829497] scsi target17:0:4: handle(0x0012), sas_addr(0x5000cca27355f089), enclosure logical id(0x50050cc11ac018e1), slot(83)</span><br></pre></td></tr></table></figure>

<h3 id="NVME-SSD-test-with-openzfs-0-7"><a href="#NVME-SSD-test-with-openzfs-0-7" class="headerlink" title="NVME SSD test with openzfs 0.7"></a>NVME SSD test with openzfs 0.7</h3><h3 id="6x-Intel-P3520"><a href="#6x-Intel-P3520" class="headerlink" title="[6x Intel P3520]"></a>[6x Intel P3520]</h3><p>Before modify default parameter. there are only 1.xGB/s read or write. too slow…….</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">parted /dev/nvme0n1 p</span><br><span class="line">Model: Unknown (unknown)</span><br><span class="line">Disk /dev/nvme0n1: 2000GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags:</span><br><span class="line"></span><br><span class="line">Number  Start   End     Size    File system  Name  Flags</span><br><span class="line"> 1      1049kB  1900GB  1900GB               disk1</span><br><span class="line"></span><br><span class="line">zpool destroy tank01</span><br><span class="line">zpool create tank01 -o ashift=13 -O recordsize=1M /dev/nvme&#123;0..5&#125;n1p1</span><br><span class="line"></span><br><span class="line"><span class="comment">#test mds</span></span><br><span class="line">$ zpool create -f tank01 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=12 -O recordsize=32k -O secondarycache=none -O logbias=throughput  /dev/sd&#123;b..d&#125;</span><br><span class="line"></span><br><span class="line">xattr=on stores extended attributes <span class="keyword">in</span> hidden sub directories, <span class="built_in">which</span> can require multiple lookups when accessing a file</span><br><span class="line">The alternative, xattr=sa, stores extended attributes <span class="keyword">in</span> inodes, resulting <span class="keyword">in</span> less IO requests when extended attributes are <span class="keyword">in</span> use</span><br><span class="line"></span><br><span class="line">The logbias property – You can use this property to provide a hint to ZFS about handling synchronous requests <span class="keyword">for</span> a specific dataset. If logbias is <span class="built_in">set</span> to latency, ZFS uses the pool<span class="string">'s separate log devices, if any, to handle the requests at low latency. If logbias is set to throughput, ZFS does not use the pool'</span>s separate <span class="built_in">log</span> devices. Instead, ZFS optimizes synchronous operations <span class="keyword">for</span> global pool throughput and efficient use of resources.</span><br><span class="line"></span><br><span class="line">Does it mean <span class="keyword">in</span> all flash env, you<span class="string">'d better set logbias=throughput</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">              capacity     operations     bandwidth</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank01       132G  10.2T      0  16.7K      0  5.79G</span></span><br><span class="line"><span class="string">tank01       146G  10.2T      0  12.4K      0  7.22G</span></span><br><span class="line"><span class="string">tank01       161G  10.2T      0  10.9K      0  7.41G</span></span><br><span class="line"><span class="string">tank01       174G  10.1T      0  12.1K      0  7.18G</span></span><br></pre></td></tr></table></figure>

<h3 id="the-old-optimize-with-lustre"><a href="#the-old-optimize-with-lustre" class="headerlink" title="the old optimize with lustre"></a><a href="http://wiki.lustre.org/images/4/48/ZFS-As-Backend-File-System_Paciucci.pdf" target="_blank" rel="noopener">the old optimize with lustre</a></h3><p><code>There are some issue in this config that means the engineer not make zfs struct clearly</code><br>If you are disbale compression function, zio_taskq_batch_pct could must be limit</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /etc/modprobe.d/zfs.conf</span><br><span class="line">options zfs zfs_prefetch_disable=1</span><br><span class="line">options zfs metaslab_debug_unload=1</span><br><span class="line">options zfs zfs_arc_max=$(awk <span class="string">'$0~/MemTotal/&#123;printf "%.0f \n", $2*1024*0.6&#125;'</span> /proc/meminfo) <span class="comment">#60%, 3/5</span></span><br><span class="line">options zfs zfs_arc_meta_limit=$(awk <span class="string">'$0~/MemTotal/&#123;printf "%.0f \n", $2*1024*0.75&#125;'</span> /proc/meminfo) <span class="comment">#MDS only,75%</span></span><br><span class="line">options zfs zfs_dirty_data_max=$(awk <span class="string">'$0~/MemTotal/&#123;printf "%.0f \n", $2*1024*0.15&#125;'</span> /proc/meminfo) <span class="comment">#15%</span></span><br><span class="line">options zfs zfs_vdev_async_write_min_active=5 <span class="comment"># depends your hdd number of your volume ,SSD could be higher</span></span><br><span class="line">options zfs zfs_vdev_async_write_max_active=15</span><br><span class="line">options zfs zfs_vdev_sync_read_min_active=16 <span class="comment"># depends your hdd number of your volume ,SSD could be higher</span></span><br><span class="line">options zfs zfs_vdev_sync_read_max_active=16</span><br><span class="line">options zfs zfs_vdev_async_write_active_min_dirty_percent=20</span><br><span class="line">options zfs zfs_vdev_scheduler=deadline <span class="comment">#noop for SSD</span></span><br></pre></td></tr></table></figure>
<p>zfs_vdev_scheduler not support mq-deadline<br>since ZFS has its own I/O scheduler, using a simple scheduler can result in more consistent performance<br>expected: noop, cfq, bfq, and deadline</p>
<p><a href="http://lustre.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf" target="_blank" rel="noopener">Tutorial: How to install, tune and Monitor a ZFS based Lustre file system</a></p>
<h4 id="zfs-txg-timeout"><a href="#zfs-txg-timeout" class="headerlink" title="zfs_txg_timeout"></a>zfs_txg_timeout</h4><p>The last zfs_txg_history txg commits are available in /proc/spl/kstat/zfs/POOL_NAME/txgs</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/spl/kstat/zfs/tank/txgs</span><br><span class="line">25 0 0x01 83 9296 11320470741 2382694537989353</span><br><span class="line">txg      birth            state ndirty       nread        nwritten     reads    writes   otime        qtime        wtime        stime</span><br><span class="line">13974499 2382281791630216 C     1949696      0            1433600      0        526      4999917952   44691        34328        382573295</span><br><span class="line">13974500 2382286791548168 C     1998848      0            1453056      0        552      5000003695   46223        44384        386415831</span><br><span class="line">13974501 2382291791551863 C     1458176      0            1245696      0        539      4999926718   45704        33119        386636075</span><br><span class="line">13974502 2382296791478581 C     950272       0            887296       0        492      4999981264   44784        34896        336542457</span><br><span class="line">13974503 2382301791459845 C     1146880      0            1244672      0        546      4999967135   67884        46074        346304560</span><br><span class="line">13974504 2382306791426980 C     4177920      0            2828288      0        600      4999946876   46380        45891        423776127</span><br></pre></td></tr></table></figure>

<h4 id="L2ARC-and-slog"><a href="#L2ARC-and-slog" class="headerlink" title="L2ARC and slog"></a>L2ARC and slog</h4><p>l2arc_write_max<br>If the cache devices can sustain the write workload, increasing the rate of cache device fill when workloads generate new data at a rate higher than l2arc_write_max can increase L2ARC hit rate</p>
<h1 id="max-write-speed-to-l2arc"><a href="#max-write-speed-to-l2arc" class="headerlink" title="max write speed to l2arc"></a>max write speed to l2arc</h1><h1 id="tradeoff-between-write-read-and-durability-of-ssd"><a href="#tradeoff-between-write-read-and-durability-of-ssd" class="headerlink" title="tradeoff between write/read and durability of ssd (?)"></a>tradeoff between write/read and durability of ssd (?)</h1><h1 id="default-8-1024-1024-8-MB-s"><a href="#default-8-1024-1024-8-MB-s" class="headerlink" title="default : 8 * 1024 * 1024  = 8 MB/s"></a>default : 8 * 1024 * 1024  = 8 MB/s</h1><h1 id="setting-here-500-1024-1024"><a href="#setting-here-500-1024-1024" class="headerlink" title="setting here : 500 * 1024 * 1024"></a>setting here : 500 * 1024 * 1024</h1><p>options zfs l2arc_write_max=524288000</p>
<p>l2arc_headroom</p>
<h1 id="number-of-max-device-writes-to-precache"><a href="#number-of-max-device-writes-to-precache" class="headerlink" title="number of max device writes to precache"></a>number of max device writes to precache</h1><h1 id="default-2"><a href="#default-2" class="headerlink" title="default : 2"></a>default : 2</h1><p>options zfs l2arc_headroom=12</p>
<p>zfs_immediate_write_sz</p>
<h1 id="default-32768"><a href="#default-32768" class="headerlink" title="default : 32768"></a>default : 32768</h1><p>options zfs zfs_immediate_write_sz=131072</p>
<p>zil_slog_limit</p>
<h1 id="default-1024-1024-1mb"><a href="#default-1024-1024-1mb" class="headerlink" title="default : 1024*1024 = 1mb"></a>default : 1024*1024 = 1mb</h1><p>options zfs zil_slog_limit=536870912</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ arcstat.py -f <span class="string">"time,read,hit%,hits,miss%,miss,arcsz,c"</span> 1</span><br><span class="line"></span><br><span class="line">$ cat /proc/spl/kstat/zfs/arcstats</span><br><span class="line">$ arcstat.py 2</span><br><span class="line">$ arc_summary.py | grep <span class="string">"L2 ARC Breakdown"</span> -A 2</span><br><span class="line">L2 ARC Breakdown:                               846.25m</span><br><span class="line">        Hit Ratio:                      0.47%   3.98m</span><br><span class="line">        Miss Ratio:                     99.53%  842.27m</span><br></pre></td></tr></table></figure>


<h4 id="zfs-vdev-scheduler"><a href="#zfs-vdev-scheduler" class="headerlink" title="zfs_vdev_scheduler"></a>zfs_vdev_scheduler</h4><p>Set the IO scheduler used by ZFS. Both noop and deadline, which implement simple scheduling algorithms, are good options, as the storage daemon is run by a single Linux user.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> deadline &gt; /sys/module/zfs/parameters/zfs_vdev_scheduler</span><br></pre></td></tr></table></figure>

<h4 id="zfs-read-chunk-size"><a href="#zfs-read-chunk-size" class="headerlink" title="zfs_read_chunk_size"></a>zfs_read_chunk_size</h4><p>Data is read by ZFS in data chunks of a certain size.  it depends your zfs setting</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1310720 &gt; /sys/module/zfs/parameters/zfs_read_chunk_size</span><br></pre></td></tr></table></figure>

<h4 id="zfs-max-recordsize"><a href="#zfs-max-recordsize" class="headerlink" title="zfs_max_recordsize"></a>zfs_max_recordsize</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 4194304 &gt; /sys/module/zfs/parameters/zfs_max_recordsize</span><br></pre></td></tr></table></figure>

<h4 id="zfs-vdev-aggregation-limit"><a href="#zfs-vdev-aggregation-limit" class="headerlink" title="zfs_vdev_aggregation_limit"></a>zfs_vdev_aggregation_limit</h4><p>ZFS is able to aggregate small IO operations that handle neighboring or overlapping data into larger operations, in order to reduce the number of IOPs</p>
<p>Setting zfs_vdev_aggregation_limit = 0 effectively disables aggregation by ZFS</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 262144 &gt; /sys/module/zfs/parameters/zfs_vdev_aggregation_limit</span><br></pre></td></tr></table></figure>


<h4 id="HDD-prefetch"><a href="#HDD-prefetch" class="headerlink" title="HDD prefetch"></a>HDD prefetch</h4><p>Set this option to 0 to activate data prefetching if you are using spinning disks. Set it to 1 to disable it if you are using flash devices like SSDs.</p>
<p>increase HDD performance a lot</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zfs_prefetch_disable&#x3D;0 #Must enable for HDD</span><br></pre></td></tr></table></figure>

<h4 id="Vdev-cache"><a href="#Vdev-cache" class="headerlink" title="Vdev cache"></a>Vdev cache</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">options zfs zfs_vdev_cache_size=1310720</span><br><span class="line">options zfs zfs_vdev_cache_max=131072</span><br><span class="line">options zfs zfs_vdev_cache_bshift=17</span><br></pre></td></tr></table></figure>

<h4 id="txg-timeout"><a href="#txg-timeout" class="headerlink" title="txg_timeout"></a>txg_timeout</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/module/zfs/parameters/zfs_txg_timeout</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">every 5s will <span class="keyword">do</span> transaction sync</span><br><span class="line"></span><br><span class="line">$ zdb -lu /dev/sdc1 | grep <span class="string">"txg ="</span> -c</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line">128 x 5s= 640s</span><br><span class="line"></span><br><span class="line">If <span class="keyword">in</span> 640s, power off the server imported the zpool ASAP, reboot single server to check the zpool status.</span><br><span class="line">If out of 640s, restart another server, keep the single import server, and backup your data, hope it not be panic.</span><br><span class="line">Need a zfs fsck tool.</span><br><span class="line"></span><br><span class="line">should I improve the value ??? Dangerous !!! impact performance,  enough memory ?</span><br><span class="line">$ <span class="built_in">echo</span> 10 &gt;/sys/module/zfs/parameters/zfs_txg_timeout</span><br></pre></td></tr></table></figure>

<h4 id="zpool-sync-mode"><a href="#zpool-sync-mode" class="headerlink" title="zpool sync mode"></a>zpool sync mode</h4><p>sync=standard : sync writes are written 2 times (first to LOG, second as normal write every ~5 seconds), async write are written only once (every ~5 seconds)<br>sync=always : sync writes and async writes are written 2 times (first to LOG, second as normal write every ~5 seconds).<br>sync=disabled : sync writes and async writes are written only once (every ~5 seconds)<br>Of course ZFS can flush it write cache between 5 second period but the biggest flush is every ~5 seconds.</p>
<h4 id="Limit-memory-usage"><a href="#Limit-memory-usage" class="headerlink" title="Limit memory usage"></a>Limit memory usage</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> xxx &gt; /sys/module/zfs/parameters/zfs_arc_max</span><br><span class="line">cat xxx &gt; /sys/module/zfs/parameters/zfs_arc_min</span><br><span class="line"></span><br><span class="line">cat /etc/modprobe.d/zfs.conf <span class="comment"># Min 4096MB / Max 8192MB limit</span></span><br><span class="line">options zfs zfs_arc_min=4294967296</span><br><span class="line">options zfs zfs_arc_max=8589934592</span><br><span class="line"></span><br><span class="line">modprobe zfs zfs_arc_min=4294967296</span><br></pre></td></tr></table></figure>

<h4 id="zfs-multihost-history"><a href="#zfs-multihost-history" class="headerlink" title="zfs_multihost_history"></a>zfs_multihost_history</h4><p>The pool multihost multimodifier protection (MMP) subsystem can record historical updates in the /proc/spl/kstat/zfs/POOL_NAME/multihost</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /proc/spl/kstat/zfs/ost_0/multihost</span><br><span class="line">47 0 0x01 109 9592 181591887850 4241294257803975</span><br><span class="line">id         txg        timestamp  error  duration   mmp_delay    vdev_guid                vdev_label vdev_path</span><br><span class="line">88663237   43205944   1556725492      0     259182    409397766 5775269556738108556      0          /dev/disk/by-id/scsi-35000c500a670c847-part1</span><br><span class="line">88663238   43205944   1556725492      0    1651942    406910425 10040726243797242983     1          /dev/disk/by-id/scsi-35000c500a670c6cb-part1</span><br><span class="line">88663239   43205944   1556725492      0     262443    404453220 85871317386613310        0          /dev/disk/by-id/scsi-35000c500a65ada5f-part1</span><br><span class="line">88663240   43205944   1556725492      0   16196129    401993428 14057198631110435259     1          /dev/disk/by-id/scsi-35000c500a6753f53-part1</span><br><span class="line">88663241   43205944   1556725493      0     148851    399688210 15150783868311748605     2          /dev/disk/by-id/scsi-35000c500a670c73b-part1</span><br><span class="line">88663242   43205944   1556725493      0     213149    397151149 14018438879105052775     1          /dev/disk/by-id/scsi-35000c500a670cbc7-part1</span><br><span class="line">88663243   43205944   1556725493      0     916612    394759789 12962037773541489706     2          /dev/disk/by-id/scsi-35000c500a65afba7-part1</span><br></pre></td></tr></table></figure>

<p><a href="https://www.svennd.be/tuning-of-zfs-module/" target="_blank" rel="noopener">reference</a></p>
<h3 id="zpool-could-not-mount"><a href="#zpool-could-not-mount" class="headerlink" title="[zpool could not mount]"></a>[zpool could not mount]</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zpool <span class="built_in">set</span> cachefile=none tank</span><br><span class="line">$ zpool import -N -o cachefile=none -d /dev/disk/by-id tank01</span><br><span class="line">$ zfs mount -o ro tank01</span><br><span class="line"></span><br><span class="line">$ zpool import -Nm tank</span><br><span class="line"></span><br><span class="line"><span class="comment"># very dangerours, please backup all data</span></span><br><span class="line">$ zpool import -fFX -R /tmp/tank ost_11</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Security script</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Just check, not take action</span></span><br><span class="line">pool=<span class="string">"tank"</span></span><br><span class="line"><span class="comment">#find a vdev of pool</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> line1</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  vdev=<span class="variable">$line1</span></span><br><span class="line">  zdb -lu <span class="variable">$vdev</span> |grep <span class="string">"txg "</span> |cut -d <span class="string">" "</span> -f 3 &gt; /tmp/txg</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'echo ----'</span><span class="variable">$line1</span><span class="string">'-----'</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> <span class="built_in">read</span> LINE;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">'echo zdb -e '</span><span class="variable">$pool</span><span class="string">' -d -t '</span><span class="variable">$LINE</span></span><br><span class="line">    <span class="built_in">echo</span> zdb -e <span class="variable">$pool</span> -d -t <span class="variable">$LINE</span></span><br><span class="line">  <span class="keyword">done</span> &lt; /tmp/txg</span><br><span class="line"></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">'echo ----'</span><span class="variable">$vdev</span><span class="string">'--finish-----'</span></span><br><span class="line"><span class="keyword">done</span> &lt; zpool_devs</span><br></pre></td></tr></table></figure>

<h4 id="Try-zpool-T-parameter"><a href="#Try-zpool-T-parameter" class="headerlink" title="Try zpool -T parameter"></a><a href="https://github.com/zfsonlinux/zfs/issues/2831" target="_blank" rel="noopener">Try zpool -T parameter</a></h4><p>You’ll need to make this one line change and rebuild the module. After which you’ll be able to use the -T option. This effectively disables the logic which prevents you from using uberblocks which are older than the label.</p>
<p>import from txg, like import snapshot timestamp</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">diff --git a/module/zfs/vdev_label.c b/module/zfs/vdev_label.c</span><br><span class="line">index 1c2f00f..509e812 100644</span><br><span class="line">--- a/module/zfs/vdev_label.c</span><br><span class="line">+++ b/module/zfs/vdev_label.c</span><br><span class="line">@@ -471,7 +471,7 @@ retry:</span><br><span class="line">                        <span class="keyword">if</span> ((error || label_txg == 0) &amp;&amp; !config) &#123;</span><br><span class="line">                                config = label;</span><br><span class="line">                                <span class="built_in">break</span>;</span><br><span class="line">-                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &lt;= txg &amp;&amp; label_txg &gt; best_txg) &#123;</span><br><span class="line">+                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &gt; best_txg) &#123;</span><br><span class="line">                                best_txg = label_txg;</span><br><span class="line">                                nvlist_free(config);</span><br><span class="line">                                config = fnvlist_dup(label);</span><br></pre></td></tr></table></figure>

<h4 id="Add-skip-error-by-some-parameters-in-ZOL-0-8"><a href="#Add-skip-error-by-some-parameters-in-ZOL-0-8" class="headerlink" title="Add skip error by some parameters in ZOL 0.8"></a>Add skip error by some parameters in ZOL 0.8</h4><p>spa_load_verify_data<br>At the risk of data integrity, to speed extreme import of large pool<br>If this parameter is set to 0, the traversal skips non-metadata blocks. It can be toggled once the import has started to stop or start the traversal of non-metadata blocks.</p>
<p>spa_load_verify_metadata<br>At the risk of data integrity, to speed extreme import of large pool<br>If this parameter is set to 0, the traversal is not performed. It can be toggled once the import has started to stop or start the traversal</p>
<h4 id="zfs-vol"><a href="#zfs-vol" class="headerlink" title="zfs vol"></a>zfs vol</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs create -V 40T -o volblocksize=128k tank/zvol</span><br><span class="line">ls -l /dev/zvol/tank/zvol</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">### mount zfs</span></span><br><span class="line">```bash</span><br><span class="line">pool=tank</span><br><span class="line">zfs <span class="built_in">set</span> canmount=on <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> mountpoint=/mnt <span class="variable">$pool</span></span><br><span class="line">zfs mount <span class="variable">$pool</span></span><br><span class="line">rm -f /mnt/xxx/xxx/xxx</span><br><span class="line">zfs umount <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> canmount=off</span><br></pre></td></tr></table></figure>

<h3 id="zfs-event"><a href="#zfs-event" class="headerlink" title="zfs event"></a>zfs event</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zpool events -c <span class="comment"># clean events</span></span><br><span class="line">$ zpool import tank</span><br><span class="line">$ zpool events -v</span><br><span class="line">$ zpool events -v &gt; zpool_import_tank</span><br></pre></td></tr></table></figure>

<h3 id="Compression"><a href="#Compression" class="headerlink" title="Compression"></a>Compression</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> compression=lz4 tank</span><br><span class="line">zfs <span class="built_in">set</span> compression=gzip-9 tank</span><br></pre></td></tr></table></figure>
<h3 id="Xattr"><a href="#Xattr" class="headerlink" title="Xattr"></a>Xattr</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> xattr=sa tank</span><br></pre></td></tr></table></figure>

<h3 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> acltype=posixacl tank/gentoo/home</span><br><span class="line">zfs get acltype tank/gentoo/home</span><br></pre></td></tr></table></figure>

<h3 id="Rmount"><a href="#Rmount" class="headerlink" title="Rmount"></a>Rmount</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> mountpoint=/var/lib/mysql tank/mysql</span><br><span class="line">zfs umount tank/mysql</span><br><span class="line">zfs mount tank/mysql</span><br></pre></td></tr></table></figure>

<h3 id="Zpool-rename"><a href="#Zpool-rename" class="headerlink" title="Zpool rename"></a>Zpool rename</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zpool <span class="built_in">export</span> app</span><br><span class="line">zpool import app apps</span><br></pre></td></tr></table></figure>

<h3 id="Detach-hdd-from-mirror"><a href="#Detach-hdd-from-mirror" class="headerlink" title="Detach hdd from mirror"></a>Detach hdd from mirror</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zpool detach tank sdz <span class="comment">#detach zda from mirror-17</span></span><br><span class="line">only sdal <span class="keyword">in</span> mirror-17, and mirror-17 is missing</span><br><span class="line"></span><br><span class="line">          mirror-16  ONLINE       0     0     0</span><br><span class="line">            sdai     ONLINE       0     0     0</span><br><span class="line">            sdaj     ONLINE       0     0     0</span><br><span class="line">          sdal       ONLINE       0     0     0</span><br><span class="line">          mirror-18  ONLINE       0     0     0</span><br><span class="line">            sdam     ONLINE       0     0     0</span><br><span class="line">            sdan     ONLINE       0     0     0</span><br></pre></td></tr></table></figure>

<h3 id="Attach-one-hdd-to-mirror"><a href="#Attach-one-hdd-to-mirror" class="headerlink" title="Attach one hdd to mirror"></a>Attach one hdd to mirror</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zpool attach tank sdal sdz</span><br><span class="line">          mirror-17  ONLINE       0     0     0</span><br><span class="line">            sdal     ONLINE       0     0     0</span><br><span class="line">            sdz      ONLINE       0     0     0  (resilvering)</span><br></pre></td></tr></table></figure>

<h3 id="sub-zpool"><a href="#sub-zpool" class="headerlink" title="sub zpool"></a>sub zpool</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs create tank/<span class="built_in">test</span></span><br></pre></td></tr></table></figure>

<h3 id="Quota"><a href="#Quota" class="headerlink" title="Quota"></a>Quota</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> quota=1024G tank/<span class="built_in">test</span></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">zfs <span class="built_in">set</span> quota=none tank</span><br></pre></td></tr></table></figure>

<h3 id="user-quota"><a href="#user-quota" class="headerlink" title="user quota"></a>user quota</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> userquota@nagios=2G tank</span><br><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/tank01/<span class="built_in">test</span>/nagios bs=1M</span><br><span class="line">dd: writing /tank/<span class="built_in">test</span>/nagios:Disk quota exceeded</span><br><span class="line"></span><br><span class="line"><span class="comment"># zfs get userquota@nagios</span></span><br><span class="line">NAME            PROPERTY          VALUE             SOURCE</span><br><span class="line">tank01          userquota@nagios  2G                <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># group</span></span><br><span class="line">zfs <span class="built_in">set</span> groupquota@staff=10G tank/staff/admins</span><br></pre></td></tr></table></figure>

<h3 id="Add-and-remove-hotspare"><a href="#Add-and-remove-hotspare" class="headerlink" title="Add and remove hotspare"></a>Add and remove hotspare</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zpool add tank spare c1t4d0 c1t5d0</span><br><span class="line">zpool remove tank c1t5d0</span><br></pre></td></tr></table></figure>

<h3 id="Clear-zfs-label"><a href="#Clear-zfs-label" class="headerlink" title="Clear zfs label"></a><a href="https://icesquare.com/wordpress/freebsdhow-to-remove-zfs-meta-data/" target="_blank" rel="noopener">Clear zfs label</a></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ zpool labelclear &#x2F;dev&#x2F;sdxxx</span><br><span class="line">or</span><br><span class="line">$ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;sdXX bs&#x3D;512 count&#x3D;10</span><br><span class="line">$ dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;&#x2F;dev&#x2F;sdXX bs&#x3D;512 seek&#x3D;$(( $(blockdev --getsz &#x2F;dev&#x2F;sdXX) - 4096 ))</span><br></pre></td></tr></table></figure>

<h3 id="Auto-rebuild-in-0-7-x"><a href="#Auto-rebuild-in-0-7-x" class="headerlink" title="Auto rebuild in 0.7.x"></a>Auto rebuild in 0.7.x</h3><p><a href="https://github.com/zfsonlinux/zfs/issues/2449" target="_blank" rel="noopener">Autoreplace not working</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">zpool <span class="built_in">set</span> autoreplace=on tank</span><br></pre></td></tr></table></figure>

<h3 id="Show-status"><a href="#Show-status" class="headerlink" title="Show status"></a>Show status</h3><p>Mapping /dev/shm/zil_cache to loop0p1</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=/dev/shm/zil_cache bs=1M count=4096</span><br><span class="line">$ losetup -v -f /dev/shm/zil_cache</span><br><span class="line">$ losetup -a</span><br><span class="line">/dev/loop0: [0018]:55156303 (/dev/shm/zil_cache)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add zil by mirror</span></span><br><span class="line">$ zpool add tank <span class="built_in">log</span> mirror /dev/loop0p1 /dev/loop0p2, because it <span class="string">'s test file from /dev/shm</span></span><br><span class="line"><span class="string">$ zpool add tank log /dev/loop0p1</span></span><br><span class="line"><span class="string">$ zpool iostat -v 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pool                                            alloc   free   read  write   read  write</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank                                            24.4G  1.30T      0  8.74K      0   575M</span></span><br><span class="line"><span class="string">  scsi-3600605b005811bf01db36f957f619f26-part1  24.4G  1.30T      0  5.50K      0   448M</span></span><br><span class="line"><span class="string">logs                                                -      -      -      -      -      -</span></span><br><span class="line"><span class="string">  loop0p1                                        102M  3.87G      0  3.23K      0   127M</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -r 1</span></span><br><span class="line"><span class="string">tank_7         sync_read    sync_write    async_read    async_write      scrub</span></span><br><span class="line"><span class="string">req_size      ind    agg    ind    agg    ind    agg    ind    agg    ind    agg</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">512             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4K              0      0      8      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">64K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">128K           63      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">256K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">512K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16M             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -L -v -q 2</span></span><br><span class="line"><span class="string">              capacity     operations     bandwidth    syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank         161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">  raidz2     161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sda         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdb         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdc         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdd         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sde         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdf         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdg         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdi         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdk         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdl         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -w 2</span></span><br><span class="line"><span class="string">tank         total_wait     disk_wait    syncq_wait    asyncq_wait</span></span><br><span class="line"><span class="string">latency      read  write   read  write   read  write   read  write  scrub   trim</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">1ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">3ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">7ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">15ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">31ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">63ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">127ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">255ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">511ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">65us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">131us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">262us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">524us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">33ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">67ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">134ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">268ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">536ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">17s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">34s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">68s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">137s            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ cat /proc/spl/kstat/zfs/ost_86/dmu_tx_assign</span></span><br><span class="line"><span class="string">75 1 0x01 41 1968 259828173343194 262983153792140</span></span><br><span class="line"><span class="string">name                            type data</span></span><br><span class="line"><span class="string">1 ns                            4    0</span></span><br><span class="line"><span class="string">2 ns                            4    0</span></span><br><span class="line"><span class="string">4 ns                            4    0</span></span><br><span class="line"><span class="string">8 ns                            4    0</span></span><br><span class="line"><span class="string">16 ns                           4    0</span></span><br><span class="line"><span class="string">32 ns                           4    0</span></span><br><span class="line"><span class="string">64 ns                           4    0</span></span><br><span class="line"><span class="string">128 ns                          4    40</span></span><br><span class="line"><span class="string">256 ns                          4    40</span></span><br><span class="line"><span class="string">512 ns                          4    6</span></span><br><span class="line"><span class="string">1024 ns                         4    1</span></span><br><span class="line"><span class="string">2048 ns                         4    0</span></span><br><span class="line"><span class="string">4096 ns                         4    0</span></span><br><span class="line"><span class="string">8192 ns                         4    0</span></span><br><span class="line"><span class="string">16384 ns                        4    0</span></span><br><span class="line"><span class="string">32768 ns                        4    0</span></span><br><span class="line"><span class="string">65536 ns                        4    5</span></span><br><span class="line"><span class="string">131072 ns                       4    105</span></span><br><span class="line"><span class="string">262144 ns                       4    331</span></span><br><span class="line"><span class="string">524288 ns                       4    419</span></span><br><span class="line"><span class="string">1048576 ns                      4    325</span></span><br><span class="line"><span class="string">2097152 ns                      4    338</span></span><br><span class="line"><span class="string">4194304 ns                      4    188</span></span><br><span class="line"><span class="string">8388608 ns                      4    65</span></span><br><span class="line"><span class="string">16777216 ns                     4    59</span></span><br><span class="line"><span class="string">33554432 ns                     4    47</span></span><br><span class="line"><span class="string">67108864 ns                     4    34</span></span><br><span class="line"><span class="string">134217728 ns                    4    60</span></span><br><span class="line"><span class="string">268435456 ns                    4    55</span></span><br><span class="line"><span class="string">536870912 ns                    4    65</span></span><br><span class="line"><span class="string">1073741824 ns                   4    94</span></span><br><span class="line"><span class="string">2147483648 ns                   4    56</span></span><br><span class="line"><span class="string">4294967296 ns                   4    271</span></span><br><span class="line"><span class="string">8589934592 ns                   4    3454</span></span><br><span class="line"><span class="string">17179869184 ns                  4    4687</span></span><br><span class="line"><span class="string">34359738368 ns                  4    1970</span></span><br><span class="line"><span class="string">68719476736 ns                  4    642</span></span><br><span class="line"><span class="string">137438953472 ns                 4    380</span></span><br><span class="line"><span class="string">274877906944 ns                 4    440</span></span><br><span class="line"><span class="string">549755813888 ns                 4    299</span></span><br><span class="line"><span class="string">1099511627776 ns                4    36</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -v -y -l 2</span></span><br><span class="line"><span class="string">tank                           146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">  raidz3                       146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a66cec97        -      -     88     18  2.76M  15.6K    8ms  840us    8ms  442us  664ns  474ns  758ns  407us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670caef        -      -     93     18  2.93M  15.1K   68ms   11ms   68ms   10ms  919ns    1us  758ns  281us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a02fb        -      -     73     18  2.40M  14.8K   16ms  514us   16ms  335us  714ns  758ns  568ns  239us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670bbc7        -      -     79     17  2.74M  13.8K   10ms  556us   10ms  334us  679ns  474ns  568ns  244us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754b03        -      -     70     20  2.36M  15.6K   26ms  679us   26ms  360us  766ns  474ns  758ns  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675ae43        -      -     88     19  2.89M  15.1K   60ms    5ms   60ms    5ms  802ns  985ns  758ns  366us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c787        -      -     82     20  2.61M  16.8K   15ms  833us   15ms  468us  713ns  379ns  379ns  394us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754d7b        -      -     97     25  3.05M  43.9K    8ms   14ms    8ms    6ms  710ns  379ns  379ns    9ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a67540d7        -      -     74     19  2.50M  18.3K   23ms  796us   23ms  466us  727ns  474ns  379ns  371us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675455b        -      -     83     19  2.79M  17.3K   32ms  687us   32ms  393us  760ns  474ns  758ns  315us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670ca8f        -      -     71     19  2.38M  17.0K    9ms  771us    9ms  398us  644ns  474ns      -  361us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b55bb        -      -     89     18  2.84M  16.0K    8ms  909us    8ms  567us  779ns  682ns      -  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a7f57        -      -     82     20  2.60M  17.8K   32ms  527us   32ms  318us  744ns  695ns      -  289us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b2683        -      -     92     17  2.99M  15.3K   57ms  687us   57ms  443us  863ns    1us  758ns  242us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675449f        -      -     74     16  2.55M  16.0K   11ms  625us   11ms  425us  633ns  474ns  379ns  257us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c30f        -      -     82     16  2.82M  15.6K    9ms  808us    9ms  539us  680ns  379ns  758ns  363us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670af17        -      -     77     19  2.65M  17.5K    8ms  876us    8ms  458us  695ns  474ns  758ns  380us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c2ab        -      -     95     18  3.10M  17.8K   18ms  786us   18ms  498us  770ns  758ns  379ns  333us      -</span></span><br><span class="line"><span class="string">----------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br></pre></td></tr></table></figure>

<h3 id="Custom-script-show-temperature"><a href="#Custom-script-show-temperature" class="headerlink" title="Custom script ,show temperature"></a>Custom script ,show temperature</h3><p>/etc/zfs/zpool.d</p>
<p>$  ZPOOL_SCRIPTS_AS_ROOT=1 zpool status -c temp<br>  pool: tank<br> state: ONLINE<br>  scan: scrub repaired 0B in 0 days 06:46:11 with 0 errors on Tue Oct 15 06:46:14 2019<br>config:</p>
<pre><code>NAME        STATE     READ WRITE CKSUM  temp
tank        ONLINE       0     0     0
  raidz2-0  ONLINE       0     0     0
    sdc     ONLINE       0     0     0    27
    sdd     ONLINE       0     0     0    28
    sde     ONLINE       0     0     0    27
    sdf     ONLINE       0     0     0    29
    sdg     ONLINE       0     0     0    27
    sdh     ONLINE       0     0     0    28
logs
  mirror-1  ONLINE       0     0     0
    sda3    ONLINE       0     0     0    24
    sdb3    ONLINE       0     0     0    25
cache
  sda4      ONLINE       0     0     0    24
  sdb4      ONLINE       0     0     0    25</code></pre><p>errors: No known data errors</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">### [Tuning for Scrubs and Resilvers](http:&#x2F;&#x2F;broken.net&#x2F;uncategorized&#x2F;zfs-performance-tuning-for-scrubs-and-resilvers&#x2F;)</span><br><span class="line">Prioritize resilvering by setting the delay to zero</span><br><span class="line">Idle window in clock ticks</span><br><span class="line">* set zfs:zfs_resilver_delay &#x3D; 0</span><br><span class="line"></span><br><span class="line">Prioritize scrubs by setting the delay to zero</span><br><span class="line">Number of ticks to delay scrub</span><br><span class="line">* set zfs:zfs_scrub_delay &#x3D; 0</span><br><span class="line"></span><br><span class="line">Maximum number of scrub I&#x2F;O per top-level vdev, by default 32. Increases zfs scrub speed. (at what cost, no idea)</span><br><span class="line">set maximum number of inflight IOs to a reasonable value - this number will vary for your environment</span><br><span class="line">* set zfs:zfs_top_maxinflight &#x3D; 512</span><br><span class="line"></span><br><span class="line">zfs_scan_min_time_ms:Min millisecs to scrub per txg (int)</span><br><span class="line">resilver for five seconds per TXG</span><br><span class="line">* set zfs:zfs_resilver_min_time_ms &#x3D; 5000</span><br><span class="line"></span><br><span class="line">The resilvers or scrub speed increase from 50MB&#x2F;s to 800MB&#x2F;s, increase a lot</span><br><span class="line"></span><br><span class="line">### zdb</span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line">zdb -MM -P test_0</span><br><span class="line">        vdev          0         metaslabs  116          fragmentation 88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line">        pool test_0     fragmentation    88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line"></span><br><span class="line">$ zdb -b test_0</span><br><span class="line"></span><br><span class="line">Traversing all blocks to verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map for vdev 0 of 1, metaslab 115 of 116 ...</span><br><span class="line">39.0G completed (1569MB&#x2F;s) estimated time remaining: 0hr 30min 43sec</span><br><span class="line">2.80T completed ( 553MB&#x2F;s) estimated time remaining: 0hr 00min 00sec</span><br><span class="line">        No leaks (block sum matches space maps exactly)</span><br><span class="line"></span><br><span class="line">        bp count:      1154215521</span><br><span class="line">        ganged count:           0</span><br><span class="line">        bp logical:    3028205925888      avg:   2623</span><br><span class="line">        bp physical:   2695917303296      avg:   2335     compression:   1.12</span><br><span class="line">        bp allocated:  3076338462208      avg:   2665     compression:   0.98</span><br><span class="line">        bp deduped:             0    ref&gt;1:      0   deduplication:   1.00</span><br><span class="line">        SPA allocated: 3076338462208     used: 77.18%</span><br><span class="line"></span><br><span class="line">        additional, non-pointer bps of type 0:         40</span><br><span class="line">        Dittoed blocks on same vdev: 590073873</span><br></pre></td></tr></table></figure>

<p>ZDB Get file info</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=/tank/<span class="built_in">test</span>/1 bs=1M count=1</span><br><span class="line"></span><br><span class="line">zdb -ddddd tank/<span class="built_in">test</span></span><br><span class="line">......</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 0:86bef7d8400:600 4000L/200P F=1 B=11629333/11629333</span><br><span class="line">            4000 L0 0:867fb11c400:1800 4000L/e00P F=1 B=11629333/11629333</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000008000) size   32K</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">       256    2   128K   128K  1.00M     512     1M  100.00  ZFS plain file</span><br><span class="line">                                               168   bonus  System attributes</span><br><span class="line">        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">        dnode maxblkid: 7</span><br><span class="line">        path    /1</span><br><span class="line">        uid     0</span><br><span class="line">        gid     0</span><br><span class="line">        atime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        mtime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        ctime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        crtime  Tue Dec 25 22:25:42 2018</span><br><span class="line">        gen     11630673</span><br><span class="line">        mode    100644</span><br><span class="line">        size    1048576</span><br><span class="line">        parent  34</span><br><span class="line">        links   1</span><br><span class="line">        pflags  40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:86d5203a600:c00 20000L/400P F=8 B=11630673/11630673</span><br><span class="line">               0  L0 0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           20000  L0 0:86e648f9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           40000  L0 0:86e64929400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           60000  L0 0:86e64959400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           80000  L0 0:86e64989400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           a0000  L0 0:86e649b9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           c0000  L0 0:86e649e9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           e0000  L0 0:86e64a19400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000100000) size    1M</span><br><span class="line"></span><br><span class="line">    Dnode slots:</span><br><span class="line">        Total used:             8</span><br><span class="line">        Max used:             256</span><br><span class="line">        Percent empty:  96.875000</span><br></pre></td></tr></table></figure>
<p>0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673<br>vdev:offset:size</p>
<p>vdev=0<br>offset=86e648c9400 (offset in logic disk ?)</p>
<p>20000L/20000P means size,  compressed size(2000L),the no compression file size(2000P)<br>0x2000=131072</p>
<p>B=11630673/11630673 means write operate transaction group</p>
<h3 id="performance-tuning"><a href="#performance-tuning" class="headerlink" title="performance tuning"></a>performance tuning</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># number of z_wr_iss processes tunable</span></span><br><span class="line">zio_taskq_batch_pct</span><br><span class="line">(The bad setting will casue the performance issue, Many thanks <span class="keyword">for</span> Javen <span class="string">'s help)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">core=1</span></span><br><span class="line"><span class="string">[[ $(grep MHz /proc/cpuinfo -c) -gt 8 ]] &amp;&amp; maxcpu=8 &amp;&amp; for i in $(pgrep '</span>z_wr_iss$<span class="string">')</span></span><br><span class="line"><span class="string">do</span></span><br><span class="line"><span class="string">   [[ $core -gt $maxcpu ]] &amp;&amp; core=0</span></span><br><span class="line"><span class="string">   taskset -p -c $core $i</span></span><br><span class="line"><span class="string">   ((core++))</span></span><br><span class="line"><span class="string">done</span></span><br></pre></td></tr></table></figure>

<h3 id="check-your-zpool-could-be-imported"><a href="#check-your-zpool-could-be-imported" class="headerlink" title="check your zpool could be imported"></a>check your zpool could be imported</h3><p>running it in another server(not import the pool), you can see can be import or can not be import<br>make sure the MMP was worked</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zpool import</span><br><span class="line"></span><br><span class="line"><span class="comment">## export the zpool or you have antoher server connect this zpool</span></span><br><span class="line">$ zdb -ec <span class="variable">$poolname</span></span><br><span class="line">Traversing all blocks to verify metadata checksums and verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map <span class="keyword">for</span> vdev 0 of 1, metaslab 1 of 139 ...</span><br><span class="line"></span><br><span class="line">$ zdb -ec -AAA <span class="variable">$poolname</span></span><br><span class="line"><span class="comment"># if there is "assertion failure"...</span></span><br><span class="line"><span class="comment"># -AAA could not abort</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">'mdt_0'</span></span><br><span class="line">        state: 0</span><br><span class="line">        hostid: 4281985155</span><br><span class="line">        hostname: <span class="string">'cngb-mds-m20-1'</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">'root'</span></span><br><span class="line">            id: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">'raidz'</span></span><br><span class="line">                id: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -e -AAA <span class="variable">$poolname</span> &gt; <span class="variable">$poolnam</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">'mdt_0'</span></span><br><span class="line">        state: 0</span><br><span class="line">        hostid: 4281985155</span><br><span class="line">        hostname: <span class="string">'cngb-mds-m20-1'</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">'root'</span></span><br><span class="line">            id: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">'raidz'</span></span><br><span class="line">                id: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -em -AAA <span class="variable">$poolname</span> <span class="comment"># to check metalab</span></span><br><span class="line">Metaslabs:</span><br><span class="line">        vdev          0</span><br><span class="line">        metaslabs   139   offset                spacemap          free</span><br><span class="line">        ---------------   -------------------   ---------------   -------------</span><br><span class="line">        metaslab      0   offset            0   spacemap     39   free    27.6G</span><br><span class="line">        metaslab      1   offset    800000000   spacemap     55   free    31.1G</span><br><span class="line">        metaslab      2   offset   1000000000   spacemap     58   free    30.3G</span><br><span class="line">        metaslab      3   offset   1800000000   spacemap     42   free    29.4G</span><br><span class="line">        metaslab      4   offset   2000000000   spacemap     45   free    28.3G</span><br><span class="line"></span><br><span class="line"><span class="comment">#metaslab check</span></span><br></pre></td></tr></table></figure>

<h3 id="zpool-status"><a href="#zpool-status" class="headerlink" title="zpool status"></a>zpool status</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ zpool status -x -v</span><br><span class="line">$ zpool status -v</span><br><span class="line"></span><br><span class="line">errors: Permanent errors have been detected <span class="keyword">in</span> the following files:</span><br><span class="line"></span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc1:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc5:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fccc:0x0</span><br><span class="line">        tank/mdt_0:/oi.1/0x1:0x1fcc2:0x0</span><br></pre></td></tr></table></figure>

<h3 id="Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><a href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx" class="headerlink" title="Disable AVX512 for scalable Xeon silver and gold 5xxx"></a>Disable AVX512 for scalable Xeon silver and gold 5xxx</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Gold 5115</span></span><br><span class="line">$ cat /proc/spl/kstat/zfs/fletcher_4_bench</span><br><span class="line">0 0 0x01 -1 0 43875755056 2388098231282525</span><br><span class="line">implementation   native         byteswap</span><br><span class="line">scalar           5097645469     4106654089</span><br><span class="line">superscalar      6876819423     5086086480</span><br><span class="line">superscalar4     5926587517     4946863265</span><br><span class="line">sse2             11659567916    6572225291</span><br><span class="line">ssse3            11660661610    10379355285</span><br><span class="line">avx2             17865482202    16120833783</span><br><span class="line">avx512f          24802818495    8841728748</span><br><span class="line">fastest          avx512f        avx2</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zcommon/parameters/zfs_fletcher_4_impl</span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zfs/parameters/zfs_vdev_raidz_impl</span><br></pre></td></tr></table></figure>
<p>Because I ‘m wrong about when avx2 or avx512 called, The cpu will reduce the frequency and it will impact the others (no avx system call) at the same time<br><a href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_vdev_raidz_impl" target="_blank" rel="noopener">zfs_vdev_raidz_impl</a><br><a href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_fletcher_4_impl" target="_blank" rel="noopener">zfs_fletcher_4_impl</a></p>
<p>or you could disable AVX by kernel parameter</p>
<h1 id="not-make-sure"><a href="#not-make-sure" class="headerlink" title="not make sure"></a>not make sure</h1><h3 id="Too-bad-about-zfs-dracut"><a href="#Too-bad-about-zfs-dracut" class="headerlink" title="Too bad about zfs-dracut"></a>Too bad about zfs-dracut</h3><p>In zfs old version 0.6.x, there is no multihost parameter.<br>When you upgrade to 0.7.x, it ‘s not enable by default !!!  but you can see the mmp history is worked !!!<br>So if you zfs-dracut , maybe you will be brain-split, lsinitrd show hostid == 0 !!!<br>My version is 0.7.9-1</p>
<p>##not make sure<br>lsinitrd</p>
<p>Thanks Javen Wu save me again…</p>
<p>You could unistall the package or remove it , zfs dracut is too dangerous for HA arch</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -ql zfs-dracut-0.7.9-1.el7.x86_64</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/<span class="built_in">export</span>-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/mount-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/parse-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-generator.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-lib.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-needshutdown.sh</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9/README.dracut.markdown</span><br></pre></td></tr></table></figure>

<h3 id="ZFS-0-7-x-MMP-cause-too-many-issues-the-ZFS-0-6-5-more-stable"><a href="#ZFS-0-7-x-MMP-cause-too-many-issues-the-ZFS-0-6-5-more-stable" class="headerlink" title="ZFS 0.7.x MMP cause too many issues, the ZFS 0.6.5 more stable"></a>ZFS 0.7.x MMP cause too many issues, the ZFS 0.6.5 more stable</h3><ul>
<li><a href="https://github.com/zfsonlinux/zfs/issues/7834" target="_blank" rel="noopener">Performance impact</a></li>
<li><a href="https://github.com/zfsonlinux/zfs/issues/7731" target="_blank" rel="noopener">Zpool import slow</a></li>
<li><a href="https://github.com/zfsonlinux/zfs/issues/7709" target="_blank" rel="noopener">Disk fail cause zpool suspend</a><ul>
<li><a href="https://github.com/zfsonlinux/zfs/pull/8495" target="_blank" rel="noopener">2</a></li>
</ul>
</li>
</ul>
<h3 id="Resolved-import-too-slow"><a href="#Resolved-import-too-slow" class="headerlink" title="Resolved import too slow"></a>Resolved import too slow</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line">10</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line">$ zpool import -a</span><br><span class="line">$ <span class="built_in">echo</span> 10 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br></pre></td></tr></table></figure>

<h3 id="Enable-zfs-message-log"><a href="#Enable-zfs-message-log" class="headerlink" title="Enable zfs message log"></a>Enable zfs message log</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 1 &gt;/sys/module/zfs/parameters/zfs_dbgmsg_enable</span><br><span class="line">$ cat /proc/spl/kstat/zfs/dbgmsg</span><br></pre></td></tr></table></figure>

<h3 id="Remove-slog"><a href="#Remove-slog" class="headerlink" title="Remove slog"></a>Remove slog</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## slog device has been destroy, skip/bypass import slog</span></span><br><span class="line">$ zpool import -m -a -f</span><br><span class="line"></span><br><span class="line">$ zpool status -x</span><br><span class="line">  pool: tank</span><br><span class="line"> state: DEGRADED</span><br><span class="line">status: One or more devices could not be used because the label is missing or</span><br><span class="line">        invalid.  Sufficient replicas exist <span class="keyword">for</span> the pool to <span class="built_in">continue</span></span><br><span class="line">        functioning <span class="keyword">in</span> a degraded state.</span><br><span class="line">action: Replace the device using <span class="string">'zpool replace'</span>.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-4J</span><br><span class="line">  scan: scrub repaired 0B <span class="keyword">in</span> 0 days 06:46:11 with 0 errors on Mon Oct 14 18:46:14 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                      STATE     READ WRITE CKSUM</span><br><span class="line">        tank                      DEGRADED     0     0     0</span><br><span class="line">          raidz2-0                ONLINE       0     0     0</span><br><span class="line">            sdc                   ONLINE       0     0     0</span><br><span class="line">            sdd                   ONLINE       0     0     0</span><br><span class="line">            sde                   ONLINE       0     0     0</span><br><span class="line">            sdf                   ONLINE       0     0     0</span><br><span class="line">            sdg                   ONLINE       0     0     0</span><br><span class="line">            sdh                   ONLINE       0     0     0</span><br><span class="line">        logs</span><br><span class="line">          mirror-1                UNAVAIL      0     0     0  insufficient replicas</span><br><span class="line">            13378777246457412929  UNAVAIL      0     0     0  was /dev/sda3</span><br><span class="line">            1725913437883392852   UNAVAIL      0     0     0  was /dev/sdb</span><br><span class="line"></span><br><span class="line">$ zpool remove tank mirror-1</span><br><span class="line">that ok</span><br></pre></td></tr></table></figure>

<h3 id="About-SMR-HDD"><a href="#About-SMR-HDD" class="headerlink" title="About SMR HDD"></a><a href="https://github.com/zfsonlinux/zfs/issues/4877" target="_blank" rel="noopener">About SMR HDD</a></h3><p>Another thing you might try is to leave the block size set at 1M but increase the zfs_vdev_aggregation_limit to 16M. This way as long as your doing 1M aligned IO you should never write partial blocks and leave holes. ZFS will aggregate these 1M blocks in to larger 16M IOs to the disk.</p>
<p><a href="http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/" target="_blank" rel="noopener">zfs-performance-tuning-for-scrubs-and-resilvers</a><br><a href="https://forum.proxmox.com/threads/zfs-zil-log-txg_sync-causing-high-io-every-5-seconds-any-solution.24376/" target="_blank" rel="noopener">zfs-zil-log-txg_sync-causing-high-io-every-5-seconds</a><br><a href="https://www.svennd.be/tuning-of-zfs-module/" target="_blank" rel="noopener">tuning zfs module</a><br><a href="http://lustre.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf" target="_blank" rel="noopener">Stearman-LLNL-ZFS</a><br><a href="http://fibrevillage.com/storage/171-zfs-on-linux-performance-tuning" target="_blank" rel="noopener">171-zfs-on-linux-performance</a><br><a href="http://list.zfsonlinux.org/pipermail/zfs-discuss/2018-March/030666.html" target="_blank" rel="noopener">Is the number of z_wr_iss processes tunable?</a><br><a href="https://www.beegfs.io/wiki/StorageServerTuning#hn_59ca4f8bbb_16" target="_blank" rel="noopener">Tips and Recommendations for Storage Server Tuning</a></p>
]]></content>
      <categories>
        <category>filesystem</category>
      </categories>
      <tags>
        <tag>zfs</tag>
        <tag>scsi</tag>
      </tags>
  </entry>
  <entry>
    <title>The block device</title>
    <url>/2016/08/10/block_device/</url>
    <content><![CDATA[<p>About linux block device</p>
<a id="more"></a>

<h3 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h3><h4 id="Why-use-4K-device"><a href="#Why-use-4K-device" class="headerlink" title="Why use 4K device"></a>Why use 4K device</h4><ul>
<li>4-KB native (4Kn) HDDs<pre><code>* The 4Kn HDD directly maps 4-KB logical sectors (or blocks) to the 4-KB physical sectors.</code></pre></li>
<li>512-byte emulation (512e) HDDs<pre><code>* The 512e HDD transparently translates 512-byte logical block I/O requests into 4-KB physical sector operations. Each physical sector contains eight logical blocks.</code></pre></li>
</ul>
<p><a href="https://www.thomas-krenn.com/en/wiki/Advanced_Sector_Format_of_Block_Devices" target="_blank" rel="noopener">512e Read Operations</a><br>Read operations are very simple compared to write operations:<br>The host would like to read a 512-byte block.<br>The controller loads the complete 4KB sector containing the requested 512-byte block.<br>The controller extracts the data from the 512-byte block and delivers it in the corresponding format to the host.</p>
<p>512e Write operations use the “read-modify-write” method:<br>The host would like to write a 512-byte block.<br>The controller selects a suitable 4KB sector and loads it completely. (read)<br>The controller modifies 512 bytes in the 4KB sector. (modify)<br>The controller writes the modified 4KB sector to the hard drive. (write)</p>
<p>Compatibility<br>In order to keep performance constant, it is necessary that partitions on 512e hard drives be properly aligned (see also Partition Alignment) so that a 4KB sector contains exactly eight 512-byte blocks. Newer operating systems already correctly align the partitions (Windows &gt; Vista SP1; Linux &gt; 2.6.31 (fdisk &gt; 1.2.3))[1]. With the correct alignment, write operations can be optimally cached by the hard drive controller, which optimizes performance.</p>
<p>In <a href="https://lwn.net/Articles/322777" target="_blank" rel="noopener">This</a>, more details and add new one,the preamble</p>
<ul>
<li><p>Synchronization/Data Address Mark (Sync/DAM)</p>
<pre><code>* The Sync/DAM field indicates the beginning of the sector and identifies the sector’s number, location, and status.</code></pre></li>
<li><p>User data</p>
<pre><code>* The User data field contains actual stored data.</code></pre></li>
<li><p>Error correcting code (ECC)</p>
<pre><code>* The ECC field contains error-correcting code that is used to recover user data that mightbe damaged during the read or write operation.</code></pre></li>
<li><p>Gap</p>
<pre><code>* The Gap field is used to separate sectors from each other.</code></pre></li>
<li><p>Physical sector</p>
<pre><code>* Physical sector is the minimum amount of data that the HDD can read from or write to the physical media in a single I/O operation. For Advanced Format HDDs, the physical sector size is 4 KB.</code></pre></li>
<li><p>Logical sector</p>
<pre><code>* Logical sector is the addressable logical block, which is the minimum amount of data that the HDD can address. This amount is also the minimum amount of data that the host system can deliver to or request from the HDD in a single I/O operation. Advanced Format HDDs support 512-bytes and 4-KB logical sector sizes.</code></pre></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">------------------------------------------</span><br><span class="line">|P|D|  Data Bytes(512/4096Bytes  | ECC |G|</span><br><span class="line">------------------------------------------</span><br><span class="line">P= Preamble</span><br><span class="line">D= Data sync mark</span><br><span class="line">ECC= Error Correcting Code</span><br><span class="line">G= Inter Sector Gap</span><br></pre></td></tr></table></figure>
<p>I think the OS kernel could not recognise ECC/G/P/D and no need to know them, maybe driver will know them. it ‘s the hardware design<br>In hardware that means there is ECC to check hardware read/write signals.</p>
<p>long-data-sector (512,520,528,4096,4112,4160,4224) will show size in logic sector</p>
<h4 id="mpt3sas-mpt2sas-driver-parameter"><a href="#mpt3sas-mpt2sas-driver-parameter" class="headerlink" title="mpt3sas/mpt2sas driver parameter"></a>mpt3sas/mpt2sas driver parameter</h4><p>parm: command_retry_count: Device discovery TUR command retry count: (default=144) (int)<br>retry count default was 144, it ‘s too large</p>
<p>parm: max_queue_depth: max controller queue depth (int) </p>
<p>The Linux “scatter/gather” table size needs to be large enough to allow IO_SIZE IO, if possible. For most drivers, this typically requires a “sg_tablesize” value of 256 or greater for 4MB IO. Different vendors have different defaults for this parameter, and may require a modprobe.conf entry to increase the value. The QLogic driver defaults to a value of 1024. For Emulex, a modeprobe.conf entry needs to be added to increase the value to 256 or greater, such as:<br>options lpfc lpfc_sg_seg_cnt=256</p>
<p>#The LSI SAS driver, “mpt2sas”, uses the parameter named “max_sgl_entries” to control this value.<br>#Its maximum value in RHEL 6.x currently only 128<br>#options mpt2sas max_sgl_entries=256<br>#Try to upgrade to upgrade mpt3sas 27.00.01.00<br>LSI 9300-8e could not modify from mpt3sas 27.00.01.00, I have not LSI 9400-8e to test</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lspci | grep 2308</span><br><span class="line">01:00.0 Serial Attached SCSI controller: LSI Logic / Symbios Logic SAS2308 PCI-Express Fusion-MPT SAS-2 (rev 05)</span><br><span class="line">$ cat /sys/devices/pci0000:00/0000:00:02.1/0000:01:00.0/host1/scsi_host/host1/sg_tablesize</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 256 &gt; /sys/devices/pci0000:00/0000:00:02.1/0000:01:00.0/host1/scsi_host/host1/sg_tablesize</span><br><span class="line"><span class="built_in">echo</span>: write error: Input/output error</span><br></pre></td></tr></table></figure>

<h4 id="mpt3sas-driver-install"><a href="#mpt3sas-driver-install" class="headerlink" title="mpt3sas driver install"></a>mpt3sas driver install</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#compile to local</span><br><span class="line">$ make -j4 CONFIG_DEBUG_INFO&#x3D;1 -C &#x2F;lib&#x2F;modules&#x2F;$(uname -r)&#x2F;build M&#x3D;&#x2F;sources&#x2F;tools&#x2F;dell&#x2F;lsi-9300-8e&#x2F;mpt3sas</span><br><span class="line"></span><br><span class="line">#compile to local kernel </span><br><span class="line">$ make -j4 CONFIG_DEBUG_INFO&#x3D;1 -C &#x2F;lib&#x2F;modules&#x2F;$(uname -r)&#x2F;build M&#x3D;&#x2F;sources&#x2F;tools&#x2F;dell&#x2F;lsi-9300-8e&#x2F;mpt3sas modules_install</span><br></pre></td></tr></table></figure>

<h4 id="Qlogic-driver-setting"><a href="#Qlogic-driver-setting" class="headerlink" title="Qlogic driver setting"></a>Qlogic driver setting</h4><p>Adpater reset time e.g. Qlogic reset time<br>echo options qla2xxx ql2xextended_error_logging=1 qlport_down_retry=10 ql2xloginretrycount=10 &gt;&gt;  /etc/modprobe.d/qlogic.conf<br>Multipath check_timeout  (reduce to 10 seconds from default 60 seconds)</p>
<p>Set the max_segments for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sgl_entries|<br>| Emulex    |  lpfc_sg_seg_cnt|<br>| ib_srp    |  cmd_sg_entries |<br>| Brocade   |  bfa_io_max_sge |</p>
<p>Set max_hw_sectors_kb for HBA driver<br>| HBA       | Module Parameter|<br>|———–|:—————:|<br>| LSI       |  max_sectors    |<br>| ib_srp    |  max_sect       |<br>| Brocade   |  max_xfer_size  |</p>
<p>mpt3sas<br>max_sectors:max sectors, range 64 to 32767  default=32767 (ushort)</p>
<h4 id="SAN-controller-setting-MD3460"><a href="#SAN-controller-setting-MD3460" class="headerlink" title="SAN controller setting MD3460"></a>SAN controller setting MD3460</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RAID 6               8+2</span><br><span class="line">segment              128K</span><br><span class="line">cache block size     32K</span><br><span class="line">cache flush          90%</span><br><span class="line">Write cache mirror   enabled</span><br><span class="line">Read cache           disabled</span><br></pre></td></tr></table></figure>

<h3 id="rebuild-feature"><a href="#rebuild-feature" class="headerlink" title="rebuild feature"></a>rebuild feature</h3><p>In May 2012, with 3.5-inch hard drive capacities reaching 4TB, the T10 working group approved including Rebuild Assist in the SCSI SBC-3 specification. In August 2013, the SATA-IO committee adopted TPR-045 (Rebuild Assist) as part of the SATA 3.2 specification.”) so with a Rebuild Assist supported RAID controller: can copy good data from a failing drive to a new drive and only need to rebuild the ‘bad’ portion, supposably resulting in quicker rebuild times</p>
<h3 id="Linux-setting"><a href="#Linux-setting" class="headerlink" title="Linux setting"></a>Linux setting</h3><h4 id="block-driver"><a href="#block-driver" class="headerlink" title="block driver"></a>block driver</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ $ cat /sys/block/sdX/device/state</span><br><span class="line">running</span><br><span class="line"></span><br><span class="line">$ cat /sys/block/sdz/device/queue_depth</span><br><span class="line">254</span><br><span class="line"></span><br><span class="line">$ cat /sys/block/sda/queue/nomerges </span><br><span class="line"><span class="comment">#set nomerges to 0 for HDDs or to 1 for SSDs</span></span><br><span class="line"></span><br><span class="line">$ cat /sys/block/sdd/queue/add_random </span><br><span class="line"><span class="comment">#The default value is 1. Set add_random=0 for SSDs because random entropy pool does not optimize SSD performance</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># check scsi state</span></span><br><span class="line">$ cat /sys/block/sdX/device/state</span><br><span class="line">running</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 30 &gt; /sys/block/sdX/device/timeout</span><br><span class="line"><span class="comment"># I don 't think set the value too long</span></span><br><span class="line"></span><br><span class="line">$ cat /etc/udev/rules.d/50-udev.rules</span><br><span class="line">ACTION==<span class="string">"add"</span>, SUBSYSTEM==<span class="string">"scsi"</span> , SYSFS&#123;<span class="built_in">type</span>&#125;==<span class="string">"0|7|14"</span>, RUN+=<span class="string">"/bin/sh -c 'echo 30 &gt; /sys/block/%k/device/timeout'"</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">### another exapmle</span></span><br><span class="line">KERNEL==<span class="string">"sdc5"</span>, OWNER=<span class="string">"student"</span>, GROUP=<span class="string">"student"</span>, MODE=<span class="string">"0600"</span></span><br><span class="line">ACTION==<span class="string">"add"</span>, KERNEL==<span class="string">"sd*"</span>, SYSFS==<span class="string">"4317210A2880EF89"</span>, SYMLINK+=<span class="string">"fedora%n"</span></span><br><span class="line">ACTION==<span class="string">"add"</span>, KERNEL==<span class="string">"sdb[1-9]"</span>, RUN=<span class="string">"/usr/bin/wall  SCSI DEVICE ADDED"</span></span><br><span class="line">ACTION==<span class="string">"remove"</span>, KERNEL==<span class="string">"sdb[1-9]"</span>, RUN=<span class="string">"/usr/bin/wall SCSI DEVICE REMOVED"</span></span><br><span class="line">ACTION==<span class="string">"add"</span>, KERNEL==<span class="string">"sdb[1-9]"</span>, SYMLINK=<span class="string">"scsi%n"</span></span><br><span class="line">ACTION==<span class="string">"remove"</span>, KERNEL==<span class="string">"sdb[1-9]"</span>, SYMLINK=<span class="string">"scsi%n"</span></span><br><span class="line">ACTION==<span class="string">"add"</span>, KERNEL==<span class="string">"sd*[!0-9]"</span>, SYSFS&#123;vendor&#125;==<span class="string">"WDC WD32"</span>, RUN+=<span class="string">"/bin/sh -c 'echo 128 &gt; /sys/block/%k/queue/max_sectors_kb'"</span></span><br><span class="line">ACTION==<span class="string">"add"</span>, KERNEL==<span class="string">"sd*[!0-9]"</span>, SYSFS&#123;vendor&#125;==<span class="string">"WDC WD32"</span>, RUN+=<span class="string">"/usr/bin/wall /sys/block/%k/queue/max_sectors_kb set to 128"</span></span><br></pre></td></tr></table></figure>

<h4 id="block-driver-2"><a href="#block-driver-2" class="headerlink" title="block driver 2"></a><a href="https://library.netapp.com/ecmdocs/ECMP12404601/html/GUID-436F7286-AD26-4A8D-A2D1-2BC8B5CFC023.html" target="_blank" rel="noopener">block driver 2</a></h4><p><a href="https://access.redhat.com/solutions/43861" target="_blank" rel="noopener">redhat doc</a></p>
<ul>
<li>max_hw_sectors_kb (RO) - This parameter sets the maximum number of kilobytes that the hardware allows for request.<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ubuntu 18.04 server, sas 10TB 512e HDD</span><br><span class="line">$ cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;max_hw_sectors_kb </span><br><span class="line">16383</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>max_sectors_kb = avgrq-sz(iostat)</p>
<ul>
<li><p><a href="https://access.redhat.com/solutions/2150101" target="_blank" rel="noopener">max_sectors_kb</a> (RW) - This parameter sets the maximum number of kilobytes that the block layer allows for a file system request. The value of this parameter must be less than or equal to the maximum size allowed by the hardware. The kernel also places an upper bound on this value with the BLK_DEF_MAX_SECTORS macro. This value varies from distribution to distribution, for example, it is 1024 on RHEL 6.3, 2048 on SLES 11 SP2.</p>
<pre><code>* This specifies the maximum I/O size that the host will issue to the target storage. linxu 4.1x default value was 1280, Typically it is 512KiB (512 Bytes,1024 sectors, 4KN, 1024 sectors means 4096KiB)</code></pre><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_sectors_kb </span><br><span class="line">1280</span><br></pre></td></tr></table></figure>
</li>
<li><p>max_segments (RO) - This parameter enables low level driver to set an upper limit on the number of hardware data segments in a request. In the HBA drivers, this is also known as sg_tablesize.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_segments</span><br><span class="line">128</span><br></pre></td></tr></table></figure>
</li>
<li><p>max_segment_size (RO) - This parameter enables low level driver to set an upper limit on the size of each data segment in an I/O request in bytes. If clustering is enabled on the low level driver it is set to 65536 or it is set to system PAGE_SIZE by default, which is typically 4K. The maximum I/O size is determined by the following:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/max_segment_size </span><br><span class="line">65536</span><br></pre></td></tr></table></figure>
<p>MAX_IO_SIZE_KB = MIN(max_sectors_kb, (max_segment_size * max_segments)/1024)<br>1280 KB       = MIN(1280, (65536*128)/1024) = MIN(1280, 8192)</p>
</li>
</ul>
<p>In this command, PAGE_SIZE is architecture independent. It is 4096 for x86_64.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#or you could set it from 1280 to 8192</span><br><span class="line">$ echo 8192 &gt; max_sectors_kb</span><br></pre></td></tr></table></figure>

<ul>
<li><p>physical_block_size</p>
<ul>
<li>Smallest internal unit on which the device can operate<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4KN HDD, there is no 512n in the large capacity HDD</span></span><br><span class="line">$ cat /sys/block/sdj/queue/physical_block_size</span><br><span class="line">4096</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>logical_block_size</p>
<ul>
<li>Used externally to address a location on the device<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4KN</span></span><br><span class="line">$ cat /sys/block/sdj/queue/logical_block_size</span><br><span class="line">4096</span><br><span class="line"></span><br><span class="line"><span class="comment"># 512e</span></span><br><span class="line">$ cat /sys/block/sdi/queue/logical_block_size</span><br><span class="line">512</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>alignment_offset</p>
<ul>
<li>The number of bytes that the beginning of the Linux block device (partition/MD/LVM device) is offset from the underlying physical alignment<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sdj/alignment_offset</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>minimum_io_size</p>
<ul>
<li>The device’s preferred minimum unit for random I/O</li>
<li>The minimum_io_size and optimal_io_size values for /dev/sdX devices are retrieved by the kernel by inquiring the storage vendor-provided I/O Limits within the device VPD pages.<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sdj/queue/minimum_io_size</span><br><span class="line">4096</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><a href="http://fibrevillage.com/storage/563-storage-i-o-alignment-and-size" target="_blank" rel="noopener">optimal_io_size</a></p>
<ul>
<li>The device’s preferred unit for streaming I/O<ul>
<li>Only one layer in the I/O stack should adjust for a non-zero alignment_offset; once a layer adjusts accordingly, it will export a device with an alignment_offset of zero. </li>
<li>A striped Device Mapper (DM) device created with LVM must export a minimum_io_size and optimal_io_size relative to the stripe count (number of disks) and user-provided chunk size.<br>Note<br>Red Hat Enterprise Linux 7 cannot distinguish between devices that don’t provide I/O hints and those that do so with alignment_offset= 0 and optimal _io_size= 0 . Such a device might be a single SAS 4K device; as such, at worst 1MB of space is lost at the start of the disk.</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/block/sdj/queue/optimal_io_size</span><br><span class="line">0</span><br></pre></td></tr></table></figure>
<p>If wanting to change the optimal_io_size on a dm(multipath) device add parameter max_sectors_kb to the /etc/multipath.conf file for the specific storage array (and change the underlying sd devices using an udev rule).</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># I don 't know why only zero in [1] and [2] location</span></span><br><span class="line"></span><br><span class="line">$ sg_inq -p 0xb0 /dev/sdacn</span><br><span class="line">VPD INQUIRY: Block limits page (SBC)</span><br><span class="line">  Optimal transfer length granularity: 1 blocks   &lt;&lt; [1] block-size x this field = minimal_io_size, block-size defined <span class="keyword">in</span> READCAP <span class="built_in">command</span> below.</span><br><span class="line">  Maximum transfer length: 8192 blocks</span><br><span class="line">  Optimal transfer length: 8192 blocks            &lt;&lt; [2] block-size x this field = optimal_io_size</span><br><span class="line">  Maximum prefetch, xdread, xdwrite transfer length: 0 blocks</span><br><span class="line"></span><br><span class="line">$ sg_readcap -16 /dev/sdacn</span><br><span class="line">Read Capacity results:</span><br><span class="line">   Protection: prot_en=0, p_type=0, p_i_exponent=0</span><br><span class="line">   Thin provisioning: tpe=0, tprz=0</span><br><span class="line">   Last logical block address=4194303 (0x3fffff), Number of logical blocks=4194304</span><br><span class="line">   Logical block length=512 bytes                &lt;&lt; [3] this is block size (length), multiplier <span class="keyword">for</span> above fields.</span><br><span class="line">   Logical blocks per physical block exponent=0</span><br><span class="line">   Lowest aligned logical block address=0</span><br><span class="line">Hence:</span><br><span class="line">   Device size: 2147483648 bytes, 2048.0 MiB, 2.15 GB</span><br><span class="line"></span><br><span class="line">$ grep -v <span class="string">"zz"</span> /sys/block/sdacn/queue/*io_size</span><br><span class="line">/sys/block/sdacn/queue/minimum_io_size:512       &lt;&lt; [1] 1 block     x [3] 512-bytes/block =     512 bytes.</span><br><span class="line">/sys/block/sdacn/queue/optimal_io_size:4194304   &lt;&lt; [2] 8192 blocks x [3] 512-bytes/block = 4194034 bytes.</span><br></pre></td></tr></table></figure>
<p>This does hint to the fact that it is possible to see different optimal_io_size for the same LUN across different access paths. This can happen (from storage controller configuration issues, manual changes (echo) or a trigger of udev rules), which could cause an exceptionally high optimal_io_size for the dm(multipath) device or paths which will not function. If this is seen, and the sg_readcap and sg_inq is lining up with what is in /sys, it is likely due to a storage controller configuration issue, and the system’s SAN vendor should be contacted for analysis. Below is an example of what this would look like, noting that 4278190080 is the LCM (least common multiple) of 4177920 and 16777216</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ multipath -ll disk1</span><br><span class="line">disk1 (wwid_entry_omitted) dm-108 3PARdata,VV</span><br><span class="line">size=80G features=<span class="string">'1 queue_if_no_path'</span> hwhandler=<span class="string">'1 alua'</span> wp=rw</span><br><span class="line">`-+- policy=<span class="string">'round-robin 0'</span> prio=1 status=active</span><br><span class="line">  |- 0:0:0:152 sdcg 69:64   active ready running</span><br><span class="line">  |- 1:0:0:152 sdjc 8:352   active ready running</span><br><span class="line">  |- 0:0:1:152 sdfr 130:208 active ready running</span><br><span class="line">  `- 1:0:1:152 sdmn 69:496  active ready running</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> dm-108 sdcg sdjc sdfr sdmn ; <span class="keyword">do</span> cat /sys/block/<span class="variable">$i</span>/queue/optimal_io_size ; <span class="keyword">done</span></span><br><span class="line">4278190080     &lt;&lt; LCM of 16777216 and 4177920</span><br><span class="line">16777216       &lt;&lt; Optimal transfer length: 32768 blocks x Logical block length=512 bytes</span><br><span class="line">16777216       &lt;&lt; Optimal transfer length: 32768 blocks x Logical block length=512 bytes</span><br><span class="line">4177920        &lt;&lt; Optimal transfer length: 8160 blocks x Logical block length=512 bytes</span><br><span class="line">4177920        &lt;&lt; Optimal transfer length: 8160 blocks x Logical block length=512 bytes</span><br></pre></td></tr></table></figure>

<h5 id="parted-alignment"><a href="#parted-alignment" class="headerlink" title="parted alignment"></a>parted alignment</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ parted /dev/sdX <span class="string">'unit s print'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Is the partition align ?</span></span><br><span class="line">$ parted /dev/sdX align-check optimal 1</span><br><span class="line">1 aligned</span><br><span class="line"></span><br><span class="line"><span class="comment">#auto align</span></span><br><span class="line">$ parted -a optimal /dev/sdX mkpart primary 0% xxTB</span><br><span class="line"><span class="comment"># cylinder,none,minimal,optimal</span></span><br></pre></td></tr></table></figure>
<p>GUID Partition Table Scheme<br><img src="/img/GUID_Partition_Table_Scheme.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-------------</span><br><span class="line">LBA0 prrotective MBR</span><br><span class="line">-------------</span><br><span class="line">LBA1 Primary GPT header</span><br><span class="line">-------------</span><br><span class="line">LBA2 Entry1|Entry2|Entry3|Entry4</span><br><span class="line">-------------</span><br><span class="line">LBA3 Entries 5-128</span><br><span class="line">------------</span><br><span class="line">LBA34  Partirion1,2..</span><br><span class="line">LBA-34 remaining Partitions</span><br><span class="line">------------</span><br><span class="line">LBA-33  Entry1|Entry2|entry3|Entry4</span><br><span class="line">------------</span><br><span class="line">LBA-2   Entries 5-128</span><br><span class="line">------------</span><br><span class="line">LBA-1   Secondary GPT Header</span><br><span class="line">----</span><br></pre></td></tr></table></figure>

<h5 id="Hot-plugin-the-scsi-device"><a href="#Hot-plugin-the-scsi-device" class="headerlink" title="Hot plugin the scsi device"></a>Hot plugin the scsi device</h5><p>What is h c t l<br>          h == hostadapter id (first one being 0)<br>          c == SCSI channel on hostadapter (first one being 0)<br>          t == ID (target)<br>          l == LUN (first one being 0)<br>Generic SCSI devices can also be accessed via the bsg driver in Linux. By default, the bsg driver’s device node names  are  of  the  form ‘/dev/bsg/H:C:T:L’.  So,  for example, the SCSI device shown by this utility on a line starting with the tuple ‘6:0:1:2’ could be accessed via the bsg driver with the ‘/dev/bsg/6:0:1:2’ device node name.</p>
<h5 id="Add-the-scsi-device"><a href="#Add-the-scsi-device" class="headerlink" title="Add the scsi device"></a>Add the scsi device</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;- - -&quot; &gt; &#x2F;sys&#x2F;class&#x2F;scsi_host&#x2F;host&lt;h&gt;&#x2F;scan</span><br><span class="line">echo &quot;c t l&quot; &gt;  &#x2F;sys&#x2F;class&#x2F;scsi_host&#x2F;host&lt;h&gt;&#x2F;scan</span><br></pre></td></tr></table></figure>

<h5 id="Refresh-the-scsi-device"><a href="#Refresh-the-scsi-device" class="headerlink" title="Refresh the scsi device"></a>Refresh the scsi device</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdau/device/rescan</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/rescan</span><br></pre></td></tr></table></figure>

<h5 id="Remove-the-scsi-device"><a href="#Remove-the-scsi-device" class="headerlink" title="Remove the scsi device"></a>Remove the scsi device</h5><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/scsi_device/h:c:t:l/device/delete</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/&lt;dev&gt;/device/delete</span><br></pre></td></tr></table></figure>

<h4 id="IO-schdule"><a href="#IO-schdule" class="headerlink" title="IO schdule"></a>IO schdule</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">schdule = deadline</span><br><span class="line">nr_request = 1024</span><br><span class="line">max_sector_kb = 1024</span><br><span class="line">read_ahead_kb = 8192</span><br><span class="line">rq_affinity = 2</span><br><span class="line"><span class="comment"># For storage configurations that need to maximize distribution of completion processing setting this option to '2' forces the completion to run on the requesting cpu (bypassing the "group" aggregation logic).</span></span><br><span class="line"></span><br><span class="line">vm.dirty_ratio = 40</span><br><span class="line"><span class="comment"># dirty_ratio     "Dirty" memory is that waiting to be written to disk. dirty_ratio is the number of memory pages at which a process will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. A default of 20 is reasonable. Increase to 40 to improve throughput, decrease it to 5 to 10 to improve latency, even lower on systems with a lot of memory.</span></span><br><span class="line"></span><br><span class="line">vm.dirty_background_ratio = 20</span><br><span class="line"><span class="comment"># dirty_background_ratio  Similar, but this is the number of memory pages at which the kernel background flusher thread will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. Set this lower than dirty_ratio, dirty_ratio/2 makes sense and is what the kernel does by default. This page shows that dirty_ratio has the greater effect. Tune dirty_ratio for performance, then set dirty_background_ratio to half that value.</span></span><br><span class="line"></span><br><span class="line">vm.vfs_cache_pressure = 50</span><br><span class="line">This sets the <span class="string">"pressure"</span> or the importance the kernel places upon reclaiming memory used <span class="keyword">for</span> caching directory and inode objects. The default of 100 or relative <span class="string">"fair"</span> is appropriate <span class="keyword">for</span> compute servers. Set to lower than 100 <span class="keyword">for</span> file servers on <span class="built_in">which</span> the cache should be a priority. Set higher, maybe 500 to 1000, <span class="keyword">for</span> interactive systems.</span><br><span class="line"></span><br><span class="line">overcommit_memory       Allows <span class="keyword">for</span> poorly designed programs <span class="built_in">which</span> malloc() huge amounts of memory <span class="string">"just in case"</span> but never really use it. Set this to 0 (disabled) unless you really need it.</span><br></pre></td></tr></table></figure>

<h4 id="deadline-parameters"><a href="#deadline-parameters" class="headerlink" title="deadline parameters"></a><a href="https://cromwell-intl.com/open-source/performance-tuning/disks.html" target="_blank" rel="noopener">deadline parameters</a></h4><p>fifo_batch      Number of read or write operations to issue in one batch.  Lower values may further reduce latency. Higher values can increase throughput on rotating mechanical disks, but at the cost of worse latency. You selected the deadline scheduler to limit latency, so you probably don’t want to increase this, at least not by very much.</p>
<p>read_expire     Number of milliseconds within which a read request should be served. Reduce this from the default of 500 to 100 on a system with interactive users.</p>
<p>rite_expire     Number of milliseconds within which a write request should be served.<br>Leave at default of 5000, let write operations be done asynchronously in the background unless your specialized application uses many synchronous writes.</p>
<p>writes_starved  Number read batches that can be processed before handling a write batch. Increase this from default of 2 to give higher priority to read operations.</p>
<p>nr_requests     Maximum number of read and write requests that can be queued at one time before the next process requesting a read or write is put to sleep. Default value of 128 means 128 read requests and 128 write requests can be queued at once. Larger values may increase throughput for workloads writing many small files, smaller values increase throughput with larger I/O operations. You could decrease this if you are using latency-sensitive applications, but then you shouldn’t be using NOOP if latency is sensitive!</p>
<p>read_ahead_kb   Number of kilobytes the kernel will read ahead during a sequential read operation. 128 kbytes by default, if the disk is used with LVM the device mapper may benefit from a higher value. If your workload does a lot of large streaming reads, larger values may improve performance.</p>
<p>max_sectors_kb  Maximum allowed size of an I/O request in kilobytes, which must be within these bounds:<br>Min value = max(1, logical_block_size/1024)<br>Max value = max_hw_sectors_kb</p>
<p>rotational      Should be 0 (no) for solid-state disks, but some do not correctly report their status to the kernel. If incorrectly set to 1 for an SSD, set it to 0 to disable unneeded scheduler logic meant to reduce number of seeks.</p>
<h3 id="scsi-driver-error-handaling-EH"><a href="#scsi-driver-error-handaling-EH" class="headerlink" title="scsi_driver error handaling (EH)"></a>scsi_driver error handaling (EH)</h3><p>scsi driver error Handaling (EH) timeout – eh_timeout (from default 10 second to 5 seconds)<br>HBA reset time - eh_deadline (from disable/0 to 5 seconds, default was off)</p>
<p>The SCSI error handling (EH) mechanism attempts to perform error recovery on failed SCSI devices. The SCSI host object eh_deadline parameter enables you to configure the maximum amount of time for the recovery. After the configured time expires, SCSI EH stops and resets the entire host bus adapter (HBA).</p>
<h3 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a><a href="https://stackoverflow.com/questions/4458183/how-the-util-of-iostat-is-computed" target="_blank" rel="noopener">iostat</a></h3><p>%util = blkio.ticks / deltams * 100%</p>
<p>deltams is the time elapsed since last snapshot in ms. It uses CPU stats from /proc/stat presumably because it gives better results than to rely on system time, but I don’t know for sure. (Side note: for some reason the times are divided by HZ, while the documentation states it’s in USER_HZ, I don’t understand that.)<br>blkio.ticks is “# of milliseconds spent doing I/Os”, from /proc/diskstats docs:</p>
<p>Field  9 – # of I/Os currently in progress<br>  The only field that should go to zero. Incremented as requests are<br>  given to appropriate struct request_queue and decremented as they finish.<br>Field 10 – # of milliseconds spent doing I/Os<br>  This field increases so long as field 9 is nonzero.</p>
<p>struct ext_disk_stats *xds<br>xds-&gt;util</p>
<p>Hypothesis:<br>Simple understand about util% =  IO time/the time(the clock) , if IO time &gt;= the time, the utils = 100%(in 1s), else, that means some times there is no IO ops in this second<br>Why no ops in this sec ? maybe something hang, maybe just no any IO loading</p>
<p>In SAS arch, there are multiple lane/phy in it, if single lane/phy has full IO loading, the util% will reach 100%<br>Yes the device could be parallel, there are a lot of lane/phy are free, but the single lane or phy has full. I think the util% was right</p>
<ol>
<li>The single path(resource) was full (nvme,sas ssd)</li>
<li>If the block device ‘s performance is infinity,  All of CPU/Mem/NIC speed behind it. the bottle-neck not in the block device. some of syscall cause the util%=100% too</li>
<li>Could the util% show the device are busy ? That right, util% is not enough, Could the iostat show the device are busy ? Yes, it can<br>You can watch the await and util%, you could know the device busy or free. it ‘s so easy</li>
</ol>
<p>“iostat was not correct, it just show the wrong value”. That ‘s alarmist for iostat ,and it ‘s so funny</p>
<p>This guy analyzed the code, but it ‘s not enough, he was not understand the iostat output.<br>The <a href="https://bean-li.github.io/dive-into-iostat/" target="_blank" rel="noopener">example</a> is good.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ID      Time    Ops                                  in_flight  stamp   stamp_delta           io_ticks            time_in_queue</span><br><span class="line">0       100     new request in the queue                0       0       no need caculate          0                 0</span><br><span class="line">1       100.10  another request go to the queue         1       100     100.10-100 &#x3D; 0.1        0.1                 0.1</span><br><span class="line">2       101.20  finish the first request                2       100.10  101.20-100.10 &#x3D; 1.1     1.2(1.1+0.1)        0.1+1.1*2 (total 2x io requests) &#x3D; 2.3</span><br><span class="line">3       103.60  finish the second request               1       101.20  103.60-101.20 &#x3D; 2.4     3.6                 2.3+2.4*1&#x3D;4.7</span><br><span class="line">4       153.60  The third request go to the queue       0       103.60  no need caculate        3.6                 4.7</span><br><span class="line">5       153.90  Finish the third request                1       153.60  153.90 - 153.60 &#x3D; 0.3   3.9                 4.7+0.3 * 1&#x3D; 5</span><br></pre></td></tr></table></figure>
<p>In 53.9s, All io requests in 3.9s, the other times has no any IO in the queue.<br>io_ticks  –&gt; util %<br>time_in_queue –&gt; avgqu-sz</p>
<h3 id="Re-import-LVM"><a href="#Re-import-LVM" class="headerlink" title="Re-import LVM"></a>Re-import LVM</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ storcli64 /call/fall show</span><br><span class="line">$ storcli64 /c0/fall import</span><br><span class="line">$ vgscan</span><br><span class="line"> Reading all physical volumes.  This may take a <span class="keyword">while</span>...</span><br><span class="line">  Found volume group <span class="string">"xx"</span> using metadata <span class="built_in">type</span> lvm2</span><br><span class="line">$ vgchange -ay</span><br><span class="line">  1 logical volume(s) <span class="keyword">in</span> volume group <span class="string">"xx"</span> now active</span><br><span class="line">$ lvs</span><br><span class="line">xx_lv  xx_vg -wi<span class="_">-a</span>-----  1t</span><br><span class="line">$ lvdisplay</span><br></pre></td></tr></table></figure>

<h3 id="Advance-reformat-4Kn-SCSI-devs-to-512-Bytes-sector"><a href="#Advance-reformat-4Kn-SCSI-devs-to-512-Bytes-sector" class="headerlink" title="Advance reformat 4Kn SCSI devs to 512 Bytes sector"></a>Advance reformat 4Kn SCSI devs to 512 Bytes sector</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ smartctl -a -d scsi /dev/sdd</span><br><span class="line">smartctl 5.43 2012-06-30 r3573 [x86_64-linux-2.6.32-504.30.3.el6_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-12 by Bruce Allen, http://smartmontools.sourceforge.net</span><br><span class="line"></span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUH728080AL4200</span><br><span class="line">Revision:             A7J0</span><br><span class="line">User Capacity:        8,001,563,222,016 bytes [8.00 TB]</span><br><span class="line">Logical block size:   4096 bytes</span><br><span class="line"></span><br><span class="line">$ smartctl -a -d scsi /dev/sdc</span><br><span class="line">smartctl 5.43 2012-06-30 r3573 [x86_64-linux-2.6.32-504.30.3.el6_lustre.x86_64] (<span class="built_in">local</span> build)</span><br><span class="line">Copyright (C) 2002-12 by Bruce Allen, http://smartmontools.sourceforge.net</span><br><span class="line"></span><br><span class="line">Vendor:               HGST</span><br><span class="line">Product:              HUH728080AL4200</span><br><span class="line">Revision:             A7J0</span><br><span class="line">User Capacity:        8,001,563,222,016 bytes [8.00 TB]</span><br><span class="line">Logical block size:   512 bytes</span><br></pre></td></tr></table></figure>

<p>Update: If you have a lot of drives to format, it may take a long time with sg_format as it wait until it finished before doing the next one (with a while loop I mean). Useful tip is to use the “-e” flag with sg_format then monitor operations with sg_turs -p but it may hang you tty as well.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ cat /sys/class/block/sdc/queue/hw_sector_size</span><br><span class="line">4096</span><br><span class="line"></span><br><span class="line">$ sg_format --format --size=512 /dev/sdc</span><br><span class="line">HGST      HUH728080AL4200   A7J0   peripheral_type: disk [0x0]</span><br><span class="line">&lt;&lt; supports protection information&gt;&gt;</span><br><span class="line">Mode Sense (block descriptor) data, prior to changes:</span><br><span class="line">  Number of blocks=1953506646 [0x74702556]</span><br><span class="line">  Block size=4096 [0x1000]</span><br><span class="line"></span><br><span class="line">A FORMAT will commence <span class="keyword">in</span> 10 seconds</span><br><span class="line">    ALL data on /dev/sdc will be DESTROYED</span><br><span class="line">        Press control-C to abort</span><br><span class="line">A FORMAT will commence <span class="keyword">in</span> 5 seconds</span><br><span class="line">    ALL data on /dev/sdc will be DESTROYED</span><br><span class="line">        Press control-C to abort</span><br><span class="line"></span><br><span class="line">Format has started</span><br><span class="line">FORMAT Complete</span><br><span class="line"></span><br><span class="line">$ sg_turs -v /dev/sdb</span><br><span class="line">   <span class="built_in">test</span> unit ready cdb: 00 00 00 00 00 00</span><br><span class="line">   <span class="built_in">test</span> unit ready:  Descriptor format, current;  Sense key: Not Ready Additional sense: Logical unit not ready, format <span class="keyword">in</span> progress</span><br><span class="line">   Descriptor <span class="built_in">type</span>: Information    00 00 00 00 00 00 00 00 00 00</span><br><span class="line">   Descriptor <span class="built_in">type</span>: Command specific    0x0000000000000000</span><br><span class="line">   device not ready</span><br><span class="line"></span><br><span class="line">$ sg_inq -v /dev/sdb</span><br><span class="line"><span class="comment"># it will show the process of percent</span></span><br><span class="line"></span><br><span class="line">$ cat /sys/class/block/sdc/queue/hw_sector_size</span><br><span class="line">512</span><br></pre></td></tr></table></figure>

<h4 id="intel-nvme-switch-to-AF"><a href="#intel-nvme-switch-to-AF" class="headerlink" title="intel nvme switch to AF"></a>intel nvme switch to AF</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#change 512 Bytes to 4096 Bytes</span></span><br><span class="line">$ isdct start -intelssd 0 Function=NVMEFormat LBAForma=3 SecureEraseSetting=0 ProtectionInformation=0 MetaDataSetting=0</span><br></pre></td></tr></table></figure>

<h4 id="SATA-only-support-512-and-4096-Bytes"><a href="#SATA-only-support-512-and-4096-Bytes" class="headerlink" title="SATA only support 512 and 4096 Bytes"></a>SATA only support 512 and 4096 Bytes</h4><h4 id="long-data-sector"><a href="#long-data-sector" class="headerlink" title="long-data-sector"></a>long-data-sector</h4><p>From vendor spec, only SAS device support this feature<br>You could format the driver logic sector to 4096,4112,4116,4224KB or 512,520 Bytes<br>SATA device only support 4096 or 512</p>
<p>You could get the logic sector size(4096,4112,4224), the size could be read from driver</p>
<h5 id="T10-DIF-PI"><a href="#T10-DIF-PI" class="headerlink" title="T10 DIF/PI"></a><a href="https://www.seagate.com/files/staticfiles/docs/pdf/whitepaper/safeguarding-data-from-corruption-technology-paper-tp621us.pdf" target="_blank" rel="noopener">T10 DIF/PI</a></h5><p>T10 DIF/PI for avoid data corruption from some device no CRC/ECC product, so it check the data consistency from the source to the destination</p>
<p>The user data in ecc memory(512B) —–&gt; SAS HBA/iscsi driver(520B) -&gt; SAS IO expander/or some very complicated network contains HBA or the others -&gt; Storage medium(520)<br>Because the hardware just check from input or output port from itself</p>
<p>The risk of corrupted data falling through the inherent cracks in this protection methodology is significant, and the havoc such undetected, silent data corruption could wreak (lost or inaccurate data, significant downtime) is substantial</p>
<h5 id="About-DIF-520-Bytes"><a href="#About-DIF-520-Bytes" class="headerlink" title="About DIF 520 Bytes"></a><a href="https://lwn.net/Articles/280023/" target="_blank" rel="noopener">About DIF 520 Bytes</a></h5><p>SCSI drives can usually be reformatted to 520-byte sectors, yielding 8 extra bytes per sector.  These 8 bytes have traditionally been used by RAID controllers to store internal protection information.</p>
<p>DIF (Data Integrity Field) is an extension to the SCSI Block Commands that standardizes the format of the 8 extra bytes and defines ways to interact with the contents at the protocol level.  We refer to the extra information as “integrity metadata” or “IMD”.<br>Each 8-byte DIF tuple is split into three chunks:<br>        * a 16-bit guard tag containing a CRC of the 512-byte data portion of the sector.<br>        * a 16-bit application tag which is up for grabs.<br>        * a 32-bit reference tag which contains an incrementing counter for each sector.  For DIF Type 1 it also needs to match the physical LBA on the drive.<br>There are three types of DIF defined: Type 1, Type 2, and Type 3.  My patches are Type 1 only, although Type 3 devices should work.  Type 2 depends on 32-byte CDBs and is in progress.<br>Since the DIF tuple format is standardized, both initiators and targets (as well as potentially transport switches in-between) to verify the integrity of the data going over the bus.<br>When writing, the HBA will DMA 512-byte sectors from host memory, generate the matching integrity metadata and send out 520-byte sectors on the wire.  The disk will verify the integrity of the data before committing it to stable storage<br>When reading, the drive will send 520-byte sectors to the HBA.  The HBA will verify the data integrity and DMA 512-byte sectors to host memory.<br>IOW, DIF provides means for added integrity protection between HBA and disk</p>
<h3 id="Security-erasing"><a href="#Security-erasing" class="headerlink" title="Security erasing"></a>Security erasing</h3><p>SAS</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sg_format --format /dev/sdx</span><br></pre></td></tr></table></figure>

<p>SATA</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hdparm --user-master u --security-set-pass password /dev/sdx</span><br></pre></td></tr></table></figure>

<p>NVME</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ nvme list</span><br><span class="line">Node             SN                   Model                                    Namespace Usage                      Format           FW Rev</span><br><span class="line">---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------</span><br><span class="line">/dev/nvme0n1     xxxxxxxxxxxxxxx      INTEL SSDPED1K375GA                      1         375.08  GB / 375.08  GB    512   B +  0 B   E2010324</span><br><span class="line"></span><br><span class="line">$ nvme id-ctrl -H  /dev/nvme0n1 | grep format -i</span><br><span class="line">  [1:1] : 0x1   Format NVM Supported</span><br><span class="line">  [0:0] : 0     Admin Vendor Specific Commands uses Vendor Specific Format</span><br><span class="line">  [0:0] : 0     Format Applies to Single Namespace(s)</span><br><span class="line">  [0:0] : 0     NVM Vendor Specific Commands uses Vendor Specific Format</span><br><span class="line"></span><br><span class="line">$ nvme format /dev/nvme0n1 --ses=1</span><br><span class="line"></span><br><span class="line">$ nvme id-ns /dev/nvme1n1 | grep LBA</span><br><span class="line">LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0x1 Better</span><br><span class="line">LBA Format  1 : Metadata Size: 8   bytes - Data Size: 512 bytes - Relative Performance: 0x3 Degraded</span><br><span class="line">LBA Format  2 : Metadata Size: 0   bytes - Data Size: 4096 bytes - Relative Performance: 0 Best  (<span class="keyword">in</span> use)</span><br><span class="line">LBA Format  3 : Metadata Size: 8   bytes - Data Size: 4096 bytes - Relative Performance: 0x2 Good</span><br><span class="line"></span><br><span class="line">$ nvme format /dev/nvme1n1 -l 2</span><br></pre></td></tr></table></figure>

<h3 id="Enable-the-blk-mq-and-scsi-mq"><a href="#Enable-the-blk-mq-and-scsi-mq" class="headerlink" title="Enable the blk_mq and scsi_mq"></a>Enable the blk_mq and scsi_mq</h3><p>scsi-mq:specify scsi_mod.use_blk_mq=y<br>blk-mq infrastructure if the dm_mod.use_blk_mq=y</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ grubby --update-kernel=ALL --args=<span class="string">'scsi_mod.use_blk_mq=y dm_mod.use_blk_mq=y'</span></span><br><span class="line">$ reboot</span><br><span class="line">$ cat /sys/block/dm-X/dm/use_blk_mq</span><br></pre></td></tr></table></figure>

<h3 id="lsscsi"><a href="#lsscsi" class="headerlink" title="lsscsi"></a>lsscsi</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ lsscsi -t | grep sdio</span><br><span class="line">[4:1:8:0]   disk    sas:0x5000c657afc33e1d          /dev/sdio</span><br><span class="line"></span><br><span class="line">$ lsscsi -t -L 4:1:8:0</span><br><span class="line">[4:1:8:0]   disk    sas:0x5000c657afc33e1d          /dev/sdio</span><br><span class="line">  transport=sas</span><br><span class="line">  vendor=HGST</span><br><span class="line">  model=HUH721010AL5200</span><br><span class="line">  bay_identifier=7</span><br><span class="line">  enclosure_device:Slot07</span><br><span class="line">  enclosure_identifier=0x50030103403449ca</span><br><span class="line">  initiator_port_protocols=none</span><br><span class="line">  initiator_response_timeout=1744</span><br><span class="line">  I_T_nexus_loss_timeout=1744</span><br><span class="line">  phy_identifier=33</span><br><span class="line">  ready_led_meaning=0</span><br><span class="line">  sas_address=0x5000c657afc33e1d</span><br><span class="line">  target_port_protocols=ssp</span><br><span class="line">  tlr_enabled=0</span><br><span class="line">  tlr_supported=0</span><br><span class="line"></span><br><span class="line">$ lsscsi -gis | grep sdio</span><br><span class="line">[4:1:8:0]   disk    HGST     HUH721010AL5200  A21D  /dev/sdio  35000c657afc33e1c  /dev/sg180  10.0TB</span><br><span class="line"></span><br><span class="line">$ lsblk -o NAME,MODEL,SERIAL,SIZE,STATE,TYPE,WWN --nodeps | grep sdio</span><br><span class="line">sdio HUH721010AL5200  5000c657afc33e1c                   9.1T running disk 0x5000c657afc33e1c</span><br><span class="line"></span><br><span class="line">$ $ lsscsi -t -H</span><br><span class="line">[0]    megaraid_sas</span><br><span class="line">[10]    ahci          sata:</span><br><span class="line">[11]    ahci          sata:</span><br><span class="line">[12]    mpt3sas       sas:0x500605b00bfa0982</span><br><span class="line">[13]    mpt3sas       sas:0x500605b00acf0542</span><br><span class="line">[14]    mpt3sas       sas:0x500605b00bfac033</span><br><span class="line"></span><br><span class="line">$ lsscsi -tiv</span><br><span class="line">[0:0:7:0]    disk    sas:0x500056b3787358c7          /dev/sdh   -</span><br><span class="line">  dir: /sys/bus/scsi/devices/0:0:7:0  [/sys/devices/pci0000:ae/0000:ae:00.0/0000:af:00.0/host0/port-0:0/expander-0:0/port-0:0:7/end_device-0:0:7/target0:0:7/0:0:7:0]</span><br><span class="line">[0:0:8:0]    disk    sas:0x500056b3787358c8          /dev/sdi   -</span><br><span class="line">  dir: /sys/bus/scsi/devices/0:0:8:0  [/sys/devices/pci0000:ae/0000:ae:00.0/0000:af:00.0/host0/port-0:0/expander-0:0/port-0:0:8/end_device-0:0:8/target0:0:8/0:0:8:0]</span><br><span class="line">[0:0:9:0]    disk    sas:0x5000cca26c1b8631          /dev/sdj   35000cca26c1b8630</span><br></pre></td></tr></table></figure>

<h3 id="udevinfo"><a href="#udevinfo" class="headerlink" title="udevinfo"></a><a href="https://sites.google.com/site/itmyshare/system-admin-tips-and-tools/udevadm---useage-examples" target="_blank" rel="noopener">udevinfo</a></h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ udevadm info --query=property --name /dev/sdd</span><br><span class="line">$ udevadm info --query=path --name /dev/sdd</span><br><span class="line">$ udevadm info --query=symlink --name /dev/sdd</span><br><span class="line">$ udevadm info --a --name=/dev/sdd</span><br><span class="line">$ udevadm info --attribute-walk --name /dev/sdd</span><br><span class="line"></span><br><span class="line">$ udevadm info -a -p $(udevadm info -q path -n /dev/sda)</span><br><span class="line">$ udevadm info -a -p /sys/class/net/eth0</span><br><span class="line">$ udevadm info -q path -n /dev/sda1</span><br><span class="line"></span><br><span class="line"><span class="comment"># trigger, remember to have --dry-run option if you run it on production node.</span></span><br><span class="line">$ udevadm trigger --verbose --dry-run --<span class="built_in">type</span>=devices --subsystem-match=scsi_host</span><br><span class="line">$ udevadm trigger --verbose --dry-run --<span class="built_in">type</span>=devices --subsystem-match=scsi_disk</span><br><span class="line">$ udevadm trigger --verbose --dry-run --<span class="built_in">type</span>=devices --subsystem-match=scsi_tape</span><br><span class="line">$ udevadm trigger --verbose --dry-run --<span class="built_in">type</span>=devices --subsystem-match=fc_host </span><br><span class="line"></span><br><span class="line"><span class="comment"># This option only waits for events triggered by the same command to finish</span></span><br><span class="line">$ udevadm settle</span><br><span class="line"></span><br><span class="line"><span class="comment"># control </span></span><br><span class="line">$ udevadm control --reload-rules</span><br><span class="line">$ --<span class="built_in">log</span>-priority=&lt;level&gt;   <span class="built_in">set</span> the udev <span class="built_in">log</span> level <span class="keyword">for</span> the daemon</span><br><span class="line"></span><br><span class="line"><span class="comment"># monitor</span></span><br><span class="line">$ udevadm monitor --<span class="built_in">help</span></span><br><span class="line">Usage: udevadm monitor [--property] [--kernel] [--udev] [--<span class="built_in">help</span>]</span><br><span class="line">  --property                    <span class="built_in">print</span> the event properties</span><br><span class="line">  --kernel                      <span class="built_in">print</span> kernel uevents</span><br><span class="line">  --udev                        <span class="built_in">print</span> udev events</span><br><span class="line">  --subsystem-match=&lt;subsystem&gt; filter events</span><br><span class="line">  --<span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># test</span></span><br><span class="line">$ udevadm <span class="built_in">test</span> /block/sdd</span><br></pre></td></tr></table></figure>

<h3 id="Flush-cache-data"><a href="#Flush-cache-data" class="headerlink" title="Flush cache data"></a>Flush cache data</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## flush fs buff</span></span><br><span class="line">$ sync</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flush fs buff</span></span><br><span class="line">$ blockdev --flushbufs /dev/sdxx</span><br><span class="line"></span><br><span class="line"><span class="comment">#or for zfs</span></span><br><span class="line">$ zpool <span class="built_in">export</span> your_pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flush the SAS dev buff</span></span><br><span class="line">$ sdparm --<span class="built_in">command</span>=sync /dev/sdxx</span><br><span class="line"><span class="comment">#spindown the device</span></span><br><span class="line">$ sdparm --<span class="built_in">command</span>=stop /dev/sdxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># Flush the SATA dev buff</span></span><br><span class="line">$ hdparm -F /dev/sdxx</span><br><span class="line"></span><br><span class="line">$ storcli64 /c0 <span class="built_in">set</span> flushwriteverify=on</span><br><span class="line">$ storcli64 /c0 show flushwriteverify</span><br><span class="line">$ storcli64 /c0 flushcache</span><br><span class="line">Description = Adapter and/or disk caches flushed successfully.</span><br></pre></td></tr></table></figure>

<h3 id="dev-info"><a href="#dev-info" class="headerlink" title="dev info"></a><a href="https://www.ibm.com/support/knowledgecenter/en/linuxonibm/com.ibm.linux.z.lgdd/lgdd_t_fcp_wrk_actinfo.html" target="_blank" rel="noopener">dev info</a></h3><p>/sys/class/scsi_device/<device_name>/device/<attribute></p>
<p>ioerr_cnt       The number of SCSI commands that completed with an error.<br>scsi_level      The SCSI revision level, received from inquiry data.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">access_denied   Flag that indicates whether access to the device is restricted by the FCP channel.The value is 1 if access is denied and 0 if access is permitted.If access is denied to your Linux instance, confirm that your SCSI devices are configured as intended. Also, be sure that you really want to share a SCSI device. For shared access to a SCSI device, preferably use NPIV). You might also use different FCP channels or target ports.</span><br><span class="line">access_shared   This attribute is obsolete. The value is always 0.</span><br><span class="line">access_readonly This attribute is obsolete. The value is always 0.</span><br><span class="line">ked     Flag that indicates whether the device is in blocked state (0 or 1).</span><br><span class="line">iocounterbits   The number of bits used for I&#x2F;O counters.</span><br><span class="line">iodone_cnt      The number of completed or rejected SCSI commands.</span><br><span class="line">ioerr_cnt       The number of SCSI commands that completed with an error.</span><br><span class="line">iorequest_cnt   The number of issued SCSI commands.</span><br><span class="line">queue_type      The type of queue for the SCSI device. The value can be one of the following:</span><br><span class="line">                none</span><br><span class="line">                simple</span><br><span class="line">                ordered</span><br><span class="line">model   The model of the SCSI device, received from inquiry data.</span><br><span class="line">rev     The revision of the SCSI device, received from inquiry data.</span><br><span class="line">scsi_level      The SCSI revision level, received from inquiry data.</span><br><span class="line">type    The type of the SCSI device, received from inquiry data.</span><br><span class="line">vendor  The vendor of the SCSI device, received from inquiry data.</span><br><span class="line">fcp_lun The LUN of the SCSI device in 64-bit format.</span><br><span class="line">hba_id  The bus ID of the SCSI device.</span><br><span class="line">wwpn    The WWPN of the remote port.</span><br><span class="line">zfcp_access_denied      Flag that indicates whether access to the device is restricted by the FCP channel.The value is 1 if access is denied and 0 if access is permitted. If access is denied to your Linux instance, confirm that your SCSI devices are configured as intended. Also, be sure that you really want to share a SCSI device. For shared access to a SCSI device, preferably use NPIV). You might also use different FCP channels or target ports.</span><br><span class="line">zfcp_in_recovery        Shows if unit is in recovery (0 or 1).Shows if unit is in recovery (2 or 1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Encrypt block device</span><br><span class="line">#### *Input password*     </span><br><span class="line">or you can not use openssl          </span><br><span class="line">&#96;&#96;&#96; bash</span><br><span class="line">cryptsetup luksFormat --cipher aes-xts-plain64 --key-size 512 --hash sha256 &#x2F;dev&#x2F;vdb     </span><br><span class="line">WARNING!     </span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;     </span><br><span class="line">This will overwrite data on &#x2F;dev&#x2F;vdb irrevocably.     </span><br><span class="line">     </span><br><span class="line">Are you sure? (Type uppercase yes): YES      </span><br><span class="line">Enter passphrase:      </span><br><span class="line">Verify passphrase:</span><br></pre></td></tr></table></figure>

<h4 id="mapper-test-device"><a href="#mapper-test-device" class="headerlink" title="mapper test device"></a><em>mapper test device</em></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup luksOpen /dev/vdb <span class="built_in">test</span>     </span><br><span class="line">Enter passphrase <span class="keyword">for</span> /dev/vdb:     </span><br><span class="line">``` </span><br><span class="line">or     </span><br><span class="line">``` bash</span><br><span class="line"><span class="built_in">cd</span> /dev/shm     </span><br><span class="line">openssl rand 128 -hex -out ./key     </span><br><span class="line">openssl aes-256-cbc -<span class="keyword">in</span> ./key -out key.enc     </span><br><span class="line"></span><br><span class="line">Verifying - enter aes-256-cbc encryption password:</span><br><span class="line">openssl aes-256-cbc -d -<span class="keyword">in</span> ./key.enc | cryptsetup [--allow-discards] --key-file=- luksOpen /dev/vdb <span class="built_in">test</span>     </span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">If you have a SSD, you may want to use --allow-discards. It creates an information leak but it enhances your SSD’s lifetime.     </span><br><span class="line"></span><br><span class="line">``` bash</span><br><span class="line">mkfs.ext4 /dev/mapper/<span class="built_in">test</span>     </span><br><span class="line">cryptsetup luksDump /dev/vdb     </span><br><span class="line">     </span><br><span class="line">``` bash     </span><br><span class="line">LUKS header information <span class="keyword">for</span> /dev/vdb     </span><br><span class="line">Key Slot 0: ENABLED     </span><br><span class="line">	Iterations:         	80604     </span><br><span class="line">	Salt:               	c4 c0 08 9c 77 ef 57 a5 d2 62 f4 94 03 56 24 7a      </span><br><span class="line">	                      	86 0c f4 c6 06 6f 9e 01 80 <span class="built_in">fc</span> 0f 1d 3b a9 59 87      </span><br><span class="line">	Key material offset:	8     </span><br><span class="line">	AF stripes:            	4000     </span><br><span class="line">Key Slot 1: DISABLED</span><br></pre></td></tr></table></figure>

<h4 id="Add-remove-key"><a href="#Add-remove-key" class="headerlink" title="Add/remove key"></a><em>Add/remove key</em></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup luksAddKey --key-slot 1 /dev/vdb     </span><br><span class="line">cryptsetup luksRemoveKey /dev/vdb</span><br></pre></td></tr></table></figure>

<h4 id="Back-restore-header"><a href="#Back-restore-header" class="headerlink" title="Back/restore header"></a><em>Back/restore header</em></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup luksHeaderBackup /dev/vdb --header-backup-file /root/vdb-header-backup     </span><br><span class="line">cryptsetup luksHeaderRestore /dev/vdb --header-backup-file /root/vdb-header-backup</span><br></pre></td></tr></table></figure>

<h4 id="is-luks-partition"><a href="#is-luks-partition" class="headerlink" title="is luks partition"></a><em>is luks partition</em></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cryptsetup -v isLuks /dev/vdb</span><br></pre></td></tr></table></figure>

<h4 id="Clean-luks-partition"><a href="#Clean-luks-partition" class="headerlink" title="Clean luks partition"></a><em>Clean luks partition</em></h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">head -c 3145728 /dev/zero &gt; /dev/vdb;sync</span><br></pre></td></tr></table></figure>
<p>The default LUKS header (with only one key-slot enabled) takes 1052672 bytes, what is slightly more than 1 MiB. Having 2 key-slots enabled this would extend the header almost twice (key-slots * stripes * keysize + offset bytes). Therefore overwriting the first 3 MiB would do the job for us </p>
]]></content>
      <categories>
        <category>scsi</category>
      </categories>
      <tags>
        <tag>block</tag>
        <tag>schdule</tag>
      </tags>
  </entry>
  <entry>
    <title>Hardware info</title>
    <url>/2016/06/16/hardware/</url>
    <content><![CDATA[<h3 id="DMI-3-0"><a href="#DMI-3-0" class="headerlink" title="DMI 3.0"></a>DMI 3.0</h3><p><img src="/img/Dt9p6.jpg" alt=""><br>DMI 2.0 2 GB/s<br>DMI 3.0 3.93 GB/s<br>SATA SSD 500MB/s x 4/8 = 2000/4000MB/s<br><a href="http://electronics.stackexchange.com/questions/219440/why-do-cpus-typically-connect-to-only-one-bus/219446" target="_blank" rel="noopener">reference</a></p>
<a id="more"></a>

<h3 id="SAS-cable"><a href="#SAS-cable" class="headerlink" title="SAS cable"></a>SAS cable</h3><h4 id="SAS3-12Gbps-x-4-lane-performance"><a href="#SAS3-12Gbps-x-4-lane-performance" class="headerlink" title="SAS3 12Gbps x 4 lane performance"></a>SAS3 12Gbps x 4 lane performance</h4><p>not make sure</p>
<h3 id="NIC"><a href="#NIC" class="headerlink" title="NIC"></a>NIC</h3><h4 id="10GbE"><a href="#10GbE" class="headerlink" title="10GbE"></a><a href="http://www.missioncriticalmagazine.com/ext/resources/MC/Home/Files/PDFs/WP_Blade_Ethernet_Cabling.pdf" target="_blank" rel="noopener">10GbE</a></h4><p><img src="/img/sfp-10bt-1.png" alt=""><br><img src="/img/sfp-10bt-2.png" alt=""></p>
<h3 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h3><h4 id="NVME-Interface-reference"><a href="#NVME-Interface-reference" class="headerlink" title="NVME Interface reference"></a><a href="http://www.nvmexpress.org/wp-content/uploads/NVMe_Infrastructure_final1.pdf" target="_blank" rel="noopener">NVME Interface reference</a></h4><p><img src="/img/nvme-1.png" alt=""><br><img src="/img/nvme-2.png" alt=""><br><img src="/img/nvme-3.png" alt=""><br><img src="/img/nvme-4.png" alt=""><br><img src="/img/nvme-5.png" alt=""><br><img src="/img/nvme-6.png" alt=""><br><img src="/img/nvme-7.png" alt=""><br><img src="/img/nvme-8.png" alt=""><br><img src="/img/nvme-9.png" alt=""><br><img src="/img/nvme-10.png" alt=""><br><img src="/img/nvme-11.png" alt=""></p>
<h4 id="supermicro-benchmark"><a href="#supermicro-benchmark" class="headerlink" title="supermicro benchmark"></a><a href="https://www.supermicro.com/white_paper/white_paper_NVMe.pdf" target="_blank" rel="noopener">supermicro benchmark</a></h4><p><img src="/img/nvme-12.png" alt=""></p>
<h4 id="Risks-of-unexpected-power-loss-on-solid-state-drives"><a href="#Risks-of-unexpected-power-loss-on-solid-state-drives" class="headerlink" title="Risks of unexpected power loss on solid state drives"></a><a href="http://www8.hp.com/h20195/v2/GetPDF.aspx/4AA6-1470ENW.pdf" target="_blank" rel="noopener">Risks of unexpected power loss on solid state drives</a></h4><p>Each time data is accessed or modified on the SSD, metadata is modified pertaining to the state of the data. This information allows for the storage controller to choose what data should be in the drive’s volatile cache at any given moment as well as the ability to implement techniques which increase performance and endurance</p>
<p>The drive then translates the LBA to the Physical Block Address (PBA) by way a flash translation table (FTL). The FTL is stored in cache and it is the responsibility of the controller to flush this table to non-volatile memory at times it deems appropriate or during a clean shutdown. The metadata associated with each page of data written also has information that can be used to rebuild the FTL if it is lost, but this rebuild takes time at the next power-on. If both the FTL and metadata are corrupted due to a sudden power loss event, the data stored on the SSD can become lost or even worse, the SSD, as a whole, can become inaccessible</p>
<p>Turning drive cache off<br>Pros: Data integrity is preserved in the event of a sudden power loss.<br>Cons: Dramatic decrease in read and write performance.<br>Increased writes to NAND resulting in lower write endurance and decreased drive life. FTL data is still stored in the volatile cache, which leaves it at risk</p>
<p>Capacitor hold-up circuitry<br>Pros: Data integrity is preserved in the event of a sudden power loss.<br>Cons: Increased cost due to inclusion of additional circuitry including capacitors. Additional board space is required.</p>
<h4 id="Dell-Solid-State-Drive-FAQ"><a href="#Dell-Solid-State-Drive-FAQ" class="headerlink" title="Dell Solid-State-Drive-FAQ"></a><a href="http://www.dell.com/downloads/global/products/pvaul/en/Solid-State-Drive-FAQ-us.pdf" target="_blank" rel="noopener">Dell Solid-State-Drive-FAQ</a></h4><h3 id="Enterprise-HDD"><a href="#Enterprise-HDD" class="headerlink" title="Enterprise HDD"></a>Enterprise HDD</h3><h4 id="Rotational-Speed-10K～15K"><a href="#Rotational-Speed-10K～15K" class="headerlink" title="Rotational Speed 10K～15K"></a>Rotational Speed 10K～15K</h4><p>Error Rate (non-recoverable, bits read)  1 sector per 10E16<br> Mean Time Between Failures (MTBF, hours)   &lt;2M<br>Availability (hrs/day x days/wk)  24x7</p>
<h4 id="Rotational-Speed-7200"><a href="#Rotational-Speed-7200" class="headerlink" title="Rotational Speed 7200"></a>Rotational Speed 7200</h4><p>Error Rate (non-recoverable, bits read)  1 sector per 10E15<br>Mean Time Between Failures (MTBF, hours)  &gt;1.2M<br>Availability (hrs/day x days/wk)  24x7</p>
<p><img src="/img/hdd-spec.png" alt=""><br>there are 3 differents sector size in seagate product<br><img src="/img/sector-size.png" alt=""></p>
]]></content>
      <categories>
        <category>arch</category>
      </categories>
      <tags>
        <tag>hardware</tag>
      </tags>
  </entry>
</search>
