<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>lfs command</title>
      <link href="/2019/12/29/lfs_command/"/>
      <url>/2019/12/29/lfs_command/</url>
      
        <content type="html"><![CDATA[<h3 id="Public"><a href="#Public" class="headerlink" title="Public"></a>Public</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=260</span><br><span class="line"></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span></span><br><span class="line"><span class="comment"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span></span><br><span class="line"><span class="comment">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lustre servers is also 40 seconds</span></span><br><span class="line"><span class="comment">#Adaptive Timeouts: In a Lustre file system servers keep track of the time it takes for RPCs to be completed</span></span><br><span class="line"><span class="comment">#The quiescent_time in this formula is to account for the time it takes all Lustre clients to reestablish connections with all Lustre targets following an HSN quiesce. We've experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lustre clients, the number of Lustre targets, the number of Lustre file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"><span class="comment"># Readonly mount</span></span><br><span class="line">$ mount.lfs <span class="variable">$zpool</span> /ost0 -o rdonly_dev</span><br></pre></td></tr></table></figure><p><a href="http://wiki.lustre.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency" target="_blank" rel="noopener">Understanding Lustre Message Loss and Tuning for Resiliency</a></p><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span><br><span class="line">options ptlrpc ldlm_num_threads&#x3D;16</span><br><span class="line"></span><br><span class="line"># Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size&#x3D;clear</span><br><span class="line"></span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 &#x2F;mnt&#x2F;striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  obdfilter.*OST0000*.stats</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;nodemap&#x2F;default&#x2F;exports</span><br><span class="line"></span><br><span class="line"># re-compile the lfs 2.13.0 client, you must import openmpi PATH</span><br><span class="line">$ export PATH&#x3D;&#x2F;usr&#x2F;lib64&#x2F;openmpi&#x2F;bin&#x2F;:$PATH</span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear &#x2F;tmp&#x2F;tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? &#96;too stupid&#96; bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ .&#x2F;configure --enable-client --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;kernels&#x2F;$(uname -r);make rpms&#x2F;deps</span><br><span class="line">## install in ubuntu 18.04</span><br><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;&#x2F;lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ .&#x2F;configure --disable-server --with-linux&#x3D;&#x2F;usr&#x2F;src&#x2F;linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  &#x2F;lib&#x2F;modules&#x2F;4.15.0-64-generic&#x2F;kernel&#x2F;drivers&#x2F;staging&#x2F;lfs&#x2F;</span><br><span class="line">$ depmod -a</span><br><span class="line"></span><br><span class="line">### set lfs client for a lot metadata ops</span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size&#x3D;10000 ldlm.namespaces.*.lru_max_age&#x3D;600000</span><br></pre></td></tr></table></figure><h3 id="metadata-server"><a href="#metadata-server" class="headerlink" title="metadata server"></a>metadata server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param -P timeout&#x3D;300</span><br><span class="line">$ lctl set_param timeout&#x3D;300 </span><br><span class="line"># if you want it work ,you have set it twice...</span><br><span class="line"></span><br><span class="line"># Monitor status</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lus...&#x2F;mdc&#x2F;$&#123;fname&#125;-MDT0000-mdc-ffff88091eff0800&#x2F;state </span><br><span class="line">$ lctl get_param mdc.$fname-MDT*.state</span><br><span class="line">$ lctl get_param mdc.$fname-MDT0000-mdc-*.rpc_stats</span><br><span class="line"></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             1556726087.189561170 secs.nsecs</span><br><span class="line">open                      3412130101 samples [reqs]</span><br><span class="line">close                     2926922120 samples [reqs]</span><br><span class="line">mknod                     293730475 samples [reqs]</span><br><span class="line">link                      20713305 samples [reqs]</span><br><span class="line">unlink                    316042257 samples [reqs]</span><br><span class="line">mkdir                     3275032 samples [reqs]</span><br><span class="line">rmdir                     2731821 samples [reqs]</span><br><span class="line">rename                    7687699 samples [reqs]</span><br><span class="line">getattr                   2060900881 samples [reqs]</span><br><span class="line">setattr                   320658776 samples [reqs]</span><br><span class="line">getxattr                  1080139037 samples [reqs]</span><br><span class="line">setxattr                  222105 samples [reqs]</span><br><span class="line">statfs                    11587278 samples [reqs]</span><br><span class="line">sync                      20670980 samples [reqs]</span><br><span class="line">samedir_rename            7199107 samples [reqs]</span><br><span class="line">crossdir_rename           488592 samples [reqs]</span><br><span class="line"></span><br><span class="line"># not make sure</span><br><span class="line">$ echo 0 &gt; &#x2F;proc&#x2F;fs&#x2F;lfs&#x2F;mdc&#x2F;zfsz4-*&#x2F;active</span><br><span class="line">$ lctl set_param mdc.zfsz4-*.active&#x3D;0</span><br></pre></td></tr></table></figure><h3 id="object-server"><a href="#object-server" class="headerlink" title="object server"></a>object server</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size&#x3D;1048576</span><br><span class="line"></span><br><span class="line">options ost oss_num_threads&#x3D;0</span><br><span class="line"># you can limit the server performance by num_threads </span><br><span class="line"></span><br><span class="line">#limit ost io threads</span><br><span class="line">$ lctl set_param ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line">$ lctl set_param -P ost.OSS.ost_io.threads_max&#x3D;128</span><br><span class="line"></span><br><span class="line"># rpc info</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;lus...&#x2F;osc&#x2F;lus...-OST0000-osc-ffff88103a993000&#x2F;import</span><br><span class="line"></span><br><span class="line"># Get status</span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* timeout</span><br><span class="line">$ lctl get_param llite.*$FNAME*.stats</span><br><span class="line">$ lctl get_param obdfilter.*OST005e*.brw_stats</span><br><span class="line"></span><br><span class="line">#Read and print the last_rcvd file from a device</span><br><span class="line">#display client information</span><br><span class="line">$ lr_reader -c &#x2F;dev&#x2F;sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r &#x2F;dev&#x2F;sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line"># disable ost cache</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line"></span><br><span class="line">#make sure ost mount parameters</span><br><span class="line">$ cat &#x2F;proc&#x2F;fs&#x2F;ldiskfs&#x2F;dm-xx&#x2F;options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid&#x3D;0</span><br><span class="line">resgid&#x3D;0</span><br><span class="line">errors&#x3D;remount-ro</span><br><span class="line">commit&#x3D;5</span><br><span class="line">min_batch_time&#x3D;0</span><br><span class="line">max_batch_time&#x3D;15000</span><br><span class="line">stripe&#x3D;0</span><br><span class="line">data&#x3D;ordered</span><br><span class="line">inode_readahead_blks&#x3D;32</span><br><span class="line">init_itable&#x3D;10</span><br><span class="line">max_dir_size_kb&#x3D;0</span><br></pre></td></tr></table></figure><h3 id="net"><a href="#net" class="headerlink" title="net"></a>net</h3><p>I think in some the bad network quality env, you must improve them</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Peer Credits</span></span><br><span class="line"><span class="comment">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span></span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers </span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue </span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available <span class="keyword">for</span> this peer</span><br><span class="line">min is the smallest number of peer credits seen </span><br><span class="line">Negative credit count indicates the number of messages awaiting a credit</span><br><span class="line"></span><br><span class="line"><span class="comment">#Network Interface Credits</span></span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min </span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796 </span><br><span class="line">max is total available (i.e. value of ko2iblnd credits) </span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line"><span class="comment">#Lctl conn_list–List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span></span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="comment"># the number of concurrent sends (to all peers), defaults:64</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">peer_buffer_credits=256</span><br><span class="line">concurrent_sends=256 - send work-queue sizing</span><br><span class="line"><span class="comment"># not make sure </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">the number of concurrent sends to a single peer, <span class="comment">#default:8</span></span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 128 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line"></span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line"><span class="comment">## Acceptor's listen backlog, the number of the connections the server instance can buffer in the wait queue.</span></span><br><span class="line">options lnet accept_timeout=128</span><br><span class="line"><span class="comment">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span></span><br><span class="line">k</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 <span class="comment"># 1024 = 1024*4KB =4MB per RPC</span></span><br><span class="line"><span class="comment">#Max RPCS in flight between OSC and OST</span></span><br><span class="line">$ lctl set_param -P <span class="variable">$FNAME</span>.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"><span class="comment"># Max number of 4K pages per RPC</span></span><br><span class="line"><span class="comment"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span></span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"><span class="comment"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span></span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"><span class="comment"># Maximum MBs of dirty data that can be written and queued on a client</span></span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line"><span class="comment">## not make sure</span></span><br><span class="line">mds $ <span class="built_in">echo</span> 12 &gt; /proc/fs/lus.../mdc/<span class="variable">$FNAME</span>-MDT0000-mdc-ffff88091eff0800/max_mod_rpcs_in_flight</span><br><span class="line">mds $ lctl set_param mdc.<span class="variable">$FNAME</span>-MDT*-mdc-*.max_mod_rpcs_in_flight=12</span><br><span class="line"></span><br><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show --verbose</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane</span></span><br><span class="line">options lnet networks=<span class="string">"tcp1(eth1),tcp2(eth2),o2ib0(ib0)"</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">"tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]"</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure><h3 id="Trace-log"><a href="#Trace-log" class="headerlink" title="Trace log"></a>Trace log</h3><h4 id="lfs-log"><a href="#lfs-log" class="headerlink" title="lfs log"></a>lfs log</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># F_SETPIPE_SZ F_GETPIPE_SZ</span></span><br><span class="line"><span class="comment"># echo 104857600 &gt; /proc/sys/fs/pipe-max-size</span></span><br><span class="line"><span class="comment"># fs.pipe-max-size=104857600</span></span><br><span class="line"><span class="comment"># ulimit pipe size            (512 bytes, -p) 8 = 4096 bytes, it 's pipe buffer size in the ulimit, not pipe size</span></span><br><span class="line"><span class="comment">## POSIX.1-2001 says that write(2)s of less than PIPE_BUF  bytes  must  be atomic:  the  output  data  is  written  to  the  pipe  as a contiguous sequence.  Writes of more than PIPE_BUF bytes may  be  non-atomic:  the kernel  may  interleave the data with data written by other processes.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here 2 tools for pipe</span></span><br><span class="line"><span class="comment"># pv - Pipe Viewer - is a terminal-based tool for monitoring the progress of data through a pipeline.</span></span><br><span class="line"><span class="comment"># process1 | pv -pterbTCB 1G | process2</span></span><br><span class="line"></span><br><span class="line">$ pv -cN sources linux-image-unsigned-4.15.0-65-generic-dbgsym_4.15.0-65.74_amd64.ddeb | dd of=/tmp/tmp bs=512 | pv -cN cat</span><br><span class="line">      cat: 0.00 B 0:00:00 [0.00 B/s] [&lt;=&gt;                                                                                                                                                                                                    ]</span><br><span class="line">  sources:  751MiB 0:00:03 [ 191MiB/s] [===================================================================================================================================================================================&gt;] 100%            </span><br><span class="line">1538323+1 records <span class="keyword">in</span></span><br><span class="line">1538323+1 records out</span><br><span class="line">787621648 bytes (788 MB, 751 MiB) copied, 3.92424 s, 201 MB/s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ mkfifo -m 777 /tmp/lfs.log</span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it 'll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br></pre></td></tr></table></figure><h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat /etc/kdump.conf<br>nfs my.nfsserver.example.org:/path/to/expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p><h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:user@my.server.example.org">user@my.server.example.org</a>:/dest/path<br>#By default, uses ssh key at /root/.ssh/kdump_id_rsa<br>#core_collector makedumpfile <options></p><h3 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">* MARK – Internal record keeping</span><br><span class="line">* CREAT – Regular file creation</span><br><span class="line">* MKDIR – Directory creation</span><br><span class="line">* HLINK – Hard link</span><br><span class="line">* SLINK – Soft link</span><br><span class="line">* OPEN – open file</span><br><span class="line">* CLOSE – close file</span><br><span class="line">* MKNOD – Other file creation</span><br><span class="line">* UNLNK – Regular file removal</span><br><span class="line">* RMDIR – Directory removal</span><br><span class="line">* RNMFM – Rename, original</span><br><span class="line">* RNMTO – Rename, final</span><br><span class="line">* IOCTL – ioctl on file or directory</span><br><span class="line">* TRUNC – Regular file truncated</span><br><span class="line">* SATTR – Attribute change</span><br><span class="line">* XATTR – Extended Attribute change</span><br><span class="line">* HSM – HSM action</span><br><span class="line">* UNKNW – Unkown operation</span><br></pre></td></tr></table></figure><h4 id="Enable"><a href="#Enable" class="headerlink" title="Enable"></a>Enable</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm.max_requests&#x3D;8</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT0000.changelog_users mdd.$FNAME-MDT0000.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask&#x3D;MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure><h4 id="Disable"><a href="#Disable" class="headerlink" title="Disable"></a>Disable</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#Notify a device that user cl1 no longer needs records (up toand including 3)</span><br><span class="line">$ lfs changelog_clear $FNAME-MDT0000 cl1 3</span><br><span class="line"></span><br><span class="line">#To stop changelogs, changelog_mask should be set to MARK only</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line">mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line"></span><br><span class="line">#or youcan set it -all</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;-all</span><br></pre></td></tr></table></figure><h3 id="FSCK"><a href="#FSCK" class="headerlink" title="FSCK"></a>FSCK</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 <span class="keyword">in</span> dir <span class="comment">#145170469</span></span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem <span class="built_in">read</span>-only</span><br></pre></td></tr></table></figure><h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure><ul><li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure></li><li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p></li><li><p>Can check multiple MDT/OSTs in parallel</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp &#x2F;dev&#x2F;sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy &#x2F;dev&#x2F;sdx</span><br></pre></td></tr></table></figure><h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure><p>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</p><h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.mdt=ug</span><br><span class="line">mds $ cat /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.ost=ug</span><br><span class="line"></span><br><span class="line">client $ lfs setquota –u user1 –b 307200 –B 309200 –i 10000 –I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota –g group1 –b 5120000 –B 5150000 –i 100000 –I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota –u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure><h3 id="Disable-the-ost"><a href="#Disable-the-ost" class="headerlink" title="Disable the ost"></a>Disable the ost</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 deactivate</span><br><span class="line">mds $ lctl --device 8 activate</span><br></pre></td></tr></table></figure><h3 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lfs-OST0000-osc-MDT0000 lfs-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or </span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure><h3 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h3><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ md5sum /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont 't use lfs migrate, it 's too dangerous</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Job status</span></span><br><span class="line">```bash</span><br><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn't any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clear stats for job "dd.0" on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  cat /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure><h3 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]# lfs fid2path &#x2F;mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid &#x2F;mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure><h3 id="increase-openzfs-sync-performance-in-test-env"><a href="#increase-openzfs-sync-performance-in-test-env" class="headerlink" title="increase openzfs sync performance in test env"></a>increase openzfs sync performance in test env</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line"></span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lustre to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> filesystem </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lfs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/12/29/hello-world/"/>
      <url>/2019/12/29/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
