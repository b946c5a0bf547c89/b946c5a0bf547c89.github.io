<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="Lustre is outdated file system, if you are not use MPI, please give up it run it in your Enterprise envall bugs could not be fixed from the root, Just use performance case, Strong not recommand use in">
<meta property="og:type" content="website">
<meta property="og:title" content="Lustre operations">
<meta property="og:url" content="http://yoursite.com/lustre-operations.html">
<meta property="og:site_name" content="b946c5a0bf547c89">
<meta property="og:description" content="Lustre is outdated file system, if you are not use MPI, please give up it run it in your Enterprise envall bugs could not be fixed from the root, Just use performance case, Strong not recommand use in">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2016-04-06T03:17:42.000Z">
<meta property="article:modified_time" content="2019-12-29T04:34:53.840Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="lustre">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/lustre-operations">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: false
  };
</script>

  <title>Lustre operations | b946c5a0bf547c89
  </title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">b946c5a0bf547c89</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>Schedule</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content">
            

  <div class="posts-expand">
    
    
    
    <div class="post-block" lang="en">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Lustre operations
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <p><code>Lustre is outdated file system, if you are not use MPI, please give up it run it in your Enterprise env</code><br><code>all bugs could not be fixed from the root, Just use performance case, Strong not recommand use in enterprise env</code></p>
<h3 id="Use-high-High-frequency-and-less-cores-CPU"><a href="#Use-high-High-frequency-and-less-cores-CPU" class="headerlink" title="Use high High frequency and less cores CPU"></a>Use high High frequency and less cores CPU</h3><p>Because more cpu cores, more task queue and lock cpu overhead, it ‘s bad design but intel like it.<br>It just looks like all cpu resources exhaust</p>
<h3 id="Use-single-socket-server-you-don’t-need-to-tuning-numa"><a href="#Use-single-socket-server-you-don’t-need-to-tuning-numa" class="headerlink" title="Use single socket server, you don’t need to tuning numa"></a>Use single socket server, you don’t need to tuning numa</h3><p>Until now, Xeon E2138 or single Silver 4114 is a enough choice for lustre in dual 10GbE Ethernet port<br>If your backend is SAS SAN, E2138 is better, if it has support 128G memory, waiting Xeon E2236/2278G<br>Looks like Silver 4215 is good enough for OSS node</p>
<h3 id="SAS-HBA-tuning"><a href="#SAS-HBA-tuning" class="headerlink" title="SAS HBA tuning"></a>SAS HBA tuning</h3><p>Because there are a lot of Dell MD1280 with LSI 9207-8e</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/modprobe.d/mpt2sas.conf</span><br><span class="line">options mpt2sas max_msix_vectors=4</span><br><span class="line">options mpt2sas command_retry_count=32 <span class="comment">## decrese the value, default is 144, this value too large cause some broken device could not be ban</span></span><br><span class="line">options mpt2sas max_queue_depth=254 <span class="comment">## default is 600 , for low memory system</span></span><br><span class="line"><span class="comment">##options mpt2sas max_sgl_entries=256 </span></span><br><span class="line"></span><br><span class="line">The Linux “scatter/gather” table size needs to be large enough to allow IO_SIZE IO, <span class="keyword">if</span> possible. For most drivers, this typically requires a “sg_tablesize” value of 256 or greater <span class="keyword">for</span> 4MB IO. Different vendors have different defaults <span class="keyword">for</span> this parameter, and may require a modprobe.conf entry to increase the value. The QLogic driver defaults to a value of 1024. For Emulex, a modeprobe.conf entry needs to be added to increase the value to 256 or greater, such as:</span><br><span class="line">options lpfc lpfc_sg_seg_cnt=256</span><br><span class="line">The LSI SAS driver, “mpt2sas”, uses the parameter named “max_sgl_entries” to control this value. Its maximum value <span class="keyword">in</span> RHEL 6.x currently only 128.</span><br><span class="line"></span><br><span class="line">The value of sg_tablesize can be validated via:</span><br><span class="line">$ lspci  | grep 2308</span><br><span class="line">b3:00.0 Serial Attached SCSI controller: LSI Logic / Symbios Logic SAS2308 PCI-Express Fusion-MPT SAS-2 (rev 05)</span><br><span class="line"></span><br><span class="line">cat /sys/devices/pci0000:b2/0000:b2:00.0/0000:b3:00.0/host17/scsi_host/host17/sg_tablesize</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 256 &gt; /sys/devices/pci0000:b2/0000:b2:00.0/0000:b3:00.0/host17/scsi_host/host17/sg_tablesize</span><br><span class="line">-bash: <span class="built_in">echo</span>: write error: Input/output error</span><br><span class="line"></span><br><span class="line"><span class="comment"># LSI 3008 has the same issue, maybe upgrade driver will modify it.</span></span><br></pre></td></tr></table></figure>

<h3 id="ldiskfs-kenrel-tuning"><a href="#ldiskfs-kenrel-tuning" class="headerlink" title="ldiskfs kenrel tuning"></a><a href="https://i.dell.com/sites/doccontent/shared-content/data-sheets/en/Documents/Commoditization_of_the_High-End_Research_Storage_Market_with_the_Dell_MD3460_Intel_Enterprise_Edition_Lustre.pdf" target="_blank" rel="noopener">ldiskfs kenrel tuning</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for throughput</span></span><br><span class="line">schdule = deadline</span><br><span class="line">nr_request = 1024</span><br><span class="line">max_sector_kb = 1024</span><br><span class="line">read_ahead_kb = 8192</span><br><span class="line">rq_affinity = 2</span><br><span class="line"></span><br><span class="line">vm.dirty_ratio = 10</span><br><span class="line">vm.dirty_background_ratio = 5</span><br><span class="line">vm.vfs_cache_pressure = 50</span><br></pre></td></tr></table></figure>

<h5 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h5><p>rq_affinity = 2<br>For storage configurations that need to maximize distribution of completion processing setting this option to ‘2’ forces the completion to run on the requesting cpu (bypassing the “group” aggregation logic).</p>
<h5 id="tuning-deadline-and-noops"><a href="#tuning-deadline-and-noops" class="headerlink" title="tuning deadline and noops"></a><a href="https://cromwell-intl.com/open-source/performance-tuning/disks.html" target="_blank" rel="noopener">tuning deadline and noops</a></h5><p>fifo_batch    Number of read or write operations to issue in one batch.  Lower values may further reduce latency. Higher values can increase throughput on rotating mechanical disks, but at the cost of worse latency. You selected the deadline scheduler to limit latency, so you probably don’t want to increase this, at least not by very much.</p>
<p>read_expire    Number of milliseconds within which a read request should be served. Reduce this from the default of 500 to 100 on a system with interactive users.</p>
<p>write_expire    Number of milliseconds within which a write request should be served.<br>Leave at default of 5000, let write operations be done asynchronously in the background unless your specialized application uses many synchronous writes.</p>
<p>writes_starved    Number read batches that can be processed before handling a write batch. Increase this from default of 2 to give higher priority to read operations.</p>
<p>nr_requests    Maximum number of read and write requests that can be queued at one time before the next process requesting a read or write is put to sleep. Default value of 128 means 128 read requests and 128 write requests can be queued at once. Larger values may increase throughput for workloads writing many small files, smaller values increase throughput with larger I/O operations. You could decrease this if you are using latency-sensitive applications, but then you shouldn’t be using NOOP if latency is sensitive!</p>
<p>read_ahead_kb    Number of kilobytes the kernel will read ahead during a sequential read operation. 128 kbytes by default, if the disk is used with LVM the device mapper may benefit from a higher value. If your workload does a lot of large streaming reads, larger values may improve performance.</p>
<p>max_sectors_kb    Maximum allowed size of an I/O request in kilobytes, which must be within these bounds:<br>Min value = max(1, logical_block_size/1024)<br>Max value = max_hw_sectors_kb</p>
<p>optimal_io_size    If non-zero, the storage device has reported its own optimal I/O size. If you are developing your own applications, make its I/O requests in multiples of this size if possible.</p>
<p>rotational    Should be 0 (no) for solid-state disks, but some do not correctly report their status to the kernel. If incorrectly set to 1 for an SSD, set it to 0 to disable unneeded scheduler logic meant to reduce number of seeks.</p>
<h5 id="Memroy-setting-ldiskfs"><a href="#Memroy-setting-ldiskfs" class="headerlink" title="Memroy setting (ldiskfs)"></a><a href="https://cromwell-intl.com/open-source/performance-tuning/disks.html" target="_blank" rel="noopener">Memroy setting</a> (ldiskfs)</h5><p>dirty_ratio    “Dirty” memory is that waiting to be written to disk. dirty_ratio is the number of memory pages at which a process will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. A default of 20 is reasonable. Increase to 40 to improve throughput, decrease it to 5 to 10 to improve latency, even lower on systems with a lot of memory.</p>
<p>dirty_background_ratio    Similar, but this is the number of memory pages at which the kernel background flusher thread will start writing out dirty data, expressed as a percentage out of the total free and reclaimable pages. Set this lower than dirty_ratio, dirty_ratio/2 makes sense and is what the kernel does by default. This page shows that dirty_ratio has the greater effect. Tune dirty_ratio for performance, then set dirty_background_ratio to half that value.</p>
<p>overcommit_memory    Allows for poorly designed programs which malloc() huge amounts of memory “just in case” but never really use it. Set this to 0 (disabled) unless you really need it.</p>
<p>vfs_cache_pressure    This sets the “pressure” or the importance the kernel places upon reclaiming memory used for caching directory and inode objects. The default of 100 or relative “fair” is appropriate for compute servers. Set to lower than 100 for file servers on which the cache should be a priority. Set higher, maybe 500 to 1000, for interactive systems.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vm.dirty_ratio &#x3D; 10 #default 20</span><br><span class="line">vm.dirty_background_ratio &#x3D; 5 #default 10</span><br><span class="line">vm.vfs_cache_pressure &#x3D; 50 #default 100 </span><br><span class="line"></span><br><span class="line">The default of 100 or relative &quot;fair&quot; is appropriate for compute servers. Set to lower than 100 for file servers on which the cache should be a priority. Set higher, maybe 500 to 1000, for interactive systems. Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches</span><br></pre></td></tr></table></figure>


<h4 id="MD3460-RAID-config"><a href="#MD3460-RAID-config" class="headerlink" title="MD3460 RAID config"></a>MD3460 RAID config</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RAID 6               8+2</span><br><span class="line">segment              128K</span><br><span class="line">cache block size     32K</span><br><span class="line">cache flush          90%</span><br><span class="line">Write cache mirror   enabled</span><br><span class="line">Read cache           disabled</span><br></pre></td></tr></table></figure>

<h4 id="MDS-OSS"><a href="#MDS-OSS" class="headerlink" title="MDS OSS"></a>MDS OSS</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param -P timeout&#x3D;300</span><br><span class="line">lctl set_param -P osc.*.max_rpcs_in_flight&#x3D;64</span><br><span class="line">options mdt mds_num_threads&#x3D;384</span><br><span class="line">options ost oss_num_threads&#x3D;384</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size&#x3D;1048576</span><br><span class="line">options ptlrpc at_max&#x3D;300</span><br><span class="line">options ptlrpc at_min&#x3D;50</span><br><span class="line">options ptlrpc ldlm_enqueue_min&#x3D;250</span><br></pre></td></tr></table></figure>

<h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param -P osc.*.max_rpcs_in_flight&#x3D;32</span><br><span class="line">lctl set_param -P osc.*.max_dirty_mb&#x3D;4096 #depends your client total memory size and jobs number</span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_num_threads&#x3D;16 #depends your cpu number</span><br><span class="line">options ptlrpc at_max&#x3D;300</span><br><span class="line">options ptlrpc at_min&#x3D;50</span><br><span class="line">options ptlrpc ldlm_enqueue_min&#x3D;250</span><br></pre></td></tr></table></figure>

<p>reports the amount of space this client has reserved for writeback cache with each OST</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br></pre></td></tr></table></figure>

<h3 id="About-osd-zfs"><a href="#About-osd-zfs" class="headerlink" title="About osd_zfs"></a>About osd_zfs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">osd_sync_destroy_max_size <span class="string">"Maximum object size to use synchronous destroy."</span></span><br><span class="line">$ cat /sys/module/osd_zfs/parameters/osd_sync_destroy_max_size</span><br><span class="line">17179869184</span><br><span class="line">$ <span class="built_in">echo</span> $((17179869184/1024/1024/1024))<span class="string">"G"</span></span><br><span class="line">16G</span><br></pre></td></tr></table></figure>

<h3 id="lustre-conf"><a href="#lustre-conf" class="headerlink" title="lustre.conf"></a>lustre.conf</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=tcp(bond0)</span><br><span class="line">options ost oss_max_threads=384</span><br><span class="line">options ost oss_num_threads=128</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size=1048576</span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=230</span><br><span class="line">options lnet accept_backlog=200 <span class="comment"># for 4 cores</span></span><br><span class="line">options lnet accept_backlog=600 <span class="comment"># more cores, default 127, I guess it 's not enough in huge compute nodes env.</span></span><br><span class="line">options lnet accept_timeout=256 <span class="comment"># default 127, depends your ib/tcp settting, why default is 127s, I guess net.ipv4.tcp_syn_retries = 6</span></span><br><span class="line"></span><br><span class="line">lctl set_param ost.OSS.ost_io.threads_max=128</span><br><span class="line">lctl set_param -P ost.OSS.ost_io.threads_max=128</span><br></pre></td></tr></table></figure>
<h4 id="lustre-client"><a href="#lustre-client" class="headerlink" title="lustre client"></a><a href="https://www.eofs.eu/_media/events/lad15/14_gregoire_pichon_lad2015_lustre_2_8_multiple_modify_rpcs.pdf" target="_blank" rel="noopener">lustre client</a></h4><p>OSC - object storage client (connects over network to OST)<br>MDC - meta data client (connects over network to MDT)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/fs/lustre/mdc/testdell-MDT0000-mdc-ffff88091eff0800/state  or lctl get_param mdc.testdell-MDT*.state</span><br><span class="line">mdc.testdell-MDT0000-mdc-ffff88091eff0800.state=</span><br><span class="line">current_state: FULL</span><br><span class="line">state_history:</span><br><span class="line"> - [ 1532484699, CONNECTING ]</span><br><span class="line"> - [ 1532484699, FULL ]</span><br><span class="line"></span><br><span class="line">lctl get_param mdc.testdell-MDT0000-mdc-*.rpc_stats</span><br><span class="line">mdc.testdell-MDT0000-mdc-ffff88091eff0800.rpc_stats=</span><br><span class="line">snapshot_time:         1532694503.184275585 (secs.nsecs)</span><br><span class="line">modify_RPCs_in_flight:  7</span><br><span class="line"></span><br><span class="line">			modify</span><br><span class="line">rpcs <span class="keyword">in</span> flight        rpcs   % cum %</span><br><span class="line">0:		         0   0   0</span><br><span class="line">1:		 126796360   5   5</span><br><span class="line">2:		 219495189   9  14</span><br><span class="line">3:		 290776073  12  26</span><br><span class="line">4:		 332506222  14  40</span><br><span class="line">5:		 325463065  13  54</span><br><span class="line">6:		 331889670  13  68</span><br><span class="line">7:		 701840555  29  98</span><br><span class="line">8:		  42105797   1 100</span><br><span class="line"></span><br><span class="line">$ lctl set_param mdc.fsname-MDT*-mdc-*.max_mod_rpcs_in_flight=12</span><br><span class="line"><span class="comment"># it 's the maximum number of modify RPCs sent in parallel</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># But I can 't modify it in Lustre 2.10.2</span></span><br><span class="line">lctl set_param mdc.testdell-MDT*-mdc-*.max_mod_rpcs_in_flight=12</span><br><span class="line">error: set_param: setting /proc/fs/lustre/mdc/testdell-MDT0000-mdc-ffff88091eff0800/max_mod_rpcs_in_flight=12: Numerical result out of range</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## only for ldiskfs</span></span><br><span class="line"><span class="comment"># lr_reader -c /dev/sdh</span></span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># lr_reader -r /dev/sdh</span></span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">options ptlrpc ldlm_num_threads=8~32 <span class="comment">#because https://jira.hpdd.intel.com/browse/LU-416 and https://jira.hpdd.intel.com/browse/LU-7330</span></span><br><span class="line">options ptlrpc at_max=300</span><br><span class="line">options ptlrpc ldlm_enqueue_min=100</span><br><span class="line">options ptlrpc max_rpcs_in_flight=64</span><br><span class="line">options ptlrpc max_dirty_mb=1024</span><br></pre></td></tr></table></figure>

<h3 id="Trace-log-RT"><a href="#Trace-log-RT" class="headerlink" title="Trace log RT"></a>Trace log RT</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkfifo -m 777 /tmp/lustre.log</span><br><span class="line">lctl debug_daemon start /tmp/lustre.log</span><br><span class="line">tail -f /tmp/lustre.log</span><br></pre></td></tr></table></figure>

<h3 id="Dump-log"><a href="#Dump-log" class="headerlink" title="Dump log"></a>Dump log</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param printk=+neterror</span><br><span class="line">lctl get_param panic_on_lbug</span><br><span class="line">panic_on_lbug=1</span><br><span class="line"><span class="comment">#if not =1</span></span><br><span class="line">lctl set_param panic_on_lbug=1</span><br><span class="line"></span><br><span class="line">lctl set_param debug=-1</span><br><span class="line">lctl clear</span><br><span class="line">lctl mark <span class="string">"start mount"</span></span><br><span class="line">mount -t lustre xx.xx.xx.xx@tcp:/fsname /fsname <span class="comment"># or the other operate  </span></span><br><span class="line">lctl mark <span class="string">"Finish mount"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lctl set_param debug_mb=50</span><br><span class="line">lctl set_param debug=+trace</span><br><span class="line">lctl dk &gt;/dev/null</span><br><span class="line">ls /opt/lustrewh/pmo</span><br><span class="line">lctl dk&gt; [filename]</span><br><span class="line"></span><br><span class="line">lctl get_param mdc.*mdc*.*</span><br><span class="line"></span><br><span class="line"><span class="comment"># MDS, OSS, Client</span></span><br><span class="line">lctl dk filename</span><br><span class="line"></span><br><span class="line"><span class="comment"># in MDS</span></span><br><span class="line">mount -t lustre /dev/sdb1 /mnt/mds;</span><br><span class="line">mount -t ldiskfs /dev/sdb1 /mnt/ldiskfs;</span><br><span class="line">llog_reader /mnt/ldiskfs/CONFIGS/lustre2T-MDT0000 | grep pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using debug_daemon and interactive lctl to dump debug files to a 40 MB file:</span></span><br><span class="line">lctl</span><br><span class="line">lctl &gt; debug_daemon start /var/<span class="built_in">log</span>/lustre.40.bin 40</span><br><span class="line">&lt;run filesystem operation(s) to debug (outside of the lctl session)&gt;</span><br><span class="line">lctl &gt; debug_daemon stop</span><br><span class="line">lctl &gt; debug_file /var/<span class="built_in">log</span>/lustre.40.bin /var/<span class="built_in">log</span>/lustre.log</span><br><span class="line"></span><br><span class="line">dmesg -C </span><br><span class="line"><span class="built_in">echo</span> t &gt; /proc/sysrq-trigger</span><br><span class="line">dmesg &gt; dmesg-t-sysrq.mds</span><br><span class="line"></span><br><span class="line">cat /proc/D_state_pid/wchan</span><br><span class="line">cat /proc/D_state_pid/stack</span><br><span class="line"></span><br><span class="line">lctl --device MGS llog_print \$<span class="variable">$fsname</span>-client</span><br></pre></td></tr></table></figure>

<h3 id="Ftrace"><a href="#Ftrace" class="headerlink" title="Ftrace"></a>Ftrace</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lustre /mnt</span><br><span class="line"><span class="comment"># it 'll create trace.dat, send it to lustre team </span></span><br><span class="line"></span><br><span class="line">$ trace-cmd record -e ext4 ls</span><br><span class="line">$ trace-cmd report</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">### OST rpc info</span></span><br><span class="line">```bash</span><br><span class="line">cat /proc/fs/lustre/osc/lustre-OST0031-osc-ffff88103a993000/import</span><br></pre></td></tr></table></figure>

<h3 id="Changelog"><a href="#Changelog" class="headerlink" title="Changelog"></a>Changelog</h3><ul>
<li>MARK – Internal record keeping</li>
<li>CREAT – Regular file creation</li>
<li>MKDIR – Directory creation</li>
<li>HLINK – Hard link</li>
<li>SLINK – Soft link</li>
<li>OPEN – open file</li>
<li>CLOSE – close file</li>
<li>MKNOD – Other file creation</li>
<li>UNLNK – Regular file removal</li>
<li>RMDIR – Directory removal</li>
<li>RNMFM – Rename, original</li>
<li>RNMTO – Rename, final</li>
<li>IOCTL – ioctl on file or directory</li>
<li>TRUNC – Regular file truncated</li>
<li>SATTR – Attribute change</li>
<li>XATTR – Extended Attribute change</li>
<li>HSM – HSM action</li>
<li>UNKNW – Unkown operation</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Enable</span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line"> lctl set_param mdt.$FSNAME-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line"> lctl set_param -P  mdt.lustresz-MDT0000.hsm_control&#x3D;enabled</span><br><span class="line"> lctl set_param mdt.$FSNAME-MDT0000.hsm.max_requests&#x3D;8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#96;&#96;&#96;bash</span><br><span class="line">#Create user</span><br><span class="line">lctl --device fsname-MDT0000 changelog_register</span><br><span class="line">#Del user</span><br><span class="line">lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line">#Get size</span><br><span class="line">lctl get_param mdd.fsname-MDT0000.changelog_users mdd.fsname-MDT0000.changelog_size</span><br><span class="line">#changelog type</span><br><span class="line">lctl set_param mdd.lustrefs-MDT*.changelog_mask&#x3D;MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">lctl get_param mdd.lustrefs-MDT*.changelog_mask </span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line">#display changelog</span><br><span class="line">$ lfs changelog $fsname-MDT0000 &gt; &#x2F;tmp&#x2F;lustre-changelog</span><br><span class="line">$ lfs changelog $fsname-MDT0000 [startrec [endrec]] </span><br><span class="line"></span><br><span class="line">#clear changelog</span><br><span class="line">lctl changelog_clear mdt_name userid endrec</span><br><span class="line"></span><br><span class="line">#Notify a device that user cl1 no longer needs records (up toand including 3)</span><br><span class="line">lfs changelog_clear $fsname-MDT0000 cl1 3</span><br><span class="line">#display changelog begin from number 4 record</span><br><span class="line"></span><br><span class="line">#To stop changelogs, changelog_mask should be set to MARK only</span><br><span class="line">$ lctl set_param mdd.$fsname-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line">mdd.lustre-MDT0000.changelog_mask&#x3D;MARK</span><br><span class="line"></span><br><span class="line">#or youcan set it -all</span><br><span class="line">lctl set_param mdd.$fsname-MDT0000.changelog_mask&#x3D;-all</span><br></pre></td></tr></table></figure>

<h3 id="Set-timeout"><a href="#Set-timeout" class="headerlink" title="Set timeout"></a>Set timeout</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param -P timeout=300</span><br></pre></td></tr></table></figure>

<h3 id="Get-timeout-and-state"><a href="#Get-timeout-and-state" class="headerlink" title="Get timeout and state"></a>Get timeout and state</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osc.*OST0012*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* timeout</span><br></pre></td></tr></table></figure>

<h3 id="Kdump"><a href="#Kdump" class="headerlink" title="Kdump"></a>Kdump</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum -y install kexec-tools</span><br><span class="line">cat /etc/kdump.conf</span><br><span class="line">nfs my.nfsserver.example.org:/path/to/expor</span><br><span class="line">core_collector makedumpfile -d 16 -c</span><br><span class="line"><span class="comment">#-c Compress dump data by each page</span></span><br><span class="line"><span class="comment">#core_collector makedumpfile -d 16 -c message_level 16</span></span><br><span class="line"><span class="comment"># 1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></span><br><span class="line"><span class="comment"># 1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></span><br><span class="line"><span class="comment">#ssh user@my.server.example.org:/dest/path</span></span><br><span class="line"><span class="comment">#By default, uses ssh key at /root/.ssh/kdump_id_rsa</span></span><br><span class="line"><span class="comment">#core_collector makedumpfile &lt;options&gt;</span></span><br></pre></td></tr></table></figure>


<h3 id="Fsck-ldiskfs-file-system-corruption"><a href="#Fsck-ldiskfs-file-system-corruption" class="headerlink" title="Fsck ldiskfs file system corruption"></a>Fsck ldiskfs file system corruption</h3><h4 id="Errors"><a href="#Errors" class="headerlink" title="Errors"></a>Errors</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Dec 29 14:11:32 mookie kernel:</span> <span class="string">LDISKFS-fs</span> <span class="string">error</span> <span class="string">(device</span> <span class="string">sdz):</span> <span class="attr">ldiskfs_lookup:</span> <span class="string">unlinked</span> <span class="string">inode</span> <span class="number">5384166</span> <span class="string">in</span> <span class="string">dir</span> <span class="comment">#145170469</span></span><br><span class="line"><span class="attr">Dec 29 14:11:32 mookie kernel:</span> <span class="string">Remounting</span> <span class="string">filesystem</span> <span class="string">read-only</span></span><br></pre></td></tr></table></figure>

<h4 id="Flush-the-journal"><a href="#Flush-the-journal" class="headerlink" title="Flush the journal"></a>Flush the journal</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /ostxx</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /mnt/ost</span><br><span class="line">$ umount /mnt/ost</span><br></pre></td></tr></table></figure>

<h4 id="Caution"><a href="#Caution" class="headerlink" title="Caution"></a>Caution</h4><ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lustre version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT/OSTs in parallel</p>
</li>
</ul>
<h4 id="Check-only-mode"><a href="#Check-only-mode" class="headerlink" title="Check only mode"></a>Check only mode</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ e2fsck -fn /dev/sdx</span><br></pre></td></tr></table></figure>

<h4 id="Prudent-mode"><a href="#Prudent-mode" class="headerlink" title="Prudent mode"></a>Prudent mode</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ e2fsck -fp /dev/sdx</span><br></pre></td></tr></table></figure>

<h4 id="Answer-yes"><a href="#Answer-yes" class="headerlink" title="Answer yes"></a>Answer yes</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ e2fsck -fy /dev/sdx</span><br></pre></td></tr></table></figure>

<h3 id="Storage-target-could-not-mount-mount-lustre-failed"><a href="#Storage-target-could-not-mount-mount-lustre-failed" class="headerlink" title="Storage target could not mount (mount.lustre failed)"></a>Storage target could not mount (mount.lustre failed)</h3><h4 id="Caution-1"><a href="#Caution-1" class="headerlink" title="Caution"></a>Caution</h4><ul>
<li><font color=red>umount</font> all clients</li>
<li><font color=red>umount</font> all MGS,MDT,OSTs</li>
<li>ensure Lustre backing file systems are healthy ( fsck )</li>
</ul>
<h4 id="Writeconf"><a href="#Writeconf" class="headerlink" title="Writeconf"></a>Writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mds<span class="comment"># tunefs.lustre --writeconf /dev/sdx</span></span><br><span class="line">oss<span class="comment"># tunefs.lustre --writeconf /dev/ost0</span></span><br><span class="line">oss<span class="comment"># tunefs.lustre --writeconf /dev/ost1</span></span><br><span class="line">oss<span class="comment"># tunefs.lustre --writeconf /dev/ost2</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<h4 id="Lustre-clinet-performance"><a href="#Lustre-clinet-performance" class="headerlink" title="Lustre clinet performance"></a>Lustre clinet performance</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=1M</span><br><span class="line">lctl conf_param &lt;fsname&gt;.osc.max_pages_per_rpc=1M</span><br></pre></td></tr></table></figure>

<h4 id="Mount"><a href="#Mount" class="headerlink" title="Mount"></a>Mount</h4><ul>
<li>When you writeconf operation complete, mount MGS,MDT,OSTs,clients<ul>
<li>If MGS and MDT in single block device, you can add “-o nosvc” to avoid mount MDT</li>
</ul>
</li>
</ul>
<h3 id="Flush-all-of-the-metadata-client-mdc-locks-on-this-node"><a href="#Flush-all-of-the-metadata-client-mdc-locks-on-this-node" class="headerlink" title="Flush all of the metadata client (mdc) locks on this node"></a>Flush all of the metadata client (mdc) locks on this node</h3><p>If the client files info are not consistent , you could run the comand in the client</p>
<p><a href="http://wiki.old.lustre.org/manual/LustreManual20_HTML/LustreProc.html" target="_blank" rel="noopener">The lru_size parameter is used to control the number of client-side locks in an LRU queue. LRU size is dynamic, based on load. This optimizes the number of locks available to nodes that have different workloads</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size=clear</span><br></pre></td></tr></table></figure>
<h3 id="Lustre-quota-ldiskfs"><a href="#Lustre-quota-ldiskfs" class="headerlink" title="Lustre quota (ldiskfs)"></a>Lustre quota (ldiskfs)</h3><h4 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h4><p>From Lustre versions 2.4 on, quota (accounting) is enabled by default</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/fs/lustre/osd-ldiskfs/lfstest-MDT0000/quota_slave/info </span><br><span class="line">target name:    lfstest-MDT0000</span><br><span class="line">pool ID:        0</span><br><span class="line"><span class="built_in">type</span>:           md</span><br><span class="line">quota enabled:  <span class="string">"none"</span></span><br><span class="line">conn to master: setup</span><br><span class="line">space acct:     ug</span><br><span class="line">user uptodate:  glb[0],slv[0],reint[0]</span><br><span class="line">group uptodate: glb[0],slv[0],reint[0]</span><br><span class="line"></span><br><span class="line">mds$ lctl conf_param lfstest.quota.mdt=ug</span><br><span class="line"></span><br><span class="line">mds$ cat /proc/fs/lustre/osd-ldiskfs/lfstest-MDT0000/quota_slave/info </span><br><span class="line">target name:    lfstest-MDT0000</span><br><span class="line">pool ID:        0</span><br><span class="line"><span class="built_in">type</span>:           md</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">conn to master: setup</span><br><span class="line">space acct:     ug</span><br><span class="line">user uptodate:  glb[1],slv[1],reint[0]</span><br><span class="line">group uptodate: glb[1],slv[1],reint[0]</span><br><span class="line"></span><br><span class="line">mds$ lctl conf_param lfstest.quota.ost=ug</span><br><span class="line"></span><br><span class="line"><span class="comment">## disable</span></span><br><span class="line">lctl conf_param lfstest.quota.ost=none</span><br><span class="line">lctl conf_param lfstest.quota.mdt=none</span><br><span class="line"></span><br><span class="line"><span class="comment">## set quota</span></span><br><span class="line">client $ lfs setquota –u user1 –b 307200 –B 309200 –i 10000 –I 11000 /mnt/lustre</span><br><span class="line">client $ lfs setquota –g group1 –b 5120000 –B 5150000 –i 100000 –I 101000 /mnt/lustre</span><br><span class="line"></span><br><span class="line"><span class="comment">## report</span></span><br><span class="line">client $ lfs quota –u user1 -v /mnt/lustre</span><br><span class="line">client $ lfs quota -t -p /mnt/lustre</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h4 id="Clear-pool"><a href="#Clear-pool" class="headerlink" title="Clear pool"></a>Clear pool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe <span class="variable">$line</span></span><br></pre></td></tr></table></figure>

<h4 id="Add-pool"><a href="#Add-pool" class="headerlink" title="Add pool"></a>Add pool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#MDT</span></span><br><span class="line">lctl pool_new lustre.pool1</span><br><span class="line">lctl pool_add lustre.pool1 OST[0-10/2]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Client, set all files</span></span><br><span class="line">find /mnt/lustre/<span class="built_in">test</span> -<span class="built_in">type</span> d | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    lctl setstripe -p lustre.pool1 <span class="variable">$line</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h4 id="OSS"><a href="#OSS" class="headerlink" title="OSS"></a>OSS</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/fs/lustre/osd-ldiskfs/lfstest-OST0001/quota_slave/info </span><br><span class="line">target name:    lfstest-OST0001</span><br><span class="line">pool ID:        0</span><br><span class="line"><span class="built_in">type</span>:           dt</span><br><span class="line">quota enabled:  <span class="string">"ug"</span></span><br><span class="line">conn to master: setup</span><br><span class="line">space acct:     ug</span><br><span class="line">user uptodate:  glb[1],slv[1],reint[0]</span><br><span class="line">group uptodate: glb[1],slv[1],reint[0]</span><br></pre></td></tr></table></figure>

<h4 id="Client-1"><a href="#Client-1" class="headerlink" title="Client"></a>Client</h4><h5 id="User-group-Quota"><a href="#User-group-Quota" class="headerlink" title="User/group Quota"></a>User/group Quota</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># No soft limit, Hard limit of 1GB block space, 10000 files</span></span><br><span class="line">$ lfs setquota -u <span class="built_in">test</span> -b 0 -B 1024000 -i 0 -I 10000 /mnt/</span><br><span class="line"></span><br><span class="line"><span class="comment"># Soft limit 1G and 5000 files, Hard limit 2G and 10000 files</span></span><br><span class="line">$ lfs setquota -g test1 -b 1G -B 2G -i 5000 -I 10000 /mnt/</span><br><span class="line"></span><br><span class="line">user$ cp: writing test1_bak: Disk quota exceeded</span><br><span class="line">user$ touch: cannot touch 10000: Disk quota exceeded</span><br><span class="line">$ lfs setquota -u user_data -B 350T /mnt/user_data</span><br><span class="line">$ lfs quota -u user_data /mnt/user_data</span><br><span class="line">Disk quotas <span class="keyword">for</span> user user_data (uid 36081):</span><br><span class="line">     Filesystem  kbytes   quota   <span class="built_in">limit</span>   grace   files   quota   <span class="built_in">limit</span>   grace</span><br><span class="line">  /mnt/user_data 299994427159       0 375809638400       -  707604       0       0       -</span><br></pre></td></tr></table></figure>
<h5 id="lfs-migrate-data-to-another-OST"><a href="#lfs-migrate-data-to-another-OST" class="headerlink" title="lfs migrate data to another OST"></a>lfs migrate data to another OST</h5><p>There are some of data loss report when lustre to migrate data.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lfs find /opt/lustrewh -obd lustrewh-OST000c_UUID -size +4G | lfs_migrate -y</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># move filename to OST0000 and stripe number equal 1</span></span><br><span class="line">lfs migrate -c 1 -i 0 filename</span><br></pre></td></tr></table></figure>

<h3 id="Abort-tgt-reocv"><a href="#Abort-tgt-reocv" class="headerlink" title="Abort tgt_reocv"></a>Abort tgt_reocv</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mds lctl dl | grep osc</span><br><span class="line">8 UP osp lustre-OST0000-osc-MDT0000 lustre-MDT0000-mdtlov_UUID 5</span><br><span class="line"></span><br><span class="line">$ lctl --device 8 deactivate</span><br><span class="line">$ lctl --device 8 activate</span><br></pre></td></tr></table></figure>
<p>or mount.lustre xxx xxx -o abort_recov</p>
<h3 id="Get-lustre-version"><a href="#Get-lustre-version" class="headerlink" title="Get lustre version"></a>Get lustre version</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param version</span><br></pre></td></tr></table></figure>

<h3 id="Lustre-status"><a href="#Lustre-status" class="headerlink" title="Lustre status"></a>Lustre status</h3><p>list nids</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lctl list_nids</span><br><span class="line">lctl which_nid 10.xxx@tcp</span><br><span class="line">lctl ping 10.xxx@tcp</span><br></pre></td></tr></table></figure>

<h3 id="check-ost-status"><a href="#check-ost-status" class="headerlink" title="check ost status"></a>check ost status</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">client $ lfs osts</span><br><span class="line">client $ cat /proc/fs/lustre/lov/<span class="variable">$fsname</span>-clilov-ffff882037467800/target_obd</span><br><span class="line">mds $ cat /proc/fs/lustre/lov/<span class="variable">$fsname</span>-MDT0000-mdtlov/target_obd</span><br><span class="line">client $ lctl get_param osc.*-OST*.active</span><br></pre></td></tr></table></figure>

<h5 id="show-ost-and-ip-addr-relationship"><a href="#show-ost-and-ip-addr-relationship" class="headerlink" title="show ost and ip addr relationship"></a>show ost and ip addr relationship</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl dl -t</span><br></pre></td></tr></table></figure>

<h5 id="OST-threads"><a href="#OST-threads" class="headerlink" title="OST threads"></a>OST threads</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param ost.OSS.ost_io.threads_max=128</span><br></pre></td></tr></table></figure>

<h3 id="No-roots-quash"><a href="#No-roots-quash" class="headerlink" title="No roots quash"></a>No roots quash</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ fsname=lustre</span><br><span class="line">$ lctl set_param -P <span class="variable">$fsname</span>.mdt.root_squash=99:99</span><br><span class="line">$ <span class="built_in">echo</span> 99:99 &gt; /proc/fs/lustre/mdt/<span class="variable">$fsname</span>-MDT0000/root_squash</span><br><span class="line">$ lctl set_param -P <span class="variable">$fsname</span>.mdt.nosquash_nids=<span class="string">"10.xx.xx.xx@tcp 10.xx.xx.xx@tcp"</span></span><br><span class="line">$ cat /proc/fs/lustre/mdt/<span class="variable">$fsname</span>-MDT0000/nosquash_nids</span><br><span class="line">10.xx.xx.xx@tcp 10.xx.xx.xx@tcp</span><br><span class="line"></span><br><span class="line">or <span class="built_in">set</span> <span class="keyword">in</span> client</span><br><span class="line">$ lctl set_param llite.<span class="variable">$fsname</span>-zaff931f640e2222.nosquash_nids=<span class="string">"10.xx.xx.xx@tcp 10.xx.xx.xx@tcp"</span></span><br></pre></td></tr></table></figure>

<h3 id="Readonly-OST"><a href="#Readonly-OST" class="headerlink" title="Readonly OST"></a>Readonly OST</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Better than degraded</span></span><br><span class="line">$ lctl set_param osp.<span class="variable">$fsname</span>-OST0000.max_create_count=0</span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.<span class="variable">$fsname</span>-OST00XX.degraded=1</span><br></pre></td></tr></table></figure>

<h3 id="Rebalance-OST"><a href="#Rebalance-OST" class="headerlink" title="Rebalance OST"></a>Rebalance OST</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lfs find /opt/lustrewh -obd lustrewh-OST000c_UUID -size +4G | lfs_migrate -y</span><br></pre></td></tr></table></figure>

<h3 id="Move-data-to-one-OST"><a href="#Move-data-to-one-OST" class="headerlink" title="Move data to one OST"></a>Move data to one OST</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lfs migrate -c 1  -i 4  filepath <span class="comment">#to index 4</span></span><br></pre></td></tr></table></figure>

<h3 id="Enable-job-status"><a href="#Enable-job-status" class="headerlink" title="Enable job status"></a><a href="https://jira.hpdd.intel.com/browse/LU-694" target="_blank" rel="noopener">Enable job status</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment">### Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment">### disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### If there isn't any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment">### Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, sum:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, sum:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mknod:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  link:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  unlink:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  mkdir:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"></span><br><span class="line"><span class="comment">### Clear stats for job "dd.0" on lustre-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lustre-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment">### cleanup interval (seconds)</span></span><br><span class="line">lctl conf_param testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">[root@cngb-mds-m22-1 lustre]<span class="comment">#  cat /proc/fs/lustre/mdt/testfs5-MDT0000/job_cleanup_interval</span></span><br><span class="line">604800</span><br></pre></td></tr></table></figure>

<h3 id="Understanding-Lustre-Message-Loss-and-Tuning-for-Resiliency"><a href="#Understanding-Lustre-Message-Loss-and-Tuning-for-Resiliency" class="headerlink" title="Understanding Lustre Message Loss and Tuning for Resiliency"></a><a href="http://wiki.lustre.org/Lustre_Resiliency:_Understanding_Lustre_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lustre_for_Resiliency" target="_blank" rel="noopener">Understanding Lustre Message Loss and Tuning for Resiliency</a></h3><p>Adaptive Timeouts<br>In a Lustre file system servers keep track of the time it takes for RPCs to be completed</p>
<p>at_min<br>The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts.<br>Our recommendation for Lustre servers is also 40 seconds.</p>
<p>at_max<br>The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</p>
<p>LDLM Timeouts<br>options ptlrpc at_max=400<br>options ptlrpc at_min=40<br>options ptlrpc ldlm_enqueue_min=260<br>ldlm_enqueue_min = max(2<em>net_latency, net_latency + quiescent_time) +\ 2</em>service_time<br>ldlm_enqueue_min = max(2<em>40, 40 + 140) + 2</em>40 = 180 + 80 = 260</p>
<p>My setting<br>options ptlrpc at_max=300<br>options ptlrpc at_min=30<br>options ptlrpc ldlm_enqueue_min=230</p>
<h3 id="Evict-client"><a href="#Evict-client" class="headerlink" title="Evict client"></a>Evict client</h3><p>Get client uuid<br>lctl get_param -n llite.${fsname}-*.uuid</p>
<p>MDS<br>lctl set_param -n mdt.${fsname}-MDTxxxx.evict_client ${uuid}</p>
<p>OSS<br>lctl set_param -n obdfilter.${fsname}-OSTxxxx.evict_client ${uuid}</p>
<h3 id="OSS-setting-for-Ethernet-test"><a href="#OSS-setting-for-Ethernet-test" class="headerlink" title="OSS setting for Ethernet (test)"></a>OSS setting for Ethernet (test)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">chmod a+w /sys/module/ksocklnd/parameters/peer_timeout /sys/module/ksocklnd/parameters/peer_credits /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 512 &gt; /sys/module/ksocklnd/parameters/credits</span><br><span class="line"><span class="built_in">echo</span> 360 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line"><span class="built_in">echo</span> 16 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">chmod a+w /sys/module/lnet/parameters/accept_backlog  /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 127 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line"><span class="built_in">echo</span> 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br></pre></td></tr></table></figure>

<h3 id="Lnet"><a href="#Lnet" class="headerlink" title="Lnet"></a>Lnet</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lustre 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show --verbose</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line"></span><br><span class="line"><span class="comment">### Lnet multiple-plane </span></span><br><span class="line">options lnet networks=<span class="string">"tcp1(eth1),tcp2(eth2),o2ib0(ib0)"</span></span><br><span class="line"><span class="comment">### </span></span><br><span class="line">options lnet ip2nets=<span class="string">"tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]"</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"><span class="comment">### multi rails</span></span><br></pre></td></tr></table></figure>


<h3 id="Lustre-client-setting"><a href="#Lustre-client-setting" class="headerlink" title="Lustre client setting"></a><a href="http://www.linuxclustersinstitute.org/workshops/archive/aug2017/pdfs/Storage08-Lustre.pdf" target="_blank" rel="noopener">Lustre client setting</a></h3><ul>
<li>lctl set_param osc.*.max_pages_per_rpc=1024<pre><code>256 pages/RPC 1024 pages/RPC</code></pre>1MB Write 1 RPC, 2 Credits 1 RPC, 2 Credits<br>2MB Write 2 RPC, 4 Credits 1 RPC, 3 Credits<br>3MB Write 3 RPC, 6 Credits 1 RPC, 4 Credits<br>4MB Write 4 RPC, 8 Credits 1 RPC, 5 Credits</li>
</ul>
<h4 id="Lustre-client-rebuild"><a href="#Lustre-client-rebuild" class="headerlink" title="Lustre client rebuild"></a>Lustre client rebuild</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In lustre 2.13.0, you must import openmpi PATH</span></span><br><span class="line">$ <span class="built_in">export</span> PATH=/usr/lib64/openmpi/bin/:<span class="variable">$PATH</span></span><br><span class="line">$ rpmbuild --rebuild --without servers  lustre-2.10.3-1.src.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># New stupid bug when you compile lustre 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span></span><br><span class="line">If you want it pass, I was clear /tmp/tmp.* rpmbuild not <span class="built_in">help</span> I guess maybe the old config <span class="keyword">in</span> some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line"></span><br><span class="line"><span class="comment"># from source</span></span><br><span class="line">$ ./configure --<span class="built_in">enable</span>-client --<span class="built_in">disable</span>-server --with-linux=/usr/src/kernels/$(uname -r);make rpms/deps</span><br></pre></td></tr></table></figure>
<p><code>About 2.10.3-1 bug</code><br>Is real the realease production ? <code>too stupid</code> bug. just waste my time to type these words.<br>There is no test team in lustre develop team, All users was the test team except you are going to buy DDN.</p>
<h4 id="lustre-fid-and-path"><a href="#lustre-fid-and-path" class="headerlink" title="lustre fid and path"></a>lustre fid and path</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]# lfs fid2path &#x2F;mnt&#x2F;lustre [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid &#x2F;lustre</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>
<h4 id="Lustre-set-stripe"><a href="#Lustre-set-stripe" class="headerlink" title="Lustre set stripe"></a>Lustre set stripe</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lfs setstripe -S 4000M -c 50 /mnt/striped</span><br></pre></td></tr></table></figure>
<h4 id="Lustre-client-buffer-and-rpcs"><a href="#Lustre-client-buffer-and-rpcs" class="headerlink" title="Lustre client buffer and rpcs"></a>Lustre client buffer and rpcs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osc.*.max_pages_per_rpc&#x3D;1024 # 1024 &#x3D; 1024*4KB &#x3D;4MB per RPC</span><br><span class="line">Max RPCS in flight between OSC and OST</span><br><span class="line"></span><br><span class="line">lctl set_param osc.*.max_rpcs_in_flight&#x3D;64; </span><br><span class="line">Max number of 4K pages per RPC</span><br><span class="line">256 &#x3D; 1MB per RPC</span><br><span class="line">max_pages_per_rpc*4*max_rpcs_in_flight*2&#x3D;max_dirty_mb</span><br><span class="line">1024*4KB&#x2F;1024(KB to MB)*64*2&#x3D;512</span><br><span class="line">lctl set_param osc.*.max_dirty_mb&#x3D;512</span><br><span class="line">Maximum MBs of dirty data that can be written and queued on a client</span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4&#x2F;1024*64*2&#x3D;128 #because 128 too low, change it to 256</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc&#x3D;256; lctl set_param osc.*.max_rpcs_in_flight&#x3D;64;lctl set_param osc.*.max_dirty_mb&#x3D;256</span><br></pre></td></tr></table></figure>

<h4 id="Max-mod-rpcs-in-flight"><a href="#Max-mod-rpcs-in-flight" class="headerlink" title="Max_mod_rpcs_in_flight"></a>Max_mod_rpcs_in_flight</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl set_param mdc.fsname-MDT*-mdc-*.max_mod_rpcs_in_flight=12</span><br><span class="line"><span class="built_in">echo</span> 12 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br></pre></td></tr></table></figure>

<h3 id="About-openzfs-performance"><a href="#About-openzfs-performance" class="headerlink" title="About openzfs performance"></a>About openzfs performance</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line"></span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lustre to block waiting on a TXG to sync</span><br></pre></td></tr></table></figure>

<h3 id="Reubild-lustre-client"><a href="#Reubild-lustre-client" class="headerlink" title="Reubild lustre client"></a>Reubild lustre client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpmbuild --rebuild --without servers lustre-2.10.4-1.src.rpm</span><br></pre></td></tr></table></figure>

<h3 id="Check-zfs-dmu-tx-assign"><a href="#Check-zfs-dmu-tx-assign" class="headerlink" title="Check zfs dmu_tx_assign"></a>Check zfs dmu_tx_assign</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/spl/kstat/zfs/ost_86/dmu_tx_assign</span><br><span class="line">75 1 0x01 41 1968 259828173343194 262983153792140</span><br><span class="line">name                            <span class="built_in">type</span> data</span><br><span class="line">1 ns                            4    0</span><br><span class="line">2 ns                            4    0</span><br><span class="line">4 ns                            4    0</span><br><span class="line">8 ns                            4    0</span><br><span class="line">16 ns                           4    0</span><br><span class="line">32 ns                           4    0</span><br><span class="line">64 ns                           4    0</span><br><span class="line">128 ns                          4    40</span><br><span class="line">256 ns                          4    40</span><br><span class="line">512 ns                          4    6</span><br><span class="line">1024 ns                         4    1</span><br><span class="line">2048 ns                         4    0</span><br><span class="line">4096 ns                         4    0</span><br><span class="line">8192 ns                         4    0</span><br><span class="line">16384 ns                        4    0</span><br><span class="line">32768 ns                        4    0</span><br><span class="line">65536 ns                        4    5</span><br><span class="line">131072 ns                       4    105</span><br><span class="line">262144 ns                       4    331</span><br><span class="line">524288 ns                       4    419</span><br><span class="line">1048576 ns                      4    325</span><br><span class="line">2097152 ns                      4    338</span><br><span class="line">4194304 ns                      4    188</span><br><span class="line">8388608 ns                      4    65</span><br><span class="line">16777216 ns                     4    59</span><br><span class="line">33554432 ns                     4    47</span><br><span class="line">67108864 ns                     4    34</span><br><span class="line">134217728 ns                    4    60</span><br><span class="line">268435456 ns                    4    55</span><br><span class="line">536870912 ns                    4    65</span><br><span class="line">1073741824 ns                   4    94</span><br><span class="line">2147483648 ns                   4    56</span><br><span class="line">4294967296 ns                   4    271</span><br><span class="line">8589934592 ns                   4    3454</span><br><span class="line">17179869184 ns                  4    4687</span><br><span class="line">34359738368 ns                  4    1970</span><br><span class="line">68719476736 ns                  4    642</span><br><span class="line">137438953472 ns                 4    380</span><br><span class="line">274877906944 ns                 4    440</span><br><span class="line">549755813888 ns                 4    299</span><br><span class="line">1099511627776 ns                4    36</span><br></pre></td></tr></table></figure>

<h4 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#MDS</span></span><br><span class="line">$ watch -d lctl get_param mdt.*.md_stats</span><br><span class="line">snapshot_time             1556726087.189561170 secs.nsecs</span><br><span class="line">open                      3412130101 samples [reqs]</span><br><span class="line">close                     2926922120 samples [reqs]</span><br><span class="line">mknod                     293730475 samples [reqs]</span><br><span class="line">link                      20713305 samples [reqs]</span><br><span class="line">unlink                    316042257 samples [reqs]</span><br><span class="line">mkdir                     3275032 samples [reqs]</span><br><span class="line">rmdir                     2731821 samples [reqs]</span><br><span class="line">rename                    7687699 samples [reqs]</span><br><span class="line">getattr                   2060900881 samples [reqs]</span><br><span class="line">setattr                   320658776 samples [reqs]</span><br><span class="line">getxattr                  1080139037 samples [reqs]</span><br><span class="line">setxattr                  222105 samples [reqs]</span><br><span class="line">statfs                    11587278 samples [reqs]</span><br><span class="line">sync                      20670980 samples [reqs]</span><br><span class="line">samedir_rename            7199107 samples [reqs]</span><br><span class="line">crossdir_rename           488592 samples [reqs]</span><br><span class="line"></span><br><span class="line"><span class="comment">#OSS</span></span><br><span class="line">$ watch -d lctl get_param  obdfilter.*OST0000*.stats</span><br><span class="line"></span><br><span class="line"><span class="comment">#client</span></span><br><span class="line">$ watch -d lctl get_param llite.*your-fsname*.stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># clear it</span></span><br><span class="line">$ lctl set_param llite.*.stats=c</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Readonly mount lustre</span></span><br><span class="line">```bash</span><br><span class="line">mount.lustre <span class="variable">$zpool</span> /lustreost0 -o rdonly_dev</span><br></pre></td></tr></table></figure>


<h4 id="Lustre-OSS-info"><a href="#Lustre-OSS-info" class="headerlink" title="Lustre OSS info"></a>Lustre OSS info</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line">lctl get_param obdfilter.*OST005e*.brw_stats</span><br><span class="line">obdfilter.lustre-OST005e.brw_stats=</span><br><span class="line">snapshot_time:         1553173150.540577969 (secs.nsecs)</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">pages per bulk r/w     rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">1:		  32618621  43  43   | 540639   4   4</span><br><span class="line">2:		    638573   0  44   | 409948   3   7</span><br><span class="line">4:		   1108531   1  45   | 240798   1   8</span><br><span class="line">8:		   9386337  12  58   | 172134   1  10</span><br><span class="line">16:		    785380   1  59   | 114421   0  10</span><br><span class="line">32:		    340607   0  60   | 86635   0  11</span><br><span class="line">64:		    474447   0  60   | 147987   1  12</span><br><span class="line">128:		    745719   0  61   | 803491   5  18</span><br><span class="line">256:		  28695511  38 100   | 10967594  81 100</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">discontiguous pages    rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">0:		  74793726 100 100   | 568196   4   4</span><br><span class="line">1:		         0   0 100   | 412155   3   7</span><br><span class="line">2:		         0   0 100   | 117884   0   8</span><br><span class="line">3:		         0   0 100   | 122920   0   9</span><br><span class="line">4:		         0   0 100   | 31900   0   9</span><br><span class="line">5:		         0   0 100   | 68117   0   9</span><br><span class="line">6:		         0   0 100   | 21546   0   9</span><br><span class="line">7:		         0   0 100   | 50570   0  10</span><br><span class="line">8:		         0   0 100   | 18953   0  10</span><br><span class="line">9:		         0   0 100   | 26238   0  10</span><br><span class="line">10:		         0   0 100   | 11312   0  10</span><br><span class="line">11:		         0   0 100   | 19247   0  10</span><br><span class="line">12:		         0   0 100   | 7830   0  10</span><br><span class="line">13:		         0   0 100   | 11899   0  11</span><br><span class="line">14:		         0   0 100   | 7536   0  11</span><br><span class="line">15:		         0   0 100   | 11404   0  11</span><br><span class="line">16:		         0   0 100   | 5772   0  11</span><br><span class="line">17:		         0   0 100   | 10292   0  11</span><br><span class="line">18:		         0   0 100   | 5009   0  11</span><br><span class="line">19:		         0   0 100   | 9608   0  11</span><br><span class="line">20:		         0   0 100   | 5081   0  11</span><br><span class="line">21:		         0   0 100   | 4204   0  11</span><br><span class="line">22:		         0   0 100   | 3068   0  11</span><br><span class="line">23:		         0   0 100   | 7655   0  11</span><br><span class="line">24:		         0   0 100   | 2287   0  11</span><br><span class="line">25:		         0   0 100   | 3764   0  11</span><br><span class="line">26:		         0   0 100   | 2646   0  11</span><br><span class="line">27:		         0   0 100   | 7094   0  11</span><br><span class="line">28:		         0   0 100   | 2795   0  11</span><br><span class="line">29:		         0   0 100   | 5459   0  11</span><br><span class="line">30:		         0   0 100   | 4235   0  11</span><br><span class="line">31:		         0   0 100   | 11926820  88 100</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">disk I/Os <span class="keyword">in</span> flight    ios   % cum % |  ios         % cum %</span><br><span class="line">1:		  38351190  51  51   | 5305301  39  39</span><br><span class="line">2:		   6938269   9  60   | 191433   1  40</span><br><span class="line">3:		   3614622   4  65   | 44751   0  41</span><br><span class="line">4:		   2735516   3  69   | 660873   4  45</span><br><span class="line">5:		   2324550   3  72   | 65079   0  46</span><br><span class="line">6:		   1983419   2  74   | 163256   1  47</span><br><span class="line">7:		   1663432   2  77   | 3255   0  47</span><br><span class="line">8:		   1511366   2  79   | 2346   0  47</span><br><span class="line">9:		   1401406   1  80   | 1565   0  47</span><br><span class="line">10:		   1243768   1  82   | 1120   0  47</span><br><span class="line">11:		   1058457   1  83   |  832   0  47</span><br><span class="line">12:		    967541   1  85   |  815   0  47</span><br><span class="line">13:		    889265   1  86   |  663   0  47</span><br><span class="line">14:		    829720   1  87   |  520   0  47</span><br><span class="line">15:		    800230   1  88   |  474   0  47</span><br><span class="line">16:		    771663   1  89   |  431   0  47</span><br><span class="line">17:		    610569   0  90   |  418   0  47</span><br><span class="line">18:		    442565   0  91   |  334   0  47</span><br><span class="line">19:		    307220   0  91   |  305   0  47</span><br><span class="line">20:		    272028   0  91   |  280   0  47</span><br><span class="line">21:		    253795   0  92   |  295   0  47</span><br><span class="line">22:		    242172   0  92   |  258   0  47</span><br><span class="line">23:		    226890   0  92   |  246   0  47</span><br><span class="line">24:		    217916   0  93   |  247   0  47</span><br><span class="line">25:		    214475   0  93   |  246   0  47</span><br><span class="line">26:		    213455   0  93   |  218   0  47</span><br><span class="line">27:		    212579   0  93   |  208   0  47</span><br><span class="line">28:		    211080   0  94   |  208   0  47</span><br><span class="line">29:		    213429   0  94   |  226   0  47</span><br><span class="line">30:		    214716   0  94   |  202   0  47</span><br><span class="line">31:		   3856423   5 100   | 7067091  52 100</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">I/O time (1/1000s)     ios   % cum % |  ios         % cum %</span><br><span class="line">1:		   4912365  16  16   |    0   0   0</span><br><span class="line">2:		    388506   1  17   |    0   0   0</span><br><span class="line">4:		    149669   0  18   |    0   0   0</span><br><span class="line">8:		    244626   0  19   |    0   0   0</span><br><span class="line">16:		    935708   3  22   |    0   0   0</span><br><span class="line">32:		   2064895   6  29   |    0   0   0</span><br><span class="line">64:		   5956904  19  49   |    0   0   0</span><br><span class="line">128:		  10185223  34  83   |    0   0   0</span><br><span class="line">256:		   2682379   8  92   |    0   0   0</span><br><span class="line">512:		   1875753   6  98   |    0   0   0</span><br><span class="line">1K:		    369317   1  99   |    0   0   0</span><br><span class="line">2K:		     45895   0  99   |    0   0   0</span><br><span class="line">4K:		      9847   0  99   |    0   0   0</span><br><span class="line">8K:		      2347   0  99   |    0   0   0</span><br><span class="line">16K:		       484   0  99   |    0   0   0</span><br><span class="line">32K:		        26   0 100   |    0   0   0</span><br><span class="line"></span><br><span class="line">                           <span class="built_in">read</span>      |     write</span><br><span class="line">disk I/O size          ios   % cum % |  ios         % cum %</span><br><span class="line">1:		         0   0   0   |  167   0   0</span><br><span class="line">2:		         0   0   0   | 4361   0   0</span><br><span class="line">4:		         0   0   0   | 13790   0   0</span><br><span class="line">8:		         4   0   0   | 1318   0   0</span><br><span class="line">16:		         0   0   0   | 4056   0   0</span><br><span class="line">32:		         1   0   0   | 21350   0   0</span><br><span class="line">64:		         1   0   0   | 8433   0   0</span><br><span class="line">128:		       951   0   0   | 17370   0   0</span><br><span class="line">256:		         1   0   0   | 68966   0   1</span><br><span class="line">512:		         1   0   0   | 122303   0   1</span><br><span class="line">1K:		         0   0   0   | 86504   0   2</span><br><span class="line">2K:		         2   0   0   | 76351   0   3</span><br><span class="line">4K:		  32617656  43  43   | 116250   0   4</span><br><span class="line">8K:		    638577   0  44   | 409368   3   7</span><br><span class="line">16K:		   1108531   1  45   | 240798   1   8</span><br><span class="line">32K:		   9386337  12  58   | 172134   1  10</span><br><span class="line">64K:		    785380   1  59   | 114421   0  10</span><br><span class="line">128K:		    340607   0  60   | 86673   0  11</span><br><span class="line">256K:		    474447   0  60   | 147949   1  12</span><br><span class="line">512K:		    745719   0  61   | 803491   5  18</span><br><span class="line">1M:		  28695511  38 100   | 10967594  81 100</span><br></pre></td></tr></table></figure>

<h3 id="Get-lustre-client-to-ipaddr"><a href="#Get-lustre-client-to-ipaddr" class="headerlink" title="Get lustre client to ipaddr"></a>Get lustre client to ipaddr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/fs/lustre/nodemap/default/exports</span><br></pre></td></tr></table></figure>

<h3 id="nmount-namespace-“D”-and-clear-mds-info"><a href="#nmount-namespace-“D”-and-clear-mds-info" class="headerlink" title="nmount namespace “D” and clear mds info"></a>nmount namespace “D” and clear mds info</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 &gt; /proc/fs/lustre/mdc/zfsz4-*/active</span><br><span class="line">lctl set_param mdc.zfsz4-*.active=0</span><br></pre></td></tr></table></figure>

<h3 id="pin-cpu-for-zfs-and-process"><a href="#pin-cpu-for-zfs-and-process" class="headerlink" title="pin cpu for zfs and process"></a>pin cpu for zfs and process</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">core=0</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> `pgrep <span class="string">'z_wr_iss$'</span>`; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> taskset -p -c <span class="variable">$core</span> <span class="variable">$i</span></span><br><span class="line">  <span class="comment">#taskset -p -c $core $i</span></span><br><span class="line">  core=`expr <span class="variable">$core</span> + 1`</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="set-lustre-client-for-a-lot-of-inodes-metadata-operates"><a href="#set-lustre-client-for-a-lot-of-inodes-metadata-operates" class="headerlink" title="set lustre client for a lot of inodes metadata operates"></a>set lustre client for a lot of inodes metadata operates</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span></span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=600000</span><br></pre></td></tr></table></figure>

<h3 id="show-more-Lustre-network-error"><a href="#show-more-Lustre-network-error" class="headerlink" title="show more Lustre network error"></a>show more Lustre network error</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param printk=+neterror</span><br></pre></td></tr></table></figure>

<h3 id="Lustre-disable-ldiskfs-OSS-cache"><a href="#Lustre-disable-ldiskfs-OSS-cache" class="headerlink" title="Lustre disable ldiskfs OSS cache"></a>Lustre disable ldiskfs OSS cache</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lru_size</span><br></pre></td></tr></table></figure>

<h3 id="ubuntu-18-04-install-lustre-client-from-source-code"><a href="#ubuntu-18-04-install-lustre-client-from-source-code" class="headerlink" title="ubuntu 18.04 install lustre client from source code"></a>ubuntu 18.04 install lustre client from source code</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ <span class="built_in">cd</span> <span class="variable">$&#123;BUILDPATH&#125;</span>/lustre-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ ./configure --<span class="built_in">disable</span>-server --with-linux=/usr/src/linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  /lib/modules/4.15.0-64-generic/kernel/drivers/staging/lustre/</span><br><span class="line">$ depmod -a</span><br></pre></td></tr></table></figure>

<h3 id="Make-sure-ost-options"><a href="#Make-sure-ost-options" class="headerlink" title="Make sure ost options"></a>Make sure ost options</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/fs/ldiskfs/dm-xx/options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid=0</span><br><span class="line">resgid=0</span><br><span class="line">errors=remount-ro</span><br><span class="line">commit=5</span><br><span class="line">min_batch_time=0</span><br><span class="line">max_batch_time=15000</span><br><span class="line">stripe=0</span><br><span class="line">data=ordered</span><br><span class="line">inode_readahead_blks=32</span><br><span class="line">init_itable=10</span><br><span class="line">max_dir_size_kb=0</span><br></pre></td></tr></table></figure>

      </div>
      
      
      
    </div>
    

    
    
    
  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Use-high-High-frequency-and-less-cores-CPU"><span class="nav-number">1.</span> <span class="nav-text">Use high High frequency and less cores CPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Use-single-socket-server-you-don’t-need-to-tuning-numa"><span class="nav-number">2.</span> <span class="nav-text">Use single socket server, you don’t need to tuning numa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SAS-HBA-tuning"><span class="nav-number">3.</span> <span class="nav-text">SAS HBA tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ldiskfs-kenrel-tuning"><span class="nav-number">4.</span> <span class="nav-text">ldiskfs kenrel tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CPU"><span class="nav-number">4.0.1.</span> <span class="nav-text">CPU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tuning-deadline-and-noops"><span class="nav-number">4.0.2.</span> <span class="nav-text">tuning deadline and noops</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Memroy-setting-ldiskfs"><span class="nav-number">4.0.3.</span> <span class="nav-text">Memroy setting (ldiskfs)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MD3460-RAID-config"><span class="nav-number">4.1.</span> <span class="nav-text">MD3460 RAID config</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MDS-OSS"><span class="nav-number">4.2.</span> <span class="nav-text">MDS OSS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Client"><span class="nav-number">4.3.</span> <span class="nav-text">Client</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#About-osd-zfs"><span class="nav-number">5.</span> <span class="nav-text">About osd_zfs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lustre-conf"><span class="nav-number">6.</span> <span class="nav-text">lustre.conf</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#lustre-client"><span class="nav-number">6.1.</span> <span class="nav-text">lustre client</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trace-log-RT"><span class="nav-number">7.</span> <span class="nav-text">Trace log RT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dump-log"><span class="nav-number">8.</span> <span class="nav-text">Dump log</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ftrace"><span class="nav-number">9.</span> <span class="nav-text">Ftrace</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Changelog"><span class="nav-number">10.</span> <span class="nav-text">Changelog</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Set-timeout"><span class="nav-number">11.</span> <span class="nav-text">Set timeout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Get-timeout-and-state"><span class="nav-number">12.</span> <span class="nav-text">Get timeout and state</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kdump"><span class="nav-number">13.</span> <span class="nav-text">Kdump</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fsck-ldiskfs-file-system-corruption"><span class="nav-number">14.</span> <span class="nav-text">Fsck ldiskfs file system corruption</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Errors"><span class="nav-number">14.1.</span> <span class="nav-text">Errors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Flush-the-journal"><span class="nav-number">14.2.</span> <span class="nav-text">Flush the journal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Caution"><span class="nav-number">14.3.</span> <span class="nav-text">Caution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Check-only-mode"><span class="nav-number">14.4.</span> <span class="nav-text">Check only mode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Prudent-mode"><span class="nav-number">14.5.</span> <span class="nav-text">Prudent mode</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Answer-yes"><span class="nav-number">14.6.</span> <span class="nav-text">Answer yes</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Storage-target-could-not-mount-mount-lustre-failed"><span class="nav-number">15.</span> <span class="nav-text">Storage target could not mount (mount.lustre failed)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Caution-1"><span class="nav-number">15.1.</span> <span class="nav-text">Caution</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Writeconf"><span class="nav-number">15.2.</span> <span class="nav-text">Writeconf</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lustre-clinet-performance"><span class="nav-number">15.3.</span> <span class="nav-text">Lustre clinet performance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mount"><span class="nav-number">15.4.</span> <span class="nav-text">Mount</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flush-all-of-the-metadata-client-mdc-locks-on-this-node"><span class="nav-number">16.</span> <span class="nav-text">Flush all of the metadata client (mdc) locks on this node</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lustre-quota-ldiskfs"><span class="nav-number">17.</span> <span class="nav-text">Lustre quota (ldiskfs)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MDS"><span class="nav-number">17.1.</span> <span class="nav-text">MDS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Clear-pool"><span class="nav-number">17.2.</span> <span class="nav-text">Clear pool</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Add-pool"><span class="nav-number">17.3.</span> <span class="nav-text">Add pool</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OSS"><span class="nav-number">17.4.</span> <span class="nav-text">OSS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Client-1"><span class="nav-number">17.5.</span> <span class="nav-text">Client</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#User-group-Quota"><span class="nav-number">17.5.1.</span> <span class="nav-text">User&#x2F;group Quota</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lfs-migrate-data-to-another-OST"><span class="nav-number">17.5.2.</span> <span class="nav-text">lfs migrate data to another OST</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abort-tgt-reocv"><span class="nav-number">18.</span> <span class="nav-text">Abort tgt_reocv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Get-lustre-version"><span class="nav-number">19.</span> <span class="nav-text">Get lustre version</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lustre-status"><span class="nav-number">20.</span> <span class="nav-text">Lustre status</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#check-ost-status"><span class="nav-number">21.</span> <span class="nav-text">check ost status</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#show-ost-and-ip-addr-relationship"><span class="nav-number">21.0.1.</span> <span class="nav-text">show ost and ip addr relationship</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#OST-threads"><span class="nav-number">21.0.2.</span> <span class="nav-text">OST threads</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#No-roots-quash"><span class="nav-number">22.</span> <span class="nav-text">No roots quash</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Readonly-OST"><span class="nav-number">23.</span> <span class="nav-text">Readonly OST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rebalance-OST"><span class="nav-number">24.</span> <span class="nav-text">Rebalance OST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Move-data-to-one-OST"><span class="nav-number">25.</span> <span class="nav-text">Move data to one OST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Enable-job-status"><span class="nav-number">26.</span> <span class="nav-text">Enable job status</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-Lustre-Message-Loss-and-Tuning-for-Resiliency"><span class="nav-number">27.</span> <span class="nav-text">Understanding Lustre Message Loss and Tuning for Resiliency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evict-client"><span class="nav-number">28.</span> <span class="nav-text">Evict client</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OSS-setting-for-Ethernet-test"><span class="nav-number">29.</span> <span class="nav-text">OSS setting for Ethernet (test)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lnet"><span class="nav-number">30.</span> <span class="nav-text">Lnet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lustre-client-setting"><span class="nav-number">31.</span> <span class="nav-text">Lustre client setting</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Lustre-client-rebuild"><span class="nav-number">31.1.</span> <span class="nav-text">Lustre client rebuild</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lustre-fid-and-path"><span class="nav-number">31.2.</span> <span class="nav-text">lustre fid and path</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lustre-set-stripe"><span class="nav-number">31.3.</span> <span class="nav-text">Lustre set stripe</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lustre-client-buffer-and-rpcs"><span class="nav-number">31.4.</span> <span class="nav-text">Lustre client buffer and rpcs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Max-mod-rpcs-in-flight"><span class="nav-number">31.5.</span> <span class="nav-text">Max_mod_rpcs_in_flight</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#About-openzfs-performance"><span class="nav-number">32.</span> <span class="nav-text">About openzfs performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reubild-lustre-client"><span class="nav-number">33.</span> <span class="nav-text">Reubild lustre client</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Check-zfs-dmu-tx-assign"><span class="nav-number">34.</span> <span class="nav-text">Check zfs dmu_tx_assign</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Monitor"><span class="nav-number">34.1.</span> <span class="nav-text">Monitor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Lustre-OSS-info"><span class="nav-number">34.2.</span> <span class="nav-text">Lustre OSS info</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Get-lustre-client-to-ipaddr"><span class="nav-number">35.</span> <span class="nav-text">Get lustre client to ipaddr</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nmount-namespace-“D”-and-clear-mds-info"><span class="nav-number">36.</span> <span class="nav-text">nmount namespace “D” and clear mds info</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pin-cpu-for-zfs-and-process"><span class="nav-number">37.</span> <span class="nav-text">pin cpu for zfs and process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#set-lustre-client-for-a-lot-of-inodes-metadata-operates"><span class="nav-number">38.</span> <span class="nav-text">set lustre client for a lot of inodes metadata operates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#show-more-Lustre-network-error"><span class="nav-number">39.</span> <span class="nav-text">show more Lustre network error</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lustre-disable-ldiskfs-OSS-cache"><span class="nav-number">40.</span> <span class="nav-text">Lustre disable ldiskfs OSS cache</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ubuntu-18-04-install-lustre-client-from-source-code"><span class="nav-number">41.</span> <span class="nav-text">ubuntu 18.04 install lustre client from source code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Make-sure-ost-options"><span class="nav-number">42.</span> <span class="nav-text">Make sure ost options</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">1k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">1 mins.</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://b946c5a0bf547c89.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://yoursite.com/lustre-operations.html",
            identifier: "lustre-operations.html",
            title: "Lustre operations"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://b946c5a0bf547c89.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
