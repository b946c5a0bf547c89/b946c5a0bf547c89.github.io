<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Muse',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="slot status12345678910111213$ 05:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 for 25GbE SFP28 (rev 02)$ lspci -s 05:00.0 -vvv | grep -Ei &#39;8G|MSI-X&#39;      Capabilities: [70] MS">
<meta property="og:type" content="article">
<meta property="og:title" content="ethernet nic tuning">
<meta property="og:url" content="http://yoursite.com/2018/06/01/nic_ethernet_tuning/index.html">
<meta property="og:site_name" content="b946c5a0bf547c89">
<meta property="og:description" content="slot status12345678910111213$ 05:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 for 25GbE SFP28 (rev 02)$ lspci -s 05:00.0 -vvv | grep -Ei &#39;8G|MSI-X&#39;      Capabilities: [70] MS">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2018-06-01T02:49:42.000Z">
<meta property="article:modified_time" content="2020-01-18T11:20:26.592Z">
<meta property="article:author" content="Ginger">
<meta property="article:tag" content="benchmark">
<meta property="article:tag" content="nic">
<meta property="article:tag" content="irq">
<meta property="article:tag" content="network">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2018/06/01/nic_ethernet_tuning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>ethernet nic tuning | b946c5a0bf547c89</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">b946c5a0bf547c89</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">My Silent Hill</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/01/nic_ethernet_tuning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/logo-4-blog.png">
      <meta itemprop="name" content="Ginger">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="b946c5a0bf547c89">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ethernet nic tuning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-06-01 10:49:42" itemprop="dateCreated datePublished" datetime="2018-06-01T10:49:42+08:00">2018-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-18 19:20:26" itemprop="dateModified" datetime="2020-01-18T19:20:26+08:00">2020-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2018/06/01/nic_ethernet_tuning/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/06/01/nic_ethernet_tuning/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>61k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>55 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="slot-status"><a href="#slot-status" class="headerlink" title="slot status"></a>slot status</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ 05:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 <span class="keyword">for</span> 25GbE SFP28 (rev 02)</span><br><span class="line">$ lspci -s 05:00.0 -vvv | grep -Ei <span class="string">'8G|MSI-X'</span></span><br><span class="line">      Capabilities: [70] MSI-X: Enable+ Count=129 Masked-</span><br><span class="line">              LnkCap: Port <span class="comment">#0, Speed 8GT/s, Width x8, ASPM L1, Latency L0 &lt;2us, L1 &lt;16us</span></span><br><span class="line">              LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-</span><br><span class="line"></span><br><span class="line"><span class="comment">## RSS support</span></span><br><span class="line">$ lspci -v -s 83:00.0 | grep <span class="string">"MSI-X: Enable+"</span></span><br><span class="line">83:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</span><br><span class="line">        Flags: bus master, fast devsel, latency 0, IRQ 247, NUMA node 1</span><br><span class="line">        I/O ports at d020 [size=32]</span><br><span class="line">        Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+</span><br><span class="line">        Capabilities: [70] MSI-X: Enable+ Count=64 Masked-</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="THEORETICAL-MAXIMUM-RATE"><a href="#THEORETICAL-MAXIMUM-RATE" class="headerlink" title="THEORETICAL MAXIMUM RATE"></a><a href="https://support-kb.spirent.com/resources/sites/SPIRENT/content/live/FAQS/10000/FAQ10597/en_US/How_to_Test_10G_Ethernet_WhitePaper_RevB.PDF" target="_blank" rel="noopener">THEORETICAL MAXIMUM RATE</a></h3><p>There are two important concepts related to 10GbE performance: frame rate and throughput. The MAC bit rate of 10GbE, defined in the IEEE standard 802.3ae, is 10 billion bits per second. Frame rate is a simple arithmetic calculation based on the bit rate and frame format definitions. Throughput, defined in IETF RFC 1242, is the highest rate at which the system under test can forward the offered load, without loss. Manufacturers can claim line-rate throughput only if their switch forwards all the traffic offered at the 10Gb/s line rate for the entire duration of the test. The bit rate at which 10GbE Media Access Layer (MAC) operates, 10 billion bits per second, is only one of the parameters in defining the transmission rate for this important new technology. The usual description of true network performance is frame rate, which indicates how many Ethernet frames are moving across the network. The maximum frame rate for 10GbE is determined by a formula that divides the 10 billion bits per second by the preamble, frame length, and inter-frame gap fields, expressed in bits. The maximum frame rate is calculated using the minimum values of the following parameters, as described in the IEEE 802.3ae standard:<br>•        Preamble - 8 bytes * 8 = 64 bits<br>•        Frame length - 64 bytes (minimum) * 8 = 512 bits<br>•        Inter-frame gap - 12 bytes (minimum) * 8 = 96 bits<br>Therefore,<br>Maximum Frame Rate =<br>MAC Transmit Bit Rate/<br>(Preamble + Frame Length + Inter-frame Gap)<br>= 10,000,000,000 / (64 + 512 + 96)<br>= 10,000,000,000 / 672<br>= 14,880,952.38 frame per second (fps)</p>
<h3 id="About-FEC"><a href="#About-FEC" class="headerlink" title="About FEC"></a><a href="https://solarflare.hammer-europe.com/assets/uploads/resources/QLogic%20-%20White%20Paper%20-%2025Gb%20Ethernet.pdf" target="_blank" rel="noopener">About FEC</a></h3><p>IEEE 802.3 Standard Interfaces that Specify 25GbE</p>
<table>
<thead>
<tr>
<th align="center">Phy layer</th>
<th align="center">Name</th>
<th align="center">error Correction</th>
</tr>
</thead>
<tbody><tr>
<td align="center">MMF Optics</td>
<td align="center">25GBASE-SR</td>
<td align="center">RS-FEC</td>
</tr>
<tr>
<td align="center">Direct Attach Copper</td>
<td align="center">25GBASE-CR</td>
<td align="center">BASE-R FEC or RS-FEC</td>
</tr>
<tr>
<td align="center">Direct Attach Copper</td>
<td align="center">25GBASE-CR-S</td>
<td align="center">BASE-R FEC or disabled</td>
</tr>
<tr>
<td align="center">Electrical Backplane</td>
<td align="center">25GBASE-KR</td>
<td align="center">BASE-R FEC or RS-FEC</td>
</tr>
<tr>
<td align="center">Electrical Backplane</td>
<td align="center">25GBASE-KR-S</td>
<td align="center">BASE-R FEC or disabled</td>
</tr>
<tr>
<td align="center">Twisted Pair</td>
<td align="center">25GBASE-T</td>
<td align="center">N/A</td>
</tr>
</tbody></table>
<p>The IEEE standard specifies two backplane and copper interfaces. These have different goals, hence the different interface. The –S short reach interfaces aim to support high-quality cables without Forward Error Correction (FEC) to minimize latency. Full reach interfaces aimto support the lowest possible cable or backplane cost and the longest possible reach, which do require the use of FEC. FEC options include BASE-R FEC (also referred to as Fire Code) and RS-FEC (also referred to as Reed-Solomon). RS-FEC has been used for a range of applications including data storage satellite transmissions. BASE-R FEC is a newer technology that is particularly well suited for correction of the burst errors<br>typical in a backplane channel resulting from error propagation in the receive equalizer.</p>
<p>IEEE 标准指定了两个底板和铜接口。这些接口的目标不同，因此接口存在不同。–S 短距离接口旨在支持无需转发纠错（FEC）的高质<br>量电缆，以最大限度地降低延迟。全距离接口旨在实现尽可能低的电缆或背板成本以及尽可能长的距离，这需要使用FEC。FEC 选项包<br>括 BASE-R FEC（也称为 Fire Code）和 RS-FEC（也称为 Reed-Solomon）。RS-FEC 已用于一系列应用，包括数据存储卫星传播。BASE-R FEC 是一种新型技术，特别适合纠正背板通道中由于接收均衡器的错误传播而导致的突发错误。</p>
<h3 id="Tune-adm-policy"><a href="#Tune-adm-policy" class="headerlink" title="Tune-adm policy"></a>Tune-adm policy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># default config files</span><br><span class="line">$ ls &#x2F;usr&#x2F;lib&#x2F;tuned</span><br><span class="line">balanced  desktop  functions  latency-performance  network-latency  network-throughput  powersave  recommend.d  throughput-performance  virtual-guest  virtual-host</span><br><span class="line"></span><br><span class="line">$ tuned-adm list</span><br><span class="line">$ tuned-adm profile network-throughput</span><br><span class="line">$ tuned-adm active</span><br></pre></td></tr></table></figure>

<p>Custom tuned.conf</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;etc&#x2F;tuned&#x2F;my-variables.conf</span><br><span class="line"></span><br><span class="line">[variables]</span><br><span class="line">include&#x3D;&#x2F;etc&#x2F;tuned&#x2F;my-variables.conf</span><br><span class="line"></span><br><span class="line">[bootloader]</span><br><span class="line">cmdline&#x3D;isolcpus&#x3D;0,1,2,3</span><br></pre></td></tr></table></figure>

<h3 id="CPU-setting"><a href="#CPU-setting" class="headerlink" title="CPU setting"></a>CPU setting</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cpupower  frequency-set -g performance</span><br><span class="line">cpupower idle-set -d 3</span><br><span class="line">cpupower idle-set -d 2</span><br><span class="line">cpupower idle-set -d 1</span><br><span class="line">cpupower idle-set -d 0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> performance &gt; /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</span><br><span class="line"></span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span><br><span class="line">$ cat /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get local cpus</span></span><br><span class="line">$ cat /sys/class/net/eth7/device/local_cpus</span><br><span class="line">0000,000fffc0,00000000,0fffc000</span><br></pre></td></tr></table></figure>

<h3 id="nic-offload"><a href="#nic-offload" class="headerlink" title="nic offload"></a>nic offload</h3><table>
<thead>
<tr>
<th>Features</th>
<th>status</th>
</tr>
</thead>
<tbody><tr>
<td>TSO TCP Segmentation Offload</td>
<td>hardware on</td>
</tr>
<tr>
<td>GSO Generic Segmentation Offload,soft TSO</td>
<td>software off</td>
</tr>
<tr>
<td>GRO Generic Receive Offload/LRO</td>
<td>hardware on, options ixgbe LRO=1</td>
</tr>
<tr>
<td>UFO UDP Fragmentation Offload</td>
<td>hardware off fixed</td>
</tr>
<tr>
<td>rx-checksumming</td>
<td>hardware on</td>
</tr>
<tr>
<td>tx-checksumming</td>
<td>hardware on</td>
</tr>
<tr>
<td>scatter-gather</td>
<td>hardware on</td>
</tr>
<tr>
<td>RSS Receive side scaling</td>
<td>hardware on</td>
</tr>
<tr>
<td>RPS software RSS</td>
<td>software off</td>
</tr>
<tr>
<td>RFS Receive Flow Streering,UDP support ?</td>
<td>software on</td>
</tr>
<tr>
<td>XPS Transmit Packet Steering</td>
<td>software on</td>
</tr>
<tr>
<td>busy-poll: on fixed</td>
<td>hw and sw, work with sysctl.net.core.busy_poll &gt; 0</td>
</tr>
</tbody></table>
<p>TSO = LSO (also called large segmentation offload)<br>RFS and XPS has the same function from receive or transmit, avoid cache miss, numa overhead<br>Receive Packet Steering (RPS) is logically a software implementation of RSS<br>Generic segmentation offload (GSO) is logically a software implementation of TSO</p>
<p>if TSO was on, disable GSO, same with RSS and RPS</p>
<p>Enabling the RFS requires enabling the ‘ntuple’ flag via the ethtool,RFS requires the kernel to be compiled with the CONFIG_RFS_ACCEL option. This options is available in kernels 2.6.39 and above. Furthermore, RFS requires Device Managed Flow Steering support.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool --offload enp5s0 rx on tx on</span><br><span class="line">Actual changes:</span><br><span class="line">rx-checksumming: on</span><br><span class="line">tx-checksumming: on</span><br><span class="line">	tx-checksum-ipv4: on</span><br><span class="line">	tx-checksum-ipv6: on</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">	tx-tcp-segmentation: on</span><br><span class="line">	tx-tcp6-segmentation: on</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable TSO</span></span><br><span class="line"><span class="comment"># If your NIC not support TSO, you could enable GSO</span></span><br><span class="line">$ ethtool -K enp4s0 tso on</span><br><span class="line">$ ethtool -K enp4s0 gso off</span><br><span class="line">$ ethtool -K ethX rx on  <span class="comment">## rx-checksuming tcp offload</span></span><br><span class="line">$ ethtool -K ethX sg on  <span class="comment">## tcp-sgmentation offloading</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#In case “tx-nocache-copy” is enabled, (this is the case for some kernels, e.g. kernel 3.10, which is the default for RH7.0) “tx-nocache-copy” should be disabled.</span></span><br><span class="line">$ ethtool -K ethX tx-nocache-copy off</span><br></pre></td></tr></table></figure>
<p><a href="(https://www.mellanox.com/related-docs/prod_software/Performance_Tuning_Guide_for_Mellanox_Network_Adapters_Archive.pdf">100GbE setting</a></p>
<h4 id="Resize-the-hardware-buffer-queue-to-max"><a href="#Resize-the-hardware-buffer-queue-to-max" class="headerlink" title="Resize the hardware buffer queue to max"></a>Resize the hardware buffer queue to max</h4><p>Reduce the number of packets being dropped by increasing the size of the queue so that the it does not overflow as easily</p>
<p>big buffer will cause high latency with the throughput setting</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -G enp1s0f0 RX 4096 TX 4096</span><br><span class="line">$ ethtool -g enp1s0f0</span><br><span class="line">Ring parameters <span class="keyword">for</span> enp1s0f0:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:		4096</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		4096</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:		4096</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		4096</span><br></pre></td></tr></table></figure>

<h4 id="setting-driver"><a href="#setting-driver" class="headerlink" title="setting driver"></a>setting driver</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/modprobe.d/ixgbe.conf</span><br><span class="line">options ixgbe allow_unsupported_sfp=1,1</span><br><span class="line">options ixgbe MQ=1,1 RSS=8,8 <span class="comment"># I don 't think too many CPU is a good setting</span></span><br><span class="line">options ixgbe LRO=1</span><br><span class="line">options ixgbe InterruptThrottleRate=20000,20000</span><br><span class="line"></span><br><span class="line"><span class="comment"># unsupport optical module</span></span><br><span class="line">$ modprobe ixgbe allow_unsupported_sfp=1,1</span><br></pre></td></tr></table></figure>
<p>MQ:Disable or enable Multiple Queues, default 1 (array of int)<br>RSS:Number of Receive-Side Scaling Descriptor Queues, default 0=number of cpus (array of int)<br>InterruptThrottleRate:Maximum interrupts per second, per vector, (0,1,956-488281), default 1 (array of int)<br>LRO:Large Receive Offload (0,1), default 0 = off (array of int)</p>
<h3 id="Interrupt-Queues"><a href="#Interrupt-Queues" class="headerlink" title="Interrupt Queues"></a>Interrupt Queues</h3><h4 id="Busy-Polling"><a href="#Busy-Polling" class="headerlink" title="Busy Polling"></a>Busy Polling</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">                        high-latency</span><br><span class="line">interrupt-based -----------------------------&gt; poll-based</span><br></pre></td></tr></table></figure>

<p>Busy polling helps reduce latency in the network receive path by allowing socket layer code to poll the receive queue of a network device, and disabling network interrupts. </p>
<p><code>This removes delays caused by the interrupt and the resultant context switch.</code><br>However, <code>it also increases CPU utilization</code>. Busy polling also prevents the CPU from sleeping, which can incur additional power consumption.</p>
<p>single queue map single cpu core, avoid lock or race cpu resource </p>
<p>kernel 3.11 support SO_BUSY_POLL<br>Busy polling is disabled by default (sysctl.net.core.busy_poll = 0)<br>This parameter controls the number of microseconds to wait for packets on the device queue for socket poll and selects. Red Hat recommends a value of 50</p>
<p>To enable busy polling globally<br>you must also set sysctl.net.core.busy_read to a value other than 0.<br>This parameter controls the number of microseconds to wait for packets on the device queue for socket reads.<br>It also sets the default value of the SO_BUSY_POLL option. </p>
<p>Red Hat recommends a value of 50 for a small number of sockets, and a value of 100 for large numbers of sockets. For extremely large numbers of sockets (more than several hundred), use epoll(kernel 4.12) instead.</p>
<p><a href="https://oxnz.github.io/2016/05/03/performance-tuning-networking/" target="_blank" rel="noopener">Busy polling helps reduce latency in the network receive path by</a></p>
<ul>
<li>allowing socket layer code to poll the receive queue of a network device </li>
<li>and disable network interrupts</li>
</ul>
<p>delays caused by the interrupts and the resultant context switches<br>increses CPU utilization.Also prevent the CPU from sleeping, which can incur additional power comsumption.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sw</span></span><br><span class="line">net.core.busy_poll = 50 <span class="comment"># default 0</span></span><br><span class="line">net.core.busy_read = 100 <span class="comment"># default 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#hw</span></span><br><span class="line">$ ethtool -k device | grep <span class="string">"busy-poll"</span></span><br><span class="line">busy-poll: on [fixed]</span><br></pre></td></tr></table></figure>

<p>Busy polling behavior is supported by the following drivers. These drivers are also supported on Red Hat Enterprise Linux 7.1<br>driver support: bnx2x,be2net,ixgbe,mlx4,myri10ge</p>
<p>mlx4 driver with busy polling</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rx-usecs: 16</span><br><span class="line">rx-frames: 44</span><br></pre></td></tr></table></figure>
<p><a href="http://www.cnhalo.net/2017/07/24/linux-busy-poll/" target="_blank" rel="noopener">netperf TCP_RR 1 byte payload each way, 3.11 kernel,default 17500tps, busy poll could reach 6300tps</a></p>
<h3 id="Socket-receive-queues"><a href="#Socket-receive-queues" class="headerlink" title="Socket receive queues"></a>Socket receive queues</h3><h4 id="Change-the-speed-of-the-incoming-queue"><a href="#Change-the-speed-of-the-incoming-queue" class="headerlink" title="Change the speed of the incoming queue"></a>Change the speed of the incoming queue</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/sys/net/core/dev_weight</span><br><span class="line">64</span><br><span class="line"><span class="comment">#64 was default value</span></span><br></pre></td></tr></table></figure>

<p>The maximum number of packets that kernel can handle on a NAPI interrupt, it’s a Per-CPU variable. For drivers that support LRO HW or GRO_HW, a hardware aggregated packet is counted as one packet in this context</p>
<p>Altering the drain rate of a queue is usually the simplest way to mitigate poor network performance. However, increasing the number of packets that a device can receive at one time uses additional processor time, during which no other processes can be scheduled, so this can cause other performance problems</p>
<p>Device weight refers to the number of packets a device can receive at one time (in a single scheduled processor access). You can increase the rate at which a queue is drained by increasing its device weight, which is controlled by the dev_weight parameter.</p>
<p>it will increase CPU overhead</p>
<p>Altering the drain rate of a queue is usually the simplest way to mitigate poor network performance. However, increasing the number of packets that a device can receive at one time uses additional processor time, during which no other processes can be scheduled, so this can cause other performance problems.</p>
<h5 id="Increase-the-depth-of-the-application’s-socket-queue"><a href="#Increase-the-depth-of-the-application’s-socket-queue" class="headerlink" title="Increase the depth of the application’s socket queue"></a>Increase the depth of the application’s socket queue</h5><p>Increase the value of /proc/sys/net/core/rmem_default</p>
<ul>
<li>Decrease the speed of incoming traffic<ul>
<li>filter</li>
<li>dropping</li>
<li>lower devcei weight</li>
</ul>
</li>
<li>Increse the depth of the application’s socket queue (not a long-term solution)</li>
</ul>
<p>This parameter controls the default size of the receive buffer used by sockets. This value must be smaller than or equal to the value of /proc/sys/net/core/rmem_max.</p>
<p>This parameter controls the maximum size in bytes of a socket’s receive buffer. Use getsockopt to get current value</p>
<h4 id="Use-setsockopt-to-configure-a-larger-SO-RCVBUF-value-userspace"><a href="#Use-setsockopt-to-configure-a-larger-SO-RCVBUF-value-userspace" class="headerlink" title="Use setsockopt to configure a larger SO_RCVBUF value (userspace)"></a>Use setsockopt to configure a larger SO_RCVBUF value (userspace)</h4><p>This parameter controls the maximum size in bytes of a socket’s receive buffer. Use the getsockopt system call to determine the current value of the buffer.</p>
<h4 id="RSS-IRQ-Affinity"><a href="#RSS-IRQ-Affinity" class="headerlink" title="RSS IRQ Affinity"></a><a href="https://access.redhat.com/solutions/2144921" target="_blank" rel="noopener">RSS IRQ Affinity</a></h4><p>Get the PCIE device numa node</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/class/net/[interface]/device/numa_node</span><br><span class="line">$ cat /sys/devices/[PCI root]/[PCIe <span class="keyword">function</span>]/numa_node</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>CPU</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>0001</td>
<td>0010</td>
<td>0100</td>
<td>1000</td>
<td>10000</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deci</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>16</td>
<td>32</td>
<td>64</td>
<td>128</td>
<td>256</td>
<td>512</td>
<td>1024</td>
<td>2048</td>
<td>4096</td>
</tr>
<tr>
<td>Hex</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>10</td>
<td>20</td>
<td>40</td>
<td>80</td>
<td>100</td>
<td>200</td>
<td>400</td>
<td>800</td>
<td>1000</td>
</tr>
</tbody></table>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">printf</span> <span class="string">'%x\n'</span> <span class="string">"<span class="variable">$((2#10101010101)</span>)"</span></span><br><span class="line">555 <span class="comment">#numa_node0</span></span><br><span class="line"></span><br><span class="line">$ awk -F: <span class="string">'$0~/enp5/ || $0~/mlx4/ &#123;print $1&#125;'</span> /proc/interrupts | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> 555 &gt; /proc/irq/<span class="variable">$line</span>/smp_affinity</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment">#For example, to handle interrupts with CPUs 0, 1, 2, and 3, set the value of rps_cpus to f, which is the hexadecimal value for 15. In binary representation, 15 is 00001111 (1+2+4+8).</span></span><br><span class="line"></span><br><span class="line">or you could</span><br><span class="line">/sys/class/net</span><br><span class="line">eno1 -&gt; ../../devices/pci0000:17/0000:17:02.0/0000:18:00.0/net/eno1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0</span><br><span class="line">$ ls /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0/msi_irqs</span><br><span class="line">56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85</span><br><span class="line">$ ls -l /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0/msi_irqs</span><br><span class="line"></span><br><span class="line"><span class="comment">#intel</span></span><br><span class="line">$ set_irq_affinity -x all ethX</span><br><span class="line">$ set_irq_affinity -x <span class="built_in">local</span> ethX</span><br><span class="line">$ set_irq_affinity 1-2 ethX</span><br><span class="line"></span><br><span class="line"><span class="comment">#mellanox</span></span><br><span class="line">set_irq_affinity_bynode.sh 0 p7p1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">printf</span> %0.2x<span class="string">'\n'</span> 1024</span><br><span class="line">400</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">printf</span> %0.2x<span class="string">'\n'</span> 4096</span><br><span class="line">1000</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 400 &gt; /proc/irq/66/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 400 &gt; /proc/irq/67/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 1000 &gt; /proc/irq/69/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 1000 &gt; /proc/irq/70/smp_affinity</span><br></pre></td></tr></table></figure>
<p>When configuring RSS, Red Hat recommends <code>limiting the number of queues to one per physical CPU core</code><br>Hyper-threads are often represented as separate cores in analysis tools, but configuring queues for all cores including logical cores such as <code>hyper-threads has not proven beneficial to network performance</code></p>
<p>When enabled, RSS distributes network processing equally between available CPUs based on the amount of processing each CPU has queued. However, you can use the ethtool –show-rxfh-indir and –set-rxfh-indir parameters to modify how network activity is distributed, and weight certain types of network activity as more important than others.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool --<span class="built_in">set</span>-rxfh-indir enp1s0f0 equal 4</span><br><span class="line">$ watch -d  -n 1 <span class="string">"cat /proc/interrupts | grep enp1s0f0"</span></span><br><span class="line"><span class="comment"># You could see only 4 interrupt number increase a lot</span></span><br><span class="line"></span><br><span class="line">$ ethtool --show-rxfh-indir enp1s0f0</span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> enp1s0f0 with 12 RX ring(s):</span><br><span class="line">    0:      0     1     2     3     0     1     2     3</span><br><span class="line">    8:      0     1     2     3     0     1     2     3</span><br><span class="line">   16:      0     1     2     3     0     1     2     3</span><br><span class="line">   24:      0     1     2     3     0     1     2     3</span><br><span class="line">   32:      0     1     2     3     0     1     2     3</span><br><span class="line">   40:      0     1     2     3     0     1     2     3</span><br><span class="line">   48:      0     1     2     3     0     1     2     3</span><br><span class="line">   56:      0     1     2     3     0     1     2     3</span><br><span class="line">   64:      0     1     2     3     0     1     2     3</span><br><span class="line">   72:      0     1     2     3     0     1     2     3</span><br><span class="line">   80:      0     1     2     3     0     1     2     3</span><br><span class="line">   88:      0     1     2     3     0     1     2     3</span><br><span class="line">   96:      0     1     2     3     0     1     2     3</span><br><span class="line">  104:      0     1     2     3     0     1     2     3</span><br><span class="line">  112:      0     1     2     3     0     1     2     3</span><br><span class="line">  120:      0     1     2     3     0     1     2     3</span><br><span class="line">RSS <span class="built_in">hash</span> key:</span><br><span class="line">04:86:d4:1c:b4:14:3d:48:7b:48:29:2d:cc:c1:2c:67:eb:66:b0:c9:98:89:30:24:cb:ff:59:2a:54:13:5a:d6:1d:70:71:09:a4:fa:fd:89</span><br><span class="line">RSS <span class="built_in">hash</span> <span class="keyword">function</span>:</span><br><span class="line">    toeplitz: on</span><br><span class="line">    xor: off</span><br><span class="line">    crc32: off</span><br><span class="line"></span><br><span class="line"><span class="comment"># set weight for the queue</span></span><br><span class="line">$ ethtool --<span class="built_in">set</span>-rxfh-indir eth3 weight 6 2</span><br><span class="line">$ ethtool --<span class="built_in">set</span>-rxfh-indir eth3 weight 1 2</span><br></pre></td></tr></table></figure>

<p><code>The irqbalance daemon can be used in conjunction with RSS to reduce the likelihood of cross-node memory transfers and cache line bouncing. This lowers the latency of processing network packets.</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl <span class="built_in">enable</span> irqbalance</span><br><span class="line">$ systemctl start irqbalance</span><br><span class="line"><span class="comment"># debug irqblaance</span></span><br><span class="line">$ irqbalance -d -f</span><br></pre></td></tr></table></figure>

<h5 id="Support-for-UDP-RSS"><a href="#Support-for-UDP-RSS" class="headerlink" title="Support for UDP RSS"></a><a href="https://downloadmirror.intel.com/22919/eng/README.txt" target="_blank" rel="noopener">Support for UDP RSS</a></h5><p>   rx-flow-hash tcp4|udp4|ah4|esp4|sctp4|tcp6|udp6|ah6|esp6|sctp6<br>     Retrieves the hash options for the specified network traffic type.</p>
<p>  -N –config-nfc<br>     Configures the receive network flow classification.</p>
<p>   rx-flow-hash tcp4|udp4|ah4|esp4|sctp4|tcp6|udp6|ah6|esp6|sctp6<br>   m|v|t|s|d|f|n|r…<br>     Configures the hash options for the specified network traffic type.</p>
<pre><code>udp4    UDP over IPv4
udp6    UDP over IPv6

f   Hash on bytes 0 and 1 of the Layer 4 header of the rx packet.
n   Hash on bytes 2 and 3 of the Layer 4 header of the rx packet.</code></pre><p>The following is an example using udp4 (UDP over IPv4):</p>
<p>  To include UDP port numbers in RSS hashing run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -N ethX rx-flow-hash udp4 sdfn</span><br></pre></td></tr></table></figure>

<p>  To exclude UDP port numbers from RSS hashing run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -N ethX rx-flow-hash udp4 sd</span><br></pre></td></tr></table></figure>

<p>  To display UDP hashing current configuration run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ethtool -n ethX rx-flow-hash udp4</span><br></pre></td></tr></table></figure>

<p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html-single/performance_tuning_guide/index" target="_blank" rel="noopener">redhat performance tuning guide</a><br><a href="https://fasterdata.es.net/host-tuning/100g-tuning/" target="_blank" rel="noopener">100GbE tuning</a><br><a href="https://www.intel.com/content/dam/www/public/us/en/documents/reference-guides/xl710-x710-performance-tuning-linux-guide.pdf" target="_blank" rel="noopener">xl710-x710-performance-tuning-linux-guide</a></p>
<h3 id="Receive-Flow-Streering-RFS"><a href="#Receive-Flow-Streering-RFS" class="headerlink" title="Receive Flow Streering (RFS)"></a>Receive Flow Streering (RFS)</h3><p>RFS is disabled by default. To enable RFS, you must edit two files:<br>/proc/sys/net/core/rps_sock_flow_entries</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 32768 &gt; /proc/sys/net/core/rps_sock_flow_entries</span><br><span class="line">$ <span class="keyword">for</span> f <span class="keyword">in</span> /sys/class/net/ens6/queues/rx-*/rps_flow_cnt; <span class="keyword">do</span> <span class="built_in">echo</span> 4096 &gt; <span class="variable">$f</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>Set the value of this file to the maximum expected number of concurrently active connections. We recommend a value of 32768 for moderate server loads. All values entered are rounded up to the nearest power of 2 in practice.</p>
<p>Set the value of this file to the value of rps_sock_flow_entries divided by N, where N is the number of receive queues on a device. For example, if rps_flow_entries is set to 32768 and there are 16 configured receive queues, rps_flow_cnt should be set to 2048(I improve it to 4096). For single-queue devices, the value of rps_flow_cnt is the same as the value of rps_sock_flow_entries.</p>
<p>Data received from a single sender is not sent to more than one CPU. If the amount of data received from a single sender is greater than a single CPU can handle, configure a larger frame size to reduce the number of interrupts and therefore the amount of processing work for the CPU.</p>
<p>About large frame </p>
<ul>
<li>large mtu</li>
<li>TCP Segmentation Offload, merge to large than mtu size</li>
</ul>
<p>Consider using numactl or taskset in conjunction with RFS to pin applications to specific cores, sockets, or NUMA nodes. This can help prevent packets from being processed out of order.</p>
<h5 id="Accelerated-RFS"><a href="#Accelerated-RFS" class="headerlink" title="Accelerated RFS"></a>Accelerated RFS</h5><p>RFS and accelerated RFS (aRFS) are kernel features currently available in most distributions. The aRFS feature requires explicit configuration in order to enable it.<br>it need the driver support</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_RFS_ACCEL=y</span><br><span class="line">CONFIG_MLX5_EN_ARFS=y</span><br></pre></td></tr></table></figure>

<p>Accelerated RFS boosts the speed of RFS by adding hardware assistance. Like RFS, packets are forwarded based on the location of the application consuming the packet. Unlike traditional RFS, however, packets are sent directly to a CPU that is local to the thread consuming the data: either the CPU that is executing the application, or a CPU local to that CPU in the cache hierarchy.<br>Accelerated RFS is only available if the following conditions are met:<br>Accelerated RFS must be supported by the network interface card. Accelerated RFS is supported by cards that export the ndo_rx_flow_steer() netdevice function.<br>ntuple filtering must be enabled.</p>
<p>CPU to queue mapping is deduced based on the IRQ affinities configured by the driver for each receive queue.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default ATR （automated Application Targeting Routing）</span></span><br><span class="line">$ ethtool -K ens6 ntuple on</span><br><span class="line">$ ethtool -k ens6</span><br><span class="line">ntuple-filters: on</span><br><span class="line">$ systemctl stop irqbalance</span><br><span class="line"><span class="comment">#Configure your IRQ settings to ensure each RX queue is handled by one of your desired network processing CPUs.</span></span><br><span class="line"><span class="comment"># if you are mellanox NIC</span></span><br><span class="line"></span><br><span class="line">$ show_irq_affinity.sh ens6</span><br><span class="line"></span><br><span class="line"><span class="comment">### Custom policy, EP(Externally Programed) mode</span></span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type ip4 src-ip 192.168.100.1  action -1</span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type tcp4 src-port 80  action 2</span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type udp4 src-port 80  action 2</span><br></pre></td></tr></table></figure>

<p><a href="https://community.mellanox.com/s/article/howto-configure-arfs-on-connectx-4" target="_blank" rel="noopener">monitor</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -S eno1 | egrep rx.*pack</span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">$ ethtool -K ens6 ntuple off</span><br><span class="line"></span><br><span class="line">$ taskset -c 5 netserver &amp;</span><br><span class="line">$ netperf -H <span class="variable">$ipaddr</span> -l 200 -t TCP_STREAM &amp;</span><br><span class="line"></span><br><span class="line">$ ethtool -S ens6 | egrep rx.*pack</span><br><span class="line"><span class="comment"># only rx8 improved</span></span><br><span class="line">rx7_packets: 0</span><br><span class="line">rx7_lro_packets: 0</span><br><span class="line">rx8_packets: 6296748</span><br><span class="line">rx8_lro_packets: 0</span><br><span class="line">rx9_packets: 0</span><br><span class="line"></span><br><span class="line">$ ethtool -K ens6 ntuple on</span><br><span class="line">$ taskset -c 5 netserver &amp;</span><br><span class="line">$ netperf -H 11.134.201.5 -l 200 -t TCP_STREAM &amp;</span><br><span class="line">$ ethtool -S ens6 | egrep rx.*pack</span><br><span class="line">rx5_packets: 234532 <span class="comment"># only rx5 increase, it ‘s worked</span></span><br><span class="line">rx5_lro_packets: 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># mellanox enable the driver arfs</span></span><br><span class="line">$ enable_arfs.sh ens6</span><br></pre></td></tr></table></figure>

<h3 id="Interrupt-Moderation-interrupt-coalescence-or-Interrupt-Blanking"><a href="#Interrupt-Moderation-interrupt-coalescence-or-Interrupt-Blanking" class="headerlink" title="Interrupt Moderation (interrupt coalescence or Interrupt Blanking)"></a>Interrupt Moderation (interrupt coalescence or Interrupt Blanking)</h3><p>parm:           InterruptThrottleRate:Maximum interrupts per second, per vector, (0,1,956-488281), default 1 (array of int) </p>
<p>0 = Setting InterruptThrottleRate to 0 turns off any interrupt moderation and may improve small packet latency, but is generally not suitable for bulk throughput traffic due to the increased cpu utilization of the higher interrupt rate. Please note that on 82599-based adapters, disabling InterruptThrottleRate will also result in the driver disabling HW RSC(receive side coalescing).</p>
<p>HW RSC looks like TCP Segmentation offload or GSO, merge 1500~9000 mtu to a large frame(65536 or more), you could get it by tcpdump</p>
<p>On 82598-based adapters, disabling InterruptThrottleRate will also result in disabling LRO (Large Receive Offloads).</p>
<p>1 = Dynamic mode attempts to moderate interrupts per vector while maintaining very low latency. <code>This can sometimes cause extra CPU utilization.</code><br>If planning on deploying ixgbe in a latency sensitive environment please consider this parameter.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe ixgbe InterruptThrottleRate&#x3D;8000,4000</span><br></pre></td></tr></table></figure>
<p>956-488281, Interrupt Throttle Rate (interrupts/sec). The ITR parameter controls how many interrupts each interrupt vector can generate per second. On MQ/RSS enabled kernels with MSI-X interrupts this means that each RX vector can generate 8000 interrupts per second and each TX vector can generate 4000 interrupts per second. Increasing ITR lowers latency at the cost of increased CPU utilization, though it may help throughput in some circumstances.</p>
<p><code>Note: Adaptive moderation must be disabled in order to ensure that static values are in use.</code><br>It is possible to change the values for ethtool as well:</p>
<p>Mellanox example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -C eth4 rx-usecs 0 rx-frames 10 tx-usecs 16 tx-frames 100</span><br><span class="line">$ ethtool -c eth4</span><br></pre></td></tr></table></figure>
<p>if you need to improve value of rx-usecs, improve latency and througput, and suggestion improve </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#must tso could be enabled</span><br><span class="line">$ ethtool -K eth2 tso on </span><br><span class="line">$ net.ipv4.tcp_tso_win_divisor &#x3D; 30 #default 3</span><br></pre></td></tr></table></figure>
<p>This allows control over what percentage of the congestion window can be consumed by a single TSO frame. The setting of this parameter is a choice between burstiness and building larger TSO frames.</p>
<p>rx-frames[-irq] rx-usecs[-irq] tx-frames[-irq] tx-usecs[-irq]<br>The range of 0-235 microseconds provides an effective range of 4,310 to 250,000 interrupts per second. The value of rx-µsecs-high can be set independent of rx-µsecs and tx-µsecs in the same ethtool command, and is also independent of the adaptive interrupt moderation algorithm. The underlying hardware supports granularity in 2-microsecond intervals, so adjacent values might result in the same interrupt rate.</p>
<p>DIM is enabled by default. In case you wish to disable it. in some spcical case, you will modify these interrupt options<br><a href="https://community.mellanox.com/s/article/dynamically-tuned-interrupt-moderation--dim-x" target="_blank" rel="noopener">not recommended disable Dynamically Interrupt Moderation</a></p>
<ul>
<li><a href="https://www.ibm.com/support/knowledgecenter/en/SSQPD3_2.6.0/com.ibm.wllm.doc/usingethtoolrates.html" target="_blank" rel="noopener">adaptive-rx Dynamic control to decrease RX latency at low packet rates and increase throughput at high packet rates</a></li>
<li>rx-usecs This is the number of microseconds to wait before raising an RX interrupt after a packet has been received. When rx-usecs is set to 0 rx-frames is used</li>
<li>rx-frames     This is the number of frames to queue up before raising an RX interrupt.</li>
<li>adaptive-tx Dynamic control to decrease TX latency at low packet rates and increase throughput at high packet rates</li>
<li>tx-usecs This is the number of microseconds to wait before raising an TX interrupt after a packet has been sent. When tx-usecs is set to 0 tx-frames is used</li>
<li>tx-frames This is the number of frames to queue up before raising an TX interrupt</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#To turn on adaptive interrupt moderation, recommand</span></span><br><span class="line">$ ethtool -C ethX adaptive-rx on adaptive-tx on</span><br><span class="line"></span><br><span class="line"><span class="comment">#To turn off adaptive interrupt moderation</span></span><br><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off</span><br></pre></td></tr></table></figure>

<p>A good place to start for general tuning is 84 µs, or ~12000 interrupts/s. If you see rx_dropped counters are running during traffic (using ethtool -S ethX) then you probably have too slow of a CPU, not enough buffers from the adapter’s ring size (ethtool -G) to hold packets for 84 µs or to low of an interrupt rate.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off rx-usecs 84 tx-usecs 84</span><br></pre></td></tr></table></figure>

<p>The next value to try, if you are not maxed out on CPU utilization, is 62 µs. This uses more CPU, but it services buffers faster, and requires fewer descriptors (ring size, ethtool -G). </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off rx-usecs 62 tx-usecs 62</span><br></pre></td></tr></table></figure>
<p>If your CPU is at 100%, then increasing the interrupt rate is not advised. In certain circumstances such as a CPU bound workload, you might want to increase the µs value to enable more CPU time for other applications.</p>
<p>If you require low latency performance and/or have plenty of CPU to devote to network processing, you can disable interrupt moderation entirely, which enables the interrupts to fire as fast as possible.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off rx-usecs 0 tx-usecs 0</span><br></pre></td></tr></table></figure>
<p>Totaly, tx,rx-usecs value the lower means the smaller frame to interrupt moderation. the more cpu overhead and the lower latency<br>increase tx,tx-usecs that means high latency and get the biger frame to interrupt moderation, the lower cpu overhead and high latency and high througput, it will impact tcp performance, so increase tcp_tso_win_divisor to 30</p>
<p>If cache pressure is suspected (many queues active) reducing buffers from default can help Intel® Data Direct I/O (Intel® DDIO) operate with better efficiently. Intel recommends trying 128 or 256 per queue, being aware that an increase in interrupt rate via ethtool -C might be necessary to avoid an increase in rx_dropped.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -G eth12 rx 256 tx 256</span><br></pre></td></tr></table></figure>

<p>Change the Tx and Rx ring sizes as needed, a larger value takes more resources but can provide better forwarding rates:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -G ethX rx 4096 tx 4096</span><br></pre></td></tr></table></figure>

<p>Layer 2 flow control can impact TCP performance considerably and is recommended to be disabled for most workloads</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -A ethX rx off tx off</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -c enp4s0d1</span><br><span class="line">Coalesce parameters <span class="keyword">for</span> enp4s0d1:</span><br><span class="line">Adaptive RX: off  TX: off</span><br><span class="line">stats-block-usecs: 0</span><br><span class="line">sample-interval: 0</span><br><span class="line">pkt-rate-low: 400000</span><br><span class="line">pkt-rate-high: 450000</span><br><span class="line"></span><br><span class="line">rx-usecs: 0</span><br><span class="line">rx-frames: 0</span><br><span class="line">rx-usecs-irq: 0</span><br><span class="line">rx-frames-irq: 0</span><br><span class="line"></span><br><span class="line">tx-usecs: 8</span><br><span class="line">tx-frames: 16</span><br><span class="line">tx-usecs-irq: 0</span><br><span class="line">tx-frames-irq: 256</span><br><span class="line"></span><br><span class="line">rx-usecs-low: 0</span><br><span class="line">rx-frame-low: 0</span><br><span class="line">tx-usecs-low: 0</span><br><span class="line">tx-frame-low: 0</span><br><span class="line"></span><br><span class="line">rx-usecs-high: 128</span><br><span class="line">rx-frame-high: 0</span><br><span class="line">tx-usecs-high: 0</span><br><span class="line">tx-frame-high: 0</span><br><span class="line"></span><br><span class="line">$ ethtool -i p5p2</span><br><span class="line">driver: i40e</span><br><span class="line">version: 2.4.6</span><br><span class="line">firmware-version: 6.01 0x80003554 1.1747.0</span><br><span class="line">bus-info: 0000:05:00.1</span><br><span class="line">supports-statistics: yes</span><br><span class="line">supports-test: yes</span><br><span class="line">supports-eeprom-access: yes</span><br><span class="line">supports-register-dump: yes</span><br><span class="line">supports-priv-flags: yes</span><br><span class="line"></span><br><span class="line">$ ethtool -l p5p2</span><br><span class="line">Channel parameters <span class="keyword">for</span> p5p2:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       64</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       8</span><br><span class="line"></span><br><span class="line">$ ethtool -g p5p2</span><br><span class="line">Ring parameters <span class="keyword">for</span> p5p2:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:             4096</span><br><span class="line">RX Mini:        0</span><br><span class="line">RX Jumbo:       0</span><br><span class="line">TX:             4096</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             512</span><br><span class="line">RX Mini:        0</span><br><span class="line">RX Jumbo:       0</span><br><span class="line">TX:             512</span><br></pre></td></tr></table></figure>

<p><a href="https://download.01.org/packet-processing/ONPS2.1/Intel_ONP_Release_2.1_Performance_Test_Report_Rev1.0.pdf" target="_blank" rel="noopener">Intel_ONP_Release_2.1_performance_test_report</a></p>
<h3 id="disable-PCIE-power-save"><a href="#disable-PCIE-power-save" class="headerlink" title="disable PCIE power save"></a>disable PCIE power save</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ grubby --update-kernel=ALL --args=<span class="string">'pcie_aspm=off'</span> --args=<span class="string">'intel_idle.max_cstate=0'</span> --args=<span class="string">'processor.max_cstate=1'</span></span><br></pre></td></tr></table></figure>

<h4 id="disable-numa"><a href="#disable-numa" class="headerlink" title="disable numa"></a>disable numa</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo kernel.numa_balancing&#x3D;0 &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">$ sysctl -p</span><br></pre></td></tr></table></figure>

<h3 id="RFC2544-stipulates-that-the-latency-test"><a href="#RFC2544-stipulates-that-the-latency-test" class="headerlink" title="RFC2544 stipulates that the latency test"></a>RFC2544 stipulates that the latency test</h3><ul>
<li>Should be at least 120 seconds in duration</li>
<li>Frame sizes to be used on Ethernet 64, 128, 256, 512, 1024, 1280, and 1518</li>
<li>Should include an identifying tag in one frame after 60 seconds with the type of tag being implementation dependent</li>
<li>Records the time at which the frame is fully transmitted (timestamp A)</li>
<li>The receiver logic in the test equipment must recognize the tag information in the frame stream and record the time at which the tagged frame was received (timestamp B)</li>
<li>This test should be performed with the test frame addressed to the same destination as the rest of the data stream, and also with each of the test frames addressed to a new destination network</li>
<li>The test must be repeated at least 20 times with the reported value being the average of the recorded values</li>
<li>The latency is timestamp B minus timestamp A, as per the relevant definition from RFC</li>
</ul>
<h3 id="The-network-latency"><a href="#The-network-latency" class="headerlink" title="The network latency"></a><a href="https://www.marvell.com/documents/rjx203ukari4r93gntem/" target="_blank" rel="noopener">The network latency</a></h3><p>There are new technologies that are pushing latencies into the singledigit microsecond range when measured back-to-back in benchmark<br>repeat 20 times and get the average result</p>
<p>50 – 125μs   1Gb Ethernet (TCP/IP)</p>
<ul>
<li>Multi-tasking: multiple highbandwidth applications running<br>simultaneously</li>
<li>Bulk data transfer</li>
<li>Transactional database backup and<br>applications</li>
<li>Web (front-end for data centers)</li>
</ul>
<p>5 – 50μs 10Gb Ethernet<br>(TCP/IP)</p>
<ul>
<li>Bulk data transfer</li>
<li>Real-time video streaming</li>
<li>Database backup and applications</li>
</ul>
<p>3 – 5μs RDMA, RoCEE, and<br>iWARP</p>
<ul>
<li>High Performance Computing</li>
<li>High-Frequency Trading (HFT)</li>
<li>Inter-process communication (IPC)<br>cluster</li>
<li>Low-latency applications</li>
</ul>
<p>Sub-3μs(less than 3μs) InfiniBand (QDR)<br>and proprietary</p>
<ul>
<li>High Performance Computing</li>
<li>High Frequency Trading (HFT)</li>
<li>Ultra-low latency applications</li>
</ul>
<h3 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h3><h4 id="nc-test"><a href="#nc-test" class="headerlink" title="nc test"></a>nc test</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Server $ nc -l 22222 &lt; /dev/zero</span><br><span class="line">Client $ nc <span class="variable">$serverip</span> 22222 &gt; /dev/null</span><br><span class="line">Client $ nc <span class="variable">$serverip</span> 22222 | pv  <span class="comment"># show the speed</span></span><br></pre></td></tr></table></figure>

<h4 id="iperf3"><a href="#iperf3" class="headerlink" title="iperf3"></a>iperf3</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server $ iperf3 -s -D -p 520<span class="variable">$i</span></span><br><span class="line">Client $ iperf3 -c <span class="variable">$client_ip</span> -P 1 -t 360000 -p 520<span class="variable">$i</span></span><br></pre></td></tr></table></figure>

<h4 id="qperf"><a href="#qperf" class="headerlink" title="qperf"></a>qperf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Server $ qperf -lp <span class="variable">$port</span> &amp;</span><br><span class="line">Client $ qperf -lp <span class="variable">$port</span> -t 36000 -oo msg_size:1K:4K:*2 <span class="variable">$server_ip</span> tcp_bw </span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">Client $ qperf -lp <span class="variable">$port</span> -t 60 -oo msg_size:64:1024K:*4 <span class="variable">$server_ip</span> tcp_lat</span><br></pre></td></tr></table></figure>

<h4 id="netperf"><a href="#netperf" class="headerlink" title="netperf"></a>netperf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Server $ taskset -c 5 netserver &amp;</span><br><span class="line">Client $ netperf -H <span class="variable">$ipaddr</span> -l 200 -t TCP_STREAM </span><br><span class="line"></span><br><span class="line">Server $ numactl -C 2 netserver -D  -4 -v 2 -p 5000 -f -L 192.168.12.201</span><br><span class="line">Client $ numactl -N 0 ./netperf -n 6 -p 5000 -H 192.168.12.201 -c -C -t TCP_STREAM -l 15 -T 2,2 -- -m $((2**<span class="variable">$i</span>))</span><br><span class="line">Client $ numactl -N 0 ./netperf -n 6 -p 5000 -H 192.168.12.201 -c -C -t TCP_RR -l 20 -T 2,2 -- -m $((2**<span class="variable">$i</span>))</span><br><span class="line">Client $ numactl -N 0 ./netperf -n 6 -p 5000 -H 192.168.12.201 -c -C -t UDP_RR -l 20 -T 2,2 -- -m $((2**<span class="variable">$i</span>))</span><br></pre></td></tr></table></figure>

<h4 id="sockperf"><a href="#sockperf" class="headerlink" title="sockperf"></a>sockperf</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;sockperf server --tcp -p 11111</span><br><span class="line">.&#x2F;sockperf server -p 11111 #udp</span><br><span class="line">numactl -C 2 &#x2F;opt&#x2F;sockperf&#x2F;bin&#x2F;sockperf tp -m 256  -i 192.168.12.201 -p 11111 -t 20</span><br><span class="line">numactl -C 2 &#x2F;opt&#x2F;sockperf&#x2F;bin&#x2F;sockperf pp -m 256  -i 192.168.12.201 -p 11111 -t 20</span><br></pre></td></tr></table></figure>

<h4 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Server $ LD_PRELOAD=libvma.so VMA_STATS_FD_NUM=500 redis-server --port 7777  --protected-mode no --maxmemory 12000mb --maxmemory-samples 10 --maxmemory-policy allkeys-lru</span><br><span class="line">Client $ numactl -C 2 redis-benchmark -r 10000000 -n 20000000 -t get,<span class="built_in">set</span>,lpush,lpop -P 16 -q -h 192.168.12.201 -p 7777 -d 16</span><br></pre></td></tr></table></figure>

<h3 id="RFC6349-TCP-benchmark"><a href="#RFC6349-TCP-benchmark" class="headerlink" title="RFC6349 TCP benchmark"></a><a href="https://www.viavisolutions.com/zh-cn/literature/rfc-6349-testing-truespeed-viavi-solutions-experience-your-network-your-custom-application-notes-zh.pdf" target="_blank" rel="noopener">RFC6349 TCP benchmark</a></h3><ul>
<li><p>RFC 4821 through PLPMTUD make sure network path mtu</p>
</li>
<li><p>benchmark RRT(Round-Trip Time) and BB(bottle neck bandwidth)</p>
<ul>
<li><p>example:RRT=25ms, bandwidth=45Mbps, window=64KB, receiver take 12.5ms to sender<br>BDP(bandwidth-delay product) = BB * RRT /8 = 140.625 KB, double sender ‘s window size</p>
</li>
<li><p>TCP global synchronization</p>
</li>
<li><p>RWND(Receive Window)</p>
</li>
<li><p>CWND(Congestion Window)</p>
</li>
<li><p>LFN(Long fat network)</p>
</li>
<li><p>RTD(round-trip delay time)</p>
</li>
<li><p>TDP(Tail drop polocy)</p>
</li>
<li><p>tcp transfer time</p>
<ul>
<li>In the 500Mbps netowrk, start 5x transmit services, each transmit means 100Mbps, actually, the 5 x tansmit bandwith are not balancing. 5 x sender transfer 100MB file.<ul>
<li>500MB/(500Mbps/8)=8 sec, in fact, the test use 12sec, that means 12sec/8sec=1.5, 1.5x late than the theoretically</li>
</ul>
</li>
</ul>
</li>
<li><p>tcp efficiency</p>
<ul>
<li>(total bytes - retrans bytes)/total bytes * 100 = 98%, in our data center , there are 2 percent tcp retrans. that too bad in some times.</li>
</ul>
</li>
<li><p>tcp cache latency percent, (test average rrt - base rrt)/base rrt * 100, if base rrt= 25ms, in the test, the average rrt=32ms, (32-25)/25*100=28%, tcp RTD increase 28%(congestion)</p>
</li>
</ul>
</li>
<li><p>if the tcp performance not reach your expect</p>
<ul>
<li>re-gen tcp connection, and modify tcp rwnd size, mtu size</li>
<li>speed limit cause tail drop polocy and cause too many tcp retrans</li>
<li>the maximum tcp cache size, it limit by OS memory size, and to check tcp queue</li>
<li>socket buffer size, a lot of OS could modify the each connection receive and transmit buffer limit. the socket buffer must be large enough.</li>
</ul>
</li>
</ul>
<h4 id="irqbalance"><a href="#irqbalance" class="headerlink" title="irqbalance"></a>irqbalance</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--powerthresh </span><br><span class="line">--hintpolicy </span><br><span class="line">--policyscript </span><br><span class="line">--banirq</span><br><span class="line">--ban </span><br><span class="line">--balance_level </span><br><span class="line">--numa_node</span><br></pre></td></tr></table></figure>

<h4 id="turbostat"><a href="#turbostat" class="headerlink" title="turbostat"></a>turbostat</h4><h4 id="numastat"><a href="#numastat" class="headerlink" title="numastat"></a>numastat</h4><h4 id="numad"><a href="#numad" class="headerlink" title="numad"></a>numad</h4><h4 id="OProfile"><a href="#OProfile" class="headerlink" title="OProfile"></a>OProfile</h4><h4 id="valgrind"><a href="#valgrind" class="headerlink" title="valgrind"></a>valgrind</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">valgrind --tool&#x3D;memcheck --leak-check&#x3D;yes --show-reachable&#x3D;yes --num-callers&#x3D;20 --track-fds&#x3D;yes .&#x2F;lustre_exporter</span><br></pre></td></tr></table></figure>

<h4 id="intel-cmt-cat"><a href="#intel-cmt-cat" class="headerlink" title="intel-cmt-cat"></a>intel-cmt-cat</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ yum install intel-cmt-cat.x86_64</span><br><span class="line"></span><br><span class="line">TIME 2020-01-07 17:54:05</span><br><span class="line">           LLC(Last Level Cache): CMT(Cache Monitoring Technology) Cache Occupancy </span><br><span class="line">           MBL:MBM local bandwidth</span><br><span class="line">           MBR:MBM Remote bandwidth                             </span><br><span class="line">$ pqos</span><br><span class="line">    CORE   IPC   MISSES     LLC[KB]   MBL[MB&#x2F;s]   MBR[MB&#x2F;s]</span><br><span class="line">                                            </span><br><span class="line">       0  0.26       6k      1080.0         0.2         0.0</span><br><span class="line">       1  0.26       0k         0.0         0.0         0.0</span><br><span class="line">       2  0.27       4k      4600.0         0.5         0.0</span><br><span class="line">       3  0.26       5k       520.0         0.0         0.2</span><br><span class="line">       4  0.27       8k      1120.0         0.7         0.1</span><br><span class="line">       5  0.25      12k      1080.0         0.7         0.2</span><br><span class="line">       6  0.28      18k      1040.0         0.9         0.1</span><br><span class="line">       7  0.25       0k         0.0         0.0         0.0</span><br><span class="line">       8  0.27       4k      3400.0         0.2         0.1</span><br><span class="line">       9  0.25       0k      1360.0         0.1         0.1</span><br><span class="line">      10  0.26      14k      7000.0         1.3         0.0</span><br><span class="line">      11  0.39       1k       120.0         0.0         0.0</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/intel/intel-cmt-cat/wiki/Usage-Examples" target="_blank" rel="noopener">Usage-example</a></p>
<p>Cache Monitoring Technology (CMT)<br>Cache Allocation Technology (CAT)<br>Memory Bandwidth Monitoring (MBM)<br>Memory Bandwidth Allocation (MBA)<br>Code and Data Prioritization (CDP)</p>
<h3 id="mellanox-nic-exapmle"><a href="#mellanox-nic-exapmle" class="headerlink" title="mellanox nic exapmle"></a>mellanox nic exapmle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -k  enp131s0f1</span><br><span class="line">Features for enp131s0f1:</span><br><span class="line">rx-checksumming: on</span><br><span class="line">tx-checksumming: on</span><br><span class="line">	tx-checksum-ipv4: on</span><br><span class="line">	tx-checksum-ip-generic: off [fixed]</span><br><span class="line">	tx-checksum-ipv6: on</span><br><span class="line">	tx-checksum-fcoe-crc: off [fixed]</span><br><span class="line">	tx-checksum-sctp: off [fixed]</span><br><span class="line">scatter-gather: on</span><br><span class="line">	tx-scatter-gather: on</span><br><span class="line">	tx-scatter-gather-fraglist: off [fixed]</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">	tx-tcp-segmentation: on</span><br><span class="line">	tx-tcp-ecn-segmentation: off [fixed]</span><br><span class="line">	tx-tcp6-segmentation: on</span><br><span class="line">	tx-tcp-mangleid-segmentation: off</span><br><span class="line">udp-fragmentation-offload: off [fixed]</span><br><span class="line">generic-segmentation-offload: on</span><br><span class="line">generic-receive-offload: on</span><br><span class="line">large-receive-offload: on</span><br><span class="line">rx-vlan-offload: on</span><br><span class="line">tx-vlan-offload: on</span><br><span class="line">ntuple-filters: off</span><br><span class="line">receive-hashing: on</span><br><span class="line">highdma: on [fixed]</span><br><span class="line">rx-vlan-filter: on</span><br><span class="line">vlan-challenged: off [fixed]</span><br><span class="line">tx-lockless: off [fixed]</span><br><span class="line">netns-local: off [fixed]</span><br><span class="line">tx-gso-robust: off [fixed]</span><br><span class="line">tx-fcoe-segmentation: off [fixed]</span><br><span class="line">tx-gre-segmentation: on</span><br><span class="line">tx-ipip-segmentation: off [fixed]</span><br><span class="line">tx-sit-segmentation: off [fixed]</span><br><span class="line">tx-udp_tnl-segmentation: on</span><br><span class="line">fcoe-mtu: off [fixed]</span><br><span class="line">tx-nocache-copy: off</span><br><span class="line">loopback: off [fixed]</span><br><span class="line">rx-fcs: off</span><br><span class="line">rx-all: off</span><br><span class="line">tx-vlan-stag-hw-insert: on</span><br><span class="line">rx-vlan-stag-hw-parse: off [fixed]</span><br><span class="line">rx-vlan-stag-filter: on [fixed]</span><br><span class="line">busy-poll: off [fixed]</span><br><span class="line">tx-gre-csum-segmentation: on</span><br><span class="line">tx-udp_tnl-csum-segmentation: on</span><br><span class="line">tx-gso-partial: on</span><br><span class="line">tx-sctp-segmentation: off [fixed]</span><br><span class="line">rx-gro-hw: off [fixed]</span><br><span class="line">l2-fwd-offload: off [fixed]</span><br><span class="line">hw-tc-offload: off</span><br><span class="line">rx-udp_tunnel-port-offload: on</span><br><span class="line">[root@cngb-oss-a23-1 ~]# ethtool -i enp131s0f1</span><br><span class="line">driver: mlx5_core</span><br><span class="line">version: 5.0-0</span><br><span class="line">firmware-version: 14.20.1010 (MT_2420110034)</span><br><span class="line">expansion-rom-version:</span><br><span class="line">bus-info: 0000:83:00.1</span><br><span class="line">supports-statistics: yes</span><br><span class="line">supports-test: yes</span><br><span class="line">supports-eeprom-access: no</span><br><span class="line">supports-register-dump: no</span><br><span class="line">supports-priv-flags: yes</span><br><span class="line"></span><br><span class="line">ethtool -g enp131s0f1</span><br><span class="line">Ring parameters for enp131s0f1:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:		8192</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		8192</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:		8192</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		8192</span><br><span class="line"></span><br><span class="line">ethtool -i enp131s0f1</span><br><span class="line">driver: mlx5_core</span><br><span class="line">version: 5.0-0</span><br><span class="line">firmware-version: 14.20.1010 (MT_2420110034)</span><br><span class="line">expansion-rom-version:</span><br><span class="line">bus-info: 0000:83:00.1</span><br><span class="line">supports-statistics: yes</span><br><span class="line">supports-test: yes</span><br><span class="line">supports-eeprom-access: no</span><br><span class="line">supports-register-dump: no</span><br><span class="line">supports-priv-flags: yes</span><br></pre></td></tr></table></figure>

<h3 id="ixgbe-example"><a href="#ixgbe-example" class="headerlink" title="ixgbe example"></a>ixgbe example</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -g enp1s0f0</span><br><span class="line">Ring parameters <span class="keyword">for</span> enp1s0f0:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:		4096</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		4096</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:		4096</span><br><span class="line">RX Mini:	0</span><br><span class="line">RX Jumbo:	0</span><br><span class="line">TX:		4096</span><br><span class="line"></span><br><span class="line">$ ethtool -i enp1s0f0</span><br><span class="line">driver: ixgbe</span><br><span class="line">version: 5.5.2</span><br><span class="line">firmware-version: 0x800006da, 255.65535.255</span><br><span class="line">expansion-rom-version:</span><br><span class="line">bus-info: 0000:01:00.0</span><br><span class="line">supports-statistics: yes</span><br><span class="line">supports-test: yes</span><br><span class="line">supports-eeprom-access: yes</span><br><span class="line">supports-register-dump: yes</span><br><span class="line">supports-priv-flags: yes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Features <span class="keyword">for</span> enp1s0f0:</span><br><span class="line">rx-checksumming: on</span><br><span class="line">tx-checksumming: on</span><br><span class="line">	tx-checksum-ipv4: off [fixed]</span><br><span class="line">	tx-checksum-ip-generic: on</span><br><span class="line">	tx-checksum-ipv6: off [fixed]</span><br><span class="line">	tx-checksum-fcoe-crc: on [fixed]</span><br><span class="line">	tx-checksum-sctp: on</span><br><span class="line">scatter-gather: on</span><br><span class="line">	tx-scatter-gather: on</span><br><span class="line">	tx-scatter-gather-fraglist: off [fixed]</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">	tx-tcp-segmentation: on</span><br><span class="line">	tx-tcp-ecn-segmentation: off [fixed]</span><br><span class="line">	tx-tcp6-segmentation: on</span><br><span class="line">	tx-tcp-mangleid-segmentation: off</span><br><span class="line">udp-fragmentation-offload: off [fixed]</span><br><span class="line">generic-segmentation-offload: on</span><br><span class="line">generic-receive-offload: on</span><br><span class="line">large-receive-offload: off    </span><br><span class="line">rx-vlan-offload: on</span><br><span class="line">tx-vlan-offload: on</span><br><span class="line">ntuple-filters: off</span><br><span class="line">receive-hashing: on</span><br><span class="line">highdma: on [fixed]</span><br><span class="line">rx-vlan-filter: on</span><br><span class="line">vlan-challenged: off [fixed]</span><br><span class="line">tx-lockless: off [fixed]</span><br><span class="line">netns-local: off [fixed]</span><br><span class="line">tx-gso-robust: off [fixed]</span><br><span class="line">tx-fcoe-segmentation: on [fixed]</span><br><span class="line">tx-gre-segmentation: on</span><br><span class="line">tx-ipip-segmentation: off [fixed]</span><br><span class="line">tx-sit-segmentation: off [fixed]</span><br><span class="line">tx-udp_tnl-segmentation: on</span><br><span class="line">fcoe-mtu: off [fixed]</span><br><span class="line">tx-nocache-copy: off</span><br><span class="line">loopback: off [fixed]</span><br><span class="line">rx-fcs: off [fixed]</span><br><span class="line">rx-all: off</span><br><span class="line">tx-vlan-stag-hw-insert: off [fixed]</span><br><span class="line">rx-vlan-stag-hw-parse: off [fixed]</span><br><span class="line">rx-vlan-stag-filter: off [fixed]</span><br><span class="line">busy-poll: on [fixed]</span><br><span class="line">tx-gre-csum-segmentation: on</span><br><span class="line">tx-udp_tnl-csum-segmentation: on</span><br><span class="line">tx-gso-partial: on</span><br><span class="line">tx-sctp-segmentation: off [fixed]</span><br><span class="line">rx-gro-hw: off [fixed]</span><br><span class="line">l2-fwd-offload: off [fixed]</span><br><span class="line">hw-tc-offload: off</span><br><span class="line">rx-udp_tunnel-port-offload: on</span><br></pre></td></tr></table></figure>

<h3 id="txqueueleng"><a href="#txqueueleng" class="headerlink" title="txqueueleng"></a>txqueueleng</h3><p><a href="https://www.nas.nasa.gov/assets/pdf/papers/NAS_Technical_Report_NAS-2014-01.pdf" target="_blank" rel="noopener">This queue parameter is mostly applicable for high-speed WAN transfers. For low-latency networks, the default setting of 1000 is sufficient. The receiving end is configured with the sysctl setting net.core.netdev_max_backlog. The default for this setting is also 1000 and does not need to be modified unless there is significant latency.</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip link <span class="built_in">set</span> dev enp5s0 txqueuelen 1000</span><br><span class="line">ip link <span class="built_in">set</span> dev enp5s0d1 txqueuelen 1000</span><br><span class="line">ip link <span class="built_in">set</span> dev bond0 txqueuelen 2000</span><br></pre></td></tr></table></figure>

<h4 id="Enable-jumbo-frames"><a href="#Enable-jumbo-frames" class="headerlink" title="Enable jumbo frames"></a><a href="http://ehealth-aussie.blogspot.com/2011/11/in-depth-look-into-path-mtu-discovery.html" target="_blank" rel="noopener">Enable jumbo frames</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_mtu_probing = 1</span><br><span class="line">net.ipv4.tcp_base_mss = 512</span><br><span class="line">net.ipv4.ip_no_pmtu_disc = 0</span><br></pre></td></tr></table></figure>

<h3 id="NIC-receive-packages"><a href="#NIC-receive-packages" class="headerlink" title="NIC receive packages"></a><a href="https://oxnz.github.io/2016/05/03/performance-tuning-networking/" target="_blank" rel="noopener">NIC receive packages</a></h3><p>packet -&gt; NIC -&gt; internal hardware buffer or ring buffer -&gt; hardware interrupt request -&gt; software interrupt operation -&gt;<br>from buffer to network stack -&gt; forwarded/discarded/rejected/passed to a socket receive queue for an application -&gt;<br>remove from network stack until no packets left in NIC buffer or a certain number of packets are transferred (/proc/sys/net/core/dev_weight)</p>
<h4 id="Bottle-neck"><a href="#Bottle-neck" class="headerlink" title="Bottle neck"></a>Bottle neck</h4><ul>
<li><p>The NIC hardware buffer or ring buffer</p>
</li>
<li><p>The hardware or software interrupt queues</p>
</li>
<li><p>The socket receive queue for the application</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eno1: flags&#x3D;6211&lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST&gt;  mtu 9000</span><br><span class="line">        ether e4:43:4b:08:41:e2  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 104934573253  bytes 236357913149742 (214.9 TiB)</span><br><span class="line">        RX errors 0  dropped 228  overruns 0  frame 0</span><br><span class="line">        TX packets 311616323933  bytes 1529707025808932 (1.3 PiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure>
</li>
<li><p>RX</p>
<pre><code>* frame/length counts only misaligned frames, it means frames with a length not divisible by 8. Because of that length is not a valid frame and it is simply discarded (too-short frames and too-long frames)

* overruns/fifo counts that times when there is fifo overruns, caused by the rate at which the buffer gets full and the kernel isn&apos;t able to reclaim the buffer
        * No cpu resource, interrupt not balance and not affinity, eg: all nic interrupt in core0

* [dropped(normal) counts](https://serverfault.com/questions/528290/ifconfig-eth0-rx-dropped-packets/601186)
        * Softnet backlog full
        * Bad / Unintended VLAN tags
        * Unknown / Unregistered protocols
        * IPv6 frames when the server is not configured for IPv6

* dropped (in my opinion)
        * Overloading
        * Hardware issue
                * NIC ring buffer too slower
                * NIC/optical module issue
                * PCIE issue
                * Bad cable</code></pre></li>
<li><p><a href="http://blog.hyfather.com/blog/2013/03/04/ifconfig/" target="_blank" rel="noopener">TX</a></p>
<pre><code>*  aborted transmission
*  errors due to carrirer
*  fifo err
*  heartbeat erros
*  window err
*  collisions is the number of transmissions terminated due to CSMA/CD (Carrier Sense Multiple Access with Collision Detection).</code></pre></li>
</ul>
<p>The number of collisions tells us how many packets (ethernet frames) ended up colliding (being put on the network medium at the same time as) other packets, the result being that each of them would have to be recalled and resent by their respective network interfaces shortly afterwards. A high collision rate would indicate that the network is very busy, possibly overloaded. The more collisions you have, the more packets have to be resent, the more traffic you have and so on.</p>
<h3 id="Linux-network-parameters"><a href="#Linux-network-parameters" class="headerlink" title="Linux network parameters"></a>Linux network parameters</h3><h4 id="TCP-control-algorithms"><a href="#TCP-control-algorithms" class="headerlink" title="TCP control algorithms"></a>TCP control algorithms</h4><ul>
<li>cubic</li>
<li>reno</li>
<li>htcp<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_congestion_control=htcp</span><br><span class="line">net.ipv4.tcp_low_latency=0</span><br></pre></td></tr></table></figure></li>
<li>bbr for the bad network env<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_low_latency=1</span><br><span class="line">net.ipv4.tcp_congestion_control=bbr</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="The-others-parameters"><a href="#The-others-parameters" class="headerlink" title="The others parameters"></a>The others parameters</h4><h5 id="tcp-frto"><a href="#tcp-frto" class="headerlink" title="tcp_frto"></a>tcp_frto</h5><p>tcp_frto (integer; default: 0; since Linux 2.4.21/2.6)<br>Enable  F-RTO,  an enhanced recovery algorithm for TCP retrans‐mission timeouts (RTOs).   It  is  particularly  beneficial  in wireless  environments  where  packet  loss is typically due to random radio interference rather than intermediate router  con‐gestion.  See RFC 4138 for more details.<br>              This file can have one of the following values:<br>              0  Disabled.<br>              1  The basic version F-RTO algorithm is enabled.<br>              2  Enable  SACK-enhanced  F-RTO  if  flow uses SACK.  The basic version can be used also when SACK is in use though in  that case scenario(s) exists where F-RTO interacts badly with the packet counting of the SACK-enabled TCP flow.</p>
<p>In some of a lot of tcp-retrans in our LAN, but I have not permission to what happend in network switch, I ‘m just do my best to avoid it in linux kernel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_frto&#x3D;1</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">net.ipv4.tcp_sack&#x3D;1</span><br><span class="line">net.ipv4.tcp_frto&#x3D;2</span><br></pre></td></tr></table></figure>

<h5 id="tcp-window-scaling"><a href="#tcp-window-scaling" class="headerlink" title="tcp_window_scaling"></a>tcp_window_scaling</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#default</span></span><br><span class="line">net.ipv4.tcp_window_scaling = 1</span><br></pre></td></tr></table></figure>
<p>tcp_window_scaling - support for large TCP Windows (RFC 1323).<br>The options info in SYN or SYN/ACK packages<br>Needs to be set to 1 if the Max TCP Window is over 65535</p>
<h5 id="tcp-win-scale"><a href="#tcp-win-scale" class="headerlink" title="tcp_win_scale"></a>tcp_win_scale</h5><p>The following variable is used to tell the kernel how much of the socket buffer space should be used for TCP window size, and how much to save for an application buffer<br>A value of 1 means the socket buffer will be divided evenly between TCP windows size and application</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_adv_win_scale=1</span><br></pre></td></tr></table></figure>

<h5 id="tcp-timestamps"><a href="#tcp-timestamps" class="headerlink" title="tcp_timestamps"></a>tcp_timestamps</h5><p>add additional 10 bytes to each packet but more accurate timestamp make TCP congestion control algorithms work better and are recommonded for fast networks</p>
<p>The following changes are recommended for improving IPv4 traffic performance, Disable the TCP timestamps option for better CPU utilization</p>
<p>I will enable it</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_timestamps=1</span><br></pre></td></tr></table></figure>

<h5 id="keepalive"><a href="#keepalive" class="headerlink" title="keepalive"></a>keepalive</h5><p>This determines the wait time between isAlive interval probes</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decrease the time default value for connections to keep alive</span></span><br><span class="line"><span class="comment">#default net.ipv4.tcp_keepalive_intvl = 75</span></span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 30</span><br><span class="line"><span class="comment">#default net.ipv4.tcp_keepalive_probes = 9</span></span><br><span class="line">net.ipv4.tcp_keepalive_probes = 5</span><br><span class="line"><span class="comment">#default net.ipv4.tcp_keepalive_time = 7200</span></span><br><span class="line">net.ipv4.tcp_keepalive_time = 1800</span><br></pre></td></tr></table></figure>

<h5 id="tcp-tw-reuse"><a href="#tcp-tw-reuse" class="headerlink" title="tcp_tw_reuse"></a>tcp_tw_reuse</h5><p>This allows reusing sockets in TIME_WAIT state for new connections when it is safe from protocol viewpoint.<br>Allow to reuse TIME_WAIT sockets for new connections when it is safe from protocol viewpoint. It should not be changed without advice/request of technical experts.</p>
<p>Note: The tcp_tw_reuse setting is particularly useful in environments where numerous short connections are open and left in TIME_WAIT state, such as web servers. Reusing the sockets can be very effective in reducing server load.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_tw_reuse=1</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 1440000</span><br></pre></td></tr></table></figure>

<h5 id="tcp-sack"><a href="#tcp-sack" class="headerlink" title="tcp_sack"></a>tcp_sack</h5><p>Enable the TCP selective acks option for better throughput:<br>With selective acknowledgments, the data receiver can inform the sender about all segments that have arrived successfully, so the sender need retransmit only the segments that have actually been lost.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.tcp_sack&#x3D;1</span><br></pre></td></tr></table></figure>

<p><a href="https://support.hpe.com/hpsc/doc/public/display?docId=emr_na-c00916755" target="_blank" rel="noopener">There are warnings saying that net.ipv4.tcp_tw_reuse and</a></p>
<pre><code>net.ipv4.tcp_tw_recycle tunables should not be changed without consulting experts first.  What does this mean and what can go
wrong if they are enabled.
Answer:  Potential problems are not specific to Linux.  Enabling these tunables will not make the host crash or unstable, but it may break TCP/IP functionality if the host is connected to devices such as load-balancers or firewalls.  Some of these devices can reject SYN if it reuses the same connection (i.e. src/dst IP and src/dst ports are the same) too quickly.  RFC 1122 describes when it is acceptable to recycle the connection when SYN arrives for a connection in TIME_WAIT state.</code></pre><h5 id="netfilter"><a href="#netfilter" class="headerlink" title="netfilter"></a>netfilter</h5><p>To reduce the number of connections in TIME_WAIT state, we can decrease the number of seconds connections are kept in this state before being dropped:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># reduce TIME_WAIT from the 120s default to 30-60s</span></span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_time_wait=30</span><br><span class="line"><span class="comment"># reduce FIN_WAIT from teh 120s default to 30-60s</span></span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_fin_wait=30</span><br><span class="line">`</span><br></pre></td></tr></table></figure>

<h4 id="Neighbour-Table-Overflow"><a href="#Neighbour-Table-Overflow" class="headerlink" title="Neighbour Table Overflow"></a><a href="https://www.cyberciti.biz/faq/centos-redhat-debian-linux-neighbor-table-overflow/" target="_blank" rel="noopener">Neighbour Table Overflow</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## works best with &lt;= 500 client computers ##</span></span><br><span class="line"><span class="comment"># Force gc to clean-up quickly</span></span><br><span class="line">net.ipv4.neigh.default.gc_interval = 1800</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set ARP cache entry timeout</span></span><br><span class="line">net.ipv4.neigh.default.gc_stale_time = 1800</span><br><span class="line"></span><br><span class="line"><span class="comment"># Setup DNS threshold for arp</span></span><br><span class="line"><span class="comment"># gc_thresh3 represents the hard maximum number of entries in the ARP cache</span></span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 1024</span><br></pre></td></tr></table></figure>
<p>gc_thresh1 (since Linux 2.2)<br>       The minimum number of entries to keep in  the  ARP  cache. The garbage collector will not run if there are fewer than this number of entries in the cache.  Defaults to 128.</p>
<p>gc_thresh2 (since Linux 2.2)<br>       The soft maximum number of entries to keep  in  the  ARP  cache.The garbage collector will allow the number of entries to exceed this  for  5  seconds  before  collection  will  be   performed. Defaults to 512.</p>
<p>gc_thresh3 (since Linux 2.2)<br>       The  hard  maximum  number  of entries to keep in the ARP cache.The garbage collector will always run if  there  are  more  than this number of entries in the cache.  Defaults to 1024.</p>
<h3 id="sysctl-conf"><a href="#sysctl-conf" class="headerlink" title="sysctl.conf"></a>sysctl.conf</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Interrupt Coalescing (soft IRQ) and Ingress QDisc</span></span><br><span class="line">net.core.netdev_max_backlog = 65536 </span><br><span class="line"><span class="comment">#  the maximum number of packets, queued on the INPUT side (the ingress qdisc), when the interface receives packets faster than kernel can process them.</span></span><br><span class="line"></span><br><span class="line">net.core.netdev_budget=600</span><br><span class="line"><span class="comment">#Be careful of increasing this value unless there is a very good reason. A value exceeding 1000 is unlikely to be very helpful. In fact increasing this value too much can have detrimental effect and in the worse case scenario lead to softirq hangs or performance problems, as the softirqs can run for too long and starve other processes of CPU. the maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed netdev_budget_usecs microseconds, even if netdev_budget has not been exhausted.</span></span><br><span class="line"></span><br><span class="line">net.core.netdev_budget_usecs=2000 <span class="comment">#default 2000</span></span><br><span class="line"><span class="comment">#is the maximum number of packets, queued on the INPUT side (the ingress qdisc), when the interface receives packets faster than kernel can process them.</span></span><br><span class="line"></span><br><span class="line">net.core.dev_weight=128 <span class="comment">#default 64</span></span><br><span class="line"><span class="comment">#he maximum number of packets that kernel can handle on a NAPI interrupt, it's a Per-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet in this.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## need hardware support</span></span><br><span class="line">net.core.busy_poll = 50 </span><br><span class="line"><span class="comment"># This parameter controls the number of microseconds to wait for packets on the device queue for socket poll and selects </span></span><br><span class="line"><span class="comment"># default 0</span></span><br><span class="line"><span class="comment"># This parameter controls the number of microseconds to wait for packets on the device queue for socket reads</span></span><br><span class="line">net.core.busy_read = 100 </span><br><span class="line"><span class="comment"># default 0</span></span><br><span class="line"><span class="comment">#bnx2x</span></span><br><span class="line"><span class="comment">#be2net</span></span><br><span class="line"><span class="comment">#ixgbe</span></span><br><span class="line"><span class="comment">#mlx4</span></span><br><span class="line"><span class="comment">#myri10ge</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### Egress Qdisc</span></span><br><span class="line">sysctl net.core.default_qdisc</span><br><span class="line">net.core.default_qdisc</span><br><span class="line"><span class="comment">#net.core.default_qdisc = pfifo_fast</span></span><br><span class="line"><span class="comment">#net.core.default_qdisc=fq_codel</span></span><br><span class="line">net.core.default_qdisc = pfifo_fast</span><br><span class="line"></span><br><span class="line">$ ifconfig ethX txqueuelen 2000</span><br><span class="line">txqueuelen is the maximum number of packets, queued on the OUTPUT side.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### TCP FSM and congestion algorithm</span></span><br><span class="line">net.core.somaxconn = 65536</span><br><span class="line"><span class="comment">#Limit of socket listen() backlog, known in userspace as SOMAXCONN. Defaults to 128.</span></span><br><span class="line"><span class="comment">#provides an upper limit on the value of the backlog parameter passed to the listen() function</span></span><br><span class="line"><span class="comment">#Decrease the time default value for tcp_fin_timeout connection, FIN-WAIT-2</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_fin_timeout = 15</span><br><span class="line"><span class="comment">#tcp_available_congestion_control - shows the available congestion control choices that are registered.</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 2048 <span class="comment"># default 256</span></span><br><span class="line"><span class="comment">#Maximum number of remembered connection requests, which are still did not receive an acknowledgment from connecting client. The default value is 1024 for systems with more than 128Mb of memory, and 128 for low memory machines.</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_available_congestion_control <span class="comment">#default reno cubic</span></span><br><span class="line"><span class="comment">#shows the available congestion control choices that are registered.</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_congestion_control </span><br><span class="line"><span class="comment">#default cubic</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_slow_start_after_idle=1 </span><br><span class="line"><span class="comment">#enable tcp slow start</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### TCP buffer</span></span><br><span class="line">net.ipv4.tcp_moderate_rcvbuf = 1</span><br><span class="line"><span class="comment">#If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># min (size used under memory pressure), default (initial size), max (maximum size) - size of receive buffer used by TCP sockets.</span></span><br><span class="line"><span class="comment"># default (initial size), max (maximum size) - size of send buffer used by TCP sockets.</span></span><br><span class="line"><span class="comment"># the number of pages(default 4K) allocated falls below the low, pressure, high mark.</span></span><br><span class="line"><span class="comment">#                     (64M) (512M)  (16G)</span></span><br><span class="line">net.ipv4.tcp_mem =  16384 131072   4194304</span><br><span class="line">net.ipv4.tcp_wmem = 16384 131072   4194304</span><br><span class="line"></span><br><span class="line"><span class="comment">#                    (128M) (512M)  (24G) here is page</span></span><br><span class="line"><span class="comment"># measured in units of the system page size</span></span><br><span class="line">net.ipv4.tcp_rmem = 32768 131072  6291456</span><br><span class="line"><span class="comment"># default: net.ipv4.tcp_wmem = 4096	16384	4194304 (4194304*4/1024/1024=16M)</span></span><br><span class="line"><span class="comment"># default: net.ipv4.tcp_wmem = 4096	16384	4194304 (16M)</span></span><br><span class="line"><span class="comment"># default: net.ipv4.tcp_rmem = 4096	131072	6291456 (24M)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#256M 64M, here is bytes</span></span><br><span class="line">net.core.rmem_max = 268435456</span><br><span class="line">net.core.rmem_default = 67108864 </span><br><span class="line"><span class="comment">#Socket Receive Queues, avoid package loss</span></span><br><span class="line">net.core.wmem_max = 268435456</span><br><span class="line">net.core.wmem_default = 67108864</span><br><span class="line"></span><br><span class="line"><span class="comment">#### The others</span></span><br><span class="line">net.ipv4.tcp_sack = 1</span><br><span class="line"><span class="comment">#Enable the TCP selective acks option for better throughput</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_timestamps=1</span><br><span class="line">net.ipv4.tcp_mtu_probing=1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse=1</span><br><span class="line">net.ipv4.tcp_adv_win_scale=1</span><br><span class="line">net.ipv4.tcp_window_scaling = 1</span><br><span class="line">net.ipv4.tcp_mtu_probing=1</span><br><span class="line">net.core.rmem_max=16777216</span><br><span class="line">net.ipv4.tcp_congestion_control=cubic</span><br><span class="line"></span><br><span class="line"><span class="comment"># fast retrans</span></span><br><span class="line">net.ipv4.tcp_frto=1</span><br><span class="line"><span class="comment">#Enable  F-RTO,  an enhanced recovery algorithm for TCP retrans‐mission timeouts (RTOs).   It  is  particularly  beneficial  in wireless  environments  where  packet  loss is typically due to random radio interference rather than intermediate router  con‐gestion.  See RFC 4138 for more details.</span></span><br><span class="line"><span class="comment">#              This file can have one of the following values:</span></span><br><span class="line"><span class="comment">#              0  Disabled.</span></span><br><span class="line"><span class="comment">#              1  The basic version F-RTO algorithm is enabled.</span></span><br><span class="line"><span class="comment">#              2  Enable  SACK-enhanced  F-RTO  if  flow uses SACK.  The basic version can be used also when SACK is in use though in  that case scenario(s) exists where F-RTO interacts badly with the packet counting of the SACK-enabled TCP flow.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># works best with &lt;= 1000 client computers ##</span></span><br><span class="line">net.ipv4.neigh.default.gc_interval = 30 </span><br><span class="line"><span class="comment">#default, How frequently the garbage collector for neighbour entries should attempt to run.Defaults to 30 seconds.  </span></span><br><span class="line">net.ipv4.neigh.default.gc_stale_time = 60 </span><br><span class="line"><span class="comment">#default,  Determines how often to check for stale neighbour entries. When a neighbour entry is considered stale it is resolved again before sending data to it. Defaults to 60 seconds</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gc_thresh3 represents the hard maximum number of entries in the ARP cache</span></span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 2048</span><br><span class="line"></span><br><span class="line"><span class="comment"># don't cache ssthresh from previous connection</span></span><br><span class="line">net.ipv4.tcp_no_metrics_save = 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#use cookies to process SYN queue overflow</span></span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># RHEL 7 default: 32768 61000</span></span><br><span class="line"><span class="comment"># outgoing port range</span></span><br><span class="line">net.ipv4.ip_local_port_range = 2000 65535</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decrease the time default value for tcp_fin_timeout connection, FIN-WAIT-2</span></span><br><span class="line">net.ipv4.tcp_fin_timeout = 15</span><br><span class="line"></span><br><span class="line">net.ipv4.netfilter.ip_conntrack_max=204800</span><br><span class="line"><span class="comment"># default net.ipv4.route.gc_timeout = 300</span></span><br><span class="line"></span><br><span class="line">net.ipv4.icmp_ignore_bogus_error_responses = 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># In WAN network connection</span></span><br><span class="line">net.ipv4.tcp_bic=1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 30</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 5</span><br><span class="line">net.ipv4.tcp_keepalive_time = 1800</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse=1</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 1440000</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_adv_win_scale=1</span><br><span class="line"><span class="comment">#The options info in SYN or SYN/ACK packages </span></span><br><span class="line"><span class="comment">#Needs to be set to 1 if the Max TCP Window is over 65535</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The following variable is used to tell the kernel how much of the socket buffer space should be used for TCP window size, and how much to save for an application buffer</span></span><br><span class="line"><span class="comment">#A value of 1 means the socket buffer will be divided evenly between TCP windows size and application</span></span><br><span class="line">net.ipv4.tcp_window_scaling = 1</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_low_latency=0</span><br><span class="line"><span class="comment">### htcp</span></span><br><span class="line">net.ipv4.tcp_congestion_control=htcp</span><br><span class="line"></span><br><span class="line"><span class="comment">### default</span></span><br><span class="line">tcp_congestion_control=cubic </span><br><span class="line">net.core.default_qdisc=fq_codel</span><br><span class="line"><span class="comment">#net.core.default_qdisc = pfifo_fast</span></span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">$ cat &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"></span><br><span class="line">### performance</span><br><span class="line">net.core.netdev_max_backlog &#x3D; 65536 </span><br><span class="line">net.core.netdev_budget&#x3D;600</span><br><span class="line">net.core.busy_poll &#x3D; 50 </span><br><span class="line">net.core.busy_read &#x3D; 100 </span><br><span class="line">net.core.somaxconn &#x3D; 65536</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 15</span><br><span class="line">net.ipv4.tcp_slow_start_after_idle&#x3D;1 </span><br><span class="line">net.ipv4.tcp_moderate_rcvbuf &#x3D; 1</span><br><span class="line">net.ipv4.tcp_mem &#x3D;  16384 131072   4194304</span><br><span class="line">net.ipv4.tcp_wmem &#x3D; 16384 131072   4194304</span><br><span class="line">net.ipv4.tcp_rmem &#x3D; 32768 131072  6291456</span><br><span class="line">net.core.rmem_max &#x3D; 268435456</span><br><span class="line">net.core.rmem_default &#x3D; 67108864 </span><br><span class="line">net.core.wmem_max &#x3D; 268435456</span><br><span class="line">net.core.wmem_default &#x3D; 67108864</span><br><span class="line">net.ipv4.tcp_sack &#x3D; 1</span><br><span class="line">net.ipv4.tcp_timestamps&#x3D;1</span><br><span class="line">net.ipv4.tcp_mtu_probing&#x3D;1</span><br><span class="line">net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">net.ipv4.tcp_adv_win_scale&#x3D;1</span><br><span class="line">net.ipv4.tcp_window_scaling &#x3D; 1</span><br><span class="line">net.ipv4.tcp_mtu_probing&#x3D;1</span><br><span class="line">net.ipv4.tcp_frto&#x3D;1</span><br><span class="line">net.ipv4.neigh.default.gc_interval &#x3D; 30 </span><br><span class="line">net.ipv4.neigh.default.gc_stale_time &#x3D; 60 </span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 &#x3D; 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 &#x3D; 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 &#x3D; 2048</span><br><span class="line">net.ipv4.tcp_no_metrics_save &#x3D; 1</span><br><span class="line">net.ipv4.tcp_syncookies &#x3D; 1</span><br><span class="line">net.ipv4.ip_local_port_range &#x3D; 2000 65535</span><br><span class="line">net.ipv4.tcp_fin_timeout &#x3D; 15</span><br><span class="line">net.ipv4.netfilter.ip_conntrack_max&#x3D;204800</span><br><span class="line">net.ipv4.icmp_ignore_bogus_error_responses &#x3D; 1</span><br><span class="line">net.ipv4.tcp_keepalive_intvl &#x3D; 30</span><br><span class="line">net.ipv4.tcp_keepalive_probes &#x3D; 5</span><br><span class="line">net.ipv4.tcp_keepalive_time &#x3D; 1800</span><br><span class="line">net.ipv4.tcp_tw_reuse&#x3D;1</span><br><span class="line">net.ipv4.tcp_max_tw_buckets &#x3D; 1440000</span><br><span class="line">net.ipv4.tcp_adv_win_scale&#x3D;1</span><br><span class="line">net.ipv4.tcp_window_scaling &#x3D; 1</span><br><span class="line">net.ipv4.tcp_low_latency&#x3D;0</span><br><span class="line">net.ipv4.tcp_congestion_control&#x3D;cubic</span><br><span class="line">net.core.default_qdisc&#x3D;fq_codel</span><br><span class="line"></span><br><span class="line"># if you custom pin cpu resources</span><br><span class="line">#kernel.numa_balancing&#x3D;0</span><br><span class="line"></span><br><span class="line">#avoid the bad udp package</span><br><span class="line">#net.inet.udp.checksum&#x3D;1</span><br><span class="line">#zero means not receive source route info ip package</span><br><span class="line">#net.ipv4.conf.default.accept_source_route &#x3D; 0</span><br></pre></td></tr></table></figure>

<h3 id="NFS-hangs-because-zero-window-problem"><a href="#NFS-hangs-because-zero-window-problem" class="headerlink" title="NFS hangs because zero window problem"></a><a href="https://access.redhat.com/solutions/1610363" target="_blank" rel="noopener">NFS hangs because zero window problem</a></h3><p>Zero Window is something to investigate. TCP Zero Window is when the Window size in a machine remains at zero for a specified amount of time. This means that a client is not able to receive further information at the moment, and the TCP transmission is halted until it can process the information in its receive buffer.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_sack=0</span><br></pre></td></tr></table></figure>
<p>disable tcp sack and remount the nfs</p>
<p><a href="https://wiki.wireshark.org/TCP%20ZeroWindow" target="_blank" rel="noopener">Troubleshooting a Zero Window</a> For one reason or another, the machine alerting the Zero Window will not receive any more data from the host. It could be that the machine is running too many processes at that moment, and its processor is maxed. Or it could be that there is an error in the TCP receiver, like a Windows registry misconfiguration. Try to determine what the client was doing when the TCP Zero Window happened.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ tcpkill -i eth0 port 21</span><br><span class="line">$ tcpkill host 192.168.1.2</span><br><span class="line">$ tcpkill ip host 192.168.1.2 and not 192.168.1.111</span><br></pre></td></tr></table></figure>

<h4 id="All-NFS-connection-are-hang-all-TCP-state-are-in-TIMEWAIT-status"><a href="#All-NFS-connection-are-hang-all-TCP-state-are-in-TIMEWAIT-status" class="headerlink" title="All NFS connection are hang, all TCP state are in TIMEWAIT status"></a>All NFS connection are hang, all TCP state are in TIMEWAIT status</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#modify</span></span><br><span class="line">net.ipv4.tcp_low_latency=0 to net.ipv4.tcp_low_latency=1</span><br><span class="line">net.ipv4.tcp_adv_win_scale=0 to net.ipv4.tcp_adv_win_scale=1</span><br></pre></td></tr></table></figure>

<p>restart service ,it ‘s not work, restart rpcbind ,it ‘s not work, looks like tcp issue.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_tw_reuse=1</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 1440000</span><br><span class="line"><span class="comment"># net.ipv4.tcp_tw_recycle has been removed</span></span><br></pre></td></tr></table></figure>
<p>all TCP state to ESTABLISHED, it ‘s recovery.</p>
<h3 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h3><h4 id="kernel"><a href="#kernel" class="headerlink" title="kernel"></a>kernel</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ ss -eipnt | grep -A 1 $ipaddr --color  | grep -B 1 -Ei &#39;cwnd|ssthresh&#39;</span><br><span class="line">    * If cwnd and ssthresh and cwnd &#39;s value up and down frequently that means there is a bad network env(maybe it &#39;s trigger the tcp slow start algorithm and TCP Congestion avoidance algorithm)</span><br><span class="line"></span><br><span class="line">$ sar -n TCP,ETCP 2</span><br><span class="line">02:47:30 AM  active&#x2F;s passive&#x2F;s    iseg&#x2F;s    oseg&#x2F;s</span><br><span class="line">02:47:31 AM      0.00      0.00      8.51      6.38</span><br><span class="line"></span><br><span class="line">02:47:30 AM  atmptf&#x2F;s  estres&#x2F;s retrans&#x2F;s isegerr&#x2F;s   orsts&#x2F;s</span><br><span class="line">02:47:31 AM      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line"># active&#x2F;s: Number of locally-initiated TCP connections per second (e.g., via connect()).</span><br><span class="line"># passive&#x2F;s: Number of remotely-initiated TCP connections per second (e.g., via accept()).</span><br><span class="line"># retrans&#x2F;s: Number of TCP retransmits per second.</span><br><span class="line"># The active and passive counts are often useful as a rough measure of server load: number of new accepted connections (passive), and number of downstream connections (active). It might help to think of active as outbound, and passive as inbound, but this isn’t strictly true (e.g., consider a localhost to localhost connection).</span><br><span class="line"># Retransmits are a sign of a network or server issue; it may be an unreliable network (e.g., the public Internet), or it may be due a server being overloaded and dropping packets. The example above shows just one new TCP connection per-second.</span><br><span class="line"></span><br><span class="line">$ ip -s -s link</span><br><span class="line"></span><br><span class="line">&#x2F;sys&#x2F;class&#x2F;net&#x2F;em2&#x2F;statistics&#x2F;</span><br><span class="line">&#x2F;proc&#x2F;net&#x2F;dev</span><br><span class="line"></span><br><span class="line">$ netstat -s | grep -i retr</span><br><span class="line">    905260805 segments retransmited</span><br><span class="line">    9599 times recovered from packet loss due to fast retransmit</span><br><span class="line">    TCPLostRetransmit: 6059087</span><br><span class="line">    152 timeouts after reno fast retransmit</span><br><span class="line">    737224794 fast retransmits</span><br><span class="line">    13068581 forward retransmits</span><br><span class="line">    146850567 retransmits in slow start</span><br><span class="line">    1015 classic Reno fast retransmits failed</span><br><span class="line">    3214020 SACK retransmits failed</span><br><span class="line">    TCPRetransFail: 16</span><br><span class="line">    TCPSynRetrans: 61</span><br></pre></td></tr></table></figure>

<p><a href="http://www.brendangregg.com/blog/2014-09-06/linux-ftrace-tcp-retransmit-tracing.html" target="_blank" rel="noopener">tcpretrans</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">11:14:40 0      10.0.19.53:988      R&gt; 10.0.4.143:1022     ESTABLISHED</span><br><span class="line">11:14:40 0      10.0.19.53:988      R&gt; 10.0.4.143:1022     ESTABLISHED</span><br><span class="line">11:14:40 0      10.0.19.53:988      R&gt; 10.0.4.143:1022     ESTABLISHED</span><br></pre></td></tr></table></figure>

<h5 id="kernel-improved"><a href="#kernel-improved" class="headerlink" title="kernel improved"></a>kernel improved</h5><p>linux 4.4 lockless listener<br>&lt; linux 4.4, each listener has a request queue. after insert a new request the each sync package will lock listener</p>
<blockquote>
<p>= linux 4.4 create an new tcp ehash table for reduece the lock compete </p>
</blockquote>
<p>tcp_probe<br>That function is now replaced by tcp/tcp_probe trace-event. You can use it via ftrace or perftools</p>
<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethttool -S dev | grep -Ei <span class="string">'err|miss|dis|drop|crc|jabb|timeout'</span></span><br></pre></td></tr></table></figure>
<p>rx_no_buffer_count,there was nowhere to DMA the packet<br>DMA skb-data to rx-buffer-info(linux RAM, soft-interrupt).</p>
<p>RNBC is a warning sign of a slow drain from the MAC and can be treated by adding more buffers.<br>There will be times when the RNBC will go up, but it will look like the stack and driver have a ton of buffers but work is not being done.  If you have a task that is eating up the CPU, the ISR or polling routines won’t refill the buffers fast enough and RNBC will happen.</p>
<p>Imagine we have a slow CPU, but a wicked fastbus.  The software is very slow to process the descriptors and return them, but once the descriptors are given to the hardware, it empties the backlog (read the FIFO) faster than the incoming frames are filling the FIFO.  Returning to our kitchen sink analogy, the water is coming in at a fairly constant rate.  But imagine the stopper is down, making the sink fill up.  Just before it over flows, the drain is opened and down it goes.  Once the water doesn’t go down the drain would be the same moment our RNBC would be incremented.  The kitchen sink itself becomes our FIFO and if the FIFO is big enough, it can save frame for quiet some time.  This is 1 Gigabit (or faster) that we’re talking about, so with a good sized FIFO (24K RX for example) that’s only 375 frames at 64 bytes, or 267microseconds of data.  That’s not very much time.  But in a world full of 2 and 3 Gigahertz CPUs that’s long enough.</p>
<p>rx_missed_error,rx_no_buffer_count happened enough times that packets were dropped<br>MPC is a failure condition leading to dropped packets and can be treated with more buffers and faster interconnect buses</p>
<p><code>fifo queues(ring buffer or kernel queue?) full will cause the error too ? and the package not go to NIC reveive ring buffer(ring buffer or kernel queue ?), if you have no any CPU or memory resource, you will got the same result, NIC/CPU/MEM resource not enough will cause the same issue</code></p>
<p><code>I guess if the drop in fifo queue from driver, it &#39;s the rx_missed_error, if it dropped in kernel, it &#39;s the overrun</code> </p>
<p><a href="https://www.cisco.com/c/en/us/support/docs/dial-access/asynchronous-connections/15286-serial-overruns.html" target="_blank" rel="noopener">Q. What are overruns on a serial interface?</a></p>
<p>A. Overruns appear in the output of the show interface Serial 0 command when the serial receiver hardware is unable to hand received data to a hardware buffer because the input rate exceeds the receiver’s ability to handle the data.<br>This occurs due to a limitation of the hardware. Overruns occur when the internal First In, First Out (FIFO) buffer of the chip is full, but is still tries to handle incoming traffic. The serial controller chip has limited internal FIFO.<br>Some chips, for example, have only 256 bytes of buffer space. Data from the network is received into the buffer, whereupon the chip attempts to move the data from the buffer to the router’s shared memory for the CPU to process. If the chip is not able to move the data from its internal FIFO buffer into shared memory faster than the rate at which data is received on the interface, then the internal FIFO buffer is full, incoming data is dropped, and the overrun counter is incremented. </p>
<p>Thank goodness things like TCP/IP will tell the applications data has been dropped, but if using a lossy frame type like UDP its just too bad, your frame is lost to the ether.</p>
<p>Imagine if you will, an interconnect bus that is slow.  Very slow.  Like a PCI 33hz bus.  Now attach that to a full line rate 1 Gigabit 64 byte packet data stream.  At one descriptor per packet, that’s about 1.4 million descriptors per second.  In this case the software is very fast, faster than the bus.  So the number of available descriptors is always kept a level that keeps the buffers available to the hardware to conduct a DMA.  But because the bus is so slow, data backs up into the FIFO.  Now that is what the FIFO is for.  By buffering the packet, it tries to give the packet the best chance at making into host memory alive.  In our slow case, the buffering isn’t enough and the FIFO fills up.  It is draining slower than its filling, its just a like a slow draining kitchen sink.  Eventually it overflows and makes a big mess.  Thank goodness things like TCP/IP will tell the applications data has been dropped, but if using a lossy frame type like UDP its just too bad, your frame is lost to the ether.  If you need to keep track, but need to use UDP, you’ll need to monitor the MPC count and decide what you want to do when it goes up.</p>
<p><a href="https://community.mellanox.com/s/article/counters-troubleshooting-for-linux-driver" target="_blank" rel="noopener">Counters Troubleshooting for Linux Driver</a><br>rx_errors: Number of received packets that were dropped due to PHY layer related errors. For example:</p>
<ul>
<li>symbol error, or an invalid block.</li>
<li>Length related errors (greater than MTU octets, length less than 64 octets, error in length)</li>
<li>Bad CRC that are not runts, jabbers, or alignment errors.</li>
<li>This counter is increased at point (1) in the figure above.</li>
</ul>
<p>rx_dropped: Number of received packets which were chosen to be discarded even though no errors had been detected to prevent them from passing to the upper layer. For example, drop due to buffer overflow.</p>
<p>rx_crc_errors: Number of received frames with a bad CRC that are not runts, jabbers, or alignment errors.</p>
<p>rx_jabbers: Number of received frames with a length greater than MTU octets and a bad CRC.</p>
<p>rx_fifo_errors: an indication that the RX interrupts cannot allocate buffers fast enough and so the adapter is dropping packets.</p>
<p>rx_over_errors: Number of received frames that were dropped due to on hardware port receive buffer overflow.</p>
<p>tx_errors: Number of frames that failed to transmit. Include frame dropped due to error in the length field.</p>
<p>tx_dropped: Number of transmitted frames that were dropped.</p>
<p>vport_rx_dropped: Received packets discarded due to luck of software receive buffers (WQEs).<br>Important indication to weather RX completion routines are keeping up with HW ingress packet rate.</p>
<p>vport_rx_filtered: Received packets dropped due to packet check that was failed. For example:</p>
<ul>
<li>Incorrect VLAN</li>
<li>Incorrect Ethertype</li>
<li>unavailable queue/QP</li>
<li>Loopback prevention<br>This counter is increased at point (2) in the figure above.<br>Note: In high performance scenarios vport_rx_filters may increment due to rx_over_errors. In addition,<br>In SRIOV configurations vport_rx_filters increments can be seen and it is a normal condition (expected).</li>
</ul>
<p>vport_tx_errors: Packets dropped due to transmit errors.</p>
<p>counter:</p>
<p>rx_lro_aggregated: The number of packets processed by the LRO (Large Receive Offload) mechanism (good for IPv4 TCP), and should be equal to rx_packets in good/normal condition.</p>
<p>rx_lro_flushed: The number of offloaded packets the LRO mechanism passed to kernel. Ideally the packet size is 64KB (depends on kernel). 64KB is the maximum packet size.</p>
<p>rx_lro_no_desc: This is abnormal condition, and mostly will not happen. The LRO mechanism has no room to receive packets from the adapter. In normal condition, it should not increase, mostly when using 64 packets budget and flush LRO descriptors every NAPI cycle. In addition, LRO has a lot of space (much more than 64).</p>
<p>tx_tso_packets: When using TCO (TCP Segmentation Offload), it offloads tasks from the CPU and improve CPU utilization. This counter shows the number of offloaded TSO packets received by the driver from the TCP layer. The rate of TSO This counter is correlated strongly with the TX performance and CPU utilization. TSO is crucial for wire speed performance, and the kernel will enable it only when the CPU is not on heavy load.</p>
<p>tx_queue_stopped: The number of times the kernel didn’t manage to send packets as the queue was full. the tx_queue_stopped and tx_wake_queue are usually equal (TX queue is stopped and later gets wake up call). This is an important indication to whether TX completion routines are keeping up with the transmit routines. If the application is sending in an higher rate than driver is evicting CQEs from the buffer this will start to go up.</p>
<p>tx_wake_queue: The number of time the kernel got message from the adapters that there is a queue to run (tx_queue_stopped is released). his is an important indication to whether TX completion routines are keeping up with the transmit routines. If the application is sending in an higher rate than driver is evicting CQEs from the buffer this will start to go up.</p>
<p>tx_timeout: This a rare event, that usually indicate on a severe issue. It means around 15 sec timeframe that passed since a packet was sent without a CQE generated. Usually a lost interrupt or a bad cable.</p>
<p>rx_csum_good: The number of packets received with good checksum (in L4).</p>
<p>rx_csum_none: The number of packets received with no checksum (in L4).</p>
<p>tx_cksum_offload: The number of packets sent with hardware checksum.</p>
<h3 id="About-TCP-fast-retrans"><a href="#About-TCP-fast-retrans" class="headerlink" title="About TCP fast retrans"></a><a href="http://www.ietf.org/rfc/rfc2581.txt" target="_blank" rel="noopener">About TCP fast retrans</a></h3><p>twice duplicated ACK because package out of order<br>Dropped packages must be triple duplicated ACK</p>
<p>If A send 4 TCP segments to B, Number is as follow, N-1 reache B because A received ACK(N) of B, As the order, The received ACK number:<br>                  A ———&gt; B<br>A send order was N-1,N,N+1,N+2</p>
<p>B received order</p>
<p>N-1，N，N+1，N+2<br>A received single ACK (N)</p>
<p>N-1，N，N+2，N+1<br>A received single ACK (N)</p>
<p>N-1，N+1，N，N+2<br>A received twice ACK (N)</p>
<p>N-1，N+1，N+2，N<br>A received triple ACK (N)</p>
<p>N-1，N+2，N，N+1<br>A received twice ACK (N)</p>
<p>N-1，N+2，N+1，N<br>A received triple ACK (N)</p>
<p>If N loss, or not reach B</p>
<p>N-1，N+1，N+2<br>A received triple ACK (N)</p>
<p>N-1，N+2，N+1<br>A received triple ACK (N)</p>
<p>TCP segment out of order, there are 2/5 = 40% cause A received triple duplicated ACK(N);<br>If N loss, the ratio is 100%</p>
<p>When A received triple duplicated ACK(N) and start a Fast Retransmit is OK , retransmit N right now, It ‘s need to Fast Recovery, for reduce the package loss problem</p>
<p>If A receive twice duplicated ACK(N),It ‘s must be out of order that means all packages has reach B, Just re-sort all packages, Don’ t need to retrans.</p>
<h5 id="ABout-TCP-segment-out-of-order"><a href="#ABout-TCP-segment-out-of-order" class="headerlink" title="ABout TCP segment out of order"></a>ABout TCP segment out of order</h5><p>TCP segment packing in IP packages，If IP package out of order,also tcp</p>
<p>1) ECMP loading balance</p>
<p>multiple path loading balance, base per-packet load balance，eg: packet 1,3,5 go path1, packet 2,4,6 go to path2, it’s hard to control packet 1 arrived early than packet 2 reach the destination</p>
<p>Per-session load balance base TCP 5 tuple(source ip,source port,des ip,des port, transfer protocol), the same TCP session will go to a same path</p>
<p>2) route internal traffic scheduling<br>There are multiple traffic process unit in some of route(stream process unit),eg: packet 1,3,5 process by unit1, packet 2,4,6 process by unit2, it’s hard to control packet 1 arrived early than packet 2 reach the destination</p>
<p>kernel receive out of order TCP segment, put them to buffer and then all TCP segment has arrived, after re-sort, send all data to application<br>Out of order segment will cause buffer consumption, direct trigger B advertised window size be samller, cause send A window get smaller and smaller,to impact sender’s transmission performance.</p>
<p>If A not do fast retrans，at last it will be retrans by retransmit timer timeout(timeout retransmit),but at this time there is a little winodow size in A, sender A ‘s transmission speed will be too bad.</p>
<p>Before there is no fast retransmit/recovery, retrans by sender ‘s retransmit timeout,In timeout range, if sender not receive receiver’s ack hat means the package loss, sender will resend it.</p>
<ul>
<li>Why package loss<ul>
<li>package checksum error</li>
<li>Traffic jam/Network congestion</li>
<li>Network connection loss for some unknow reason</li>
<li>Some of convergence-algorithm in your route</li>
</ul>
</li>
</ul>
<p>But sender don ‘t know what the situation,<br>The stupid way is sender set half of speed(CWND=1/2),It ‘s good at traffic jam,if connection loss, all packages will loss no matter sender slowing down,For checksum erorr,loss packages is a happenstance.If I loss a package, sender will tuning down the speed, So fast retransmit come out because receiver could recevice the ACK ,because connection has not loss,if in timeout range not receive large than 2 duplicated ACK thaat means maybe is out of order,don ‘t need retrans, just re-sort in receiver.<br>But if sender receive &gt;= 3 x duplicated ACK,loss package has the high possibility.<br>That means sender could receive ACK,The network connection not loss,retrans it first,don ‘t slowing down the speed, if got the correct ACK that means there is no problem and drop that package, If still recive duplicated ACK, maybe it ‘s traffic jam, slowing down the speed.</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/benchmark/" rel="tag"><i class="fa fa-tag"></i> benchmark</a>
              <a href="/tags/nic/" rel="tag"><i class="fa fa-tag"></i> nic</a>
              <a href="/tags/irq/" rel="tag"><i class="fa fa-tag"></i> irq</a>
              <a href="/tags/network/" rel="tag"><i class="fa fa-tag"></i> network</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/05/27/ipmi/" rel="prev" title="ipmitool">
      <i class="fa fa-chevron-left"></i> ipmitool
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/07/02/mem/" rel="next" title="memory">
      memory <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#slot-status"><span class="nav-number">1.</span> <span class="nav-text">slot status</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#THEORETICAL-MAXIMUM-RATE"><span class="nav-number">2.</span> <span class="nav-text">THEORETICAL MAXIMUM RATE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#About-FEC"><span class="nav-number">3.</span> <span class="nav-text">About FEC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tune-adm-policy"><span class="nav-number">4.</span> <span class="nav-text">Tune-adm policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-setting"><span class="nav-number">5.</span> <span class="nav-text">CPU setting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nic-offload"><span class="nav-number">6.</span> <span class="nav-text">nic offload</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Resize-the-hardware-buffer-queue-to-max"><span class="nav-number">6.1.</span> <span class="nav-text">Resize the hardware buffer queue to max</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#setting-driver"><span class="nav-number">6.2.</span> <span class="nav-text">setting driver</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interrupt-Queues"><span class="nav-number">7.</span> <span class="nav-text">Interrupt Queues</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Busy-Polling"><span class="nav-number">7.1.</span> <span class="nav-text">Busy Polling</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Socket-receive-queues"><span class="nav-number">8.</span> <span class="nav-text">Socket receive queues</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Change-the-speed-of-the-incoming-queue"><span class="nav-number">8.1.</span> <span class="nav-text">Change the speed of the incoming queue</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Increase-the-depth-of-the-application’s-socket-queue"><span class="nav-number">8.1.1.</span> <span class="nav-text">Increase the depth of the application’s socket queue</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Use-setsockopt-to-configure-a-larger-SO-RCVBUF-value-userspace"><span class="nav-number">8.2.</span> <span class="nav-text">Use setsockopt to configure a larger SO_RCVBUF value (userspace)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RSS-IRQ-Affinity"><span class="nav-number">8.3.</span> <span class="nav-text">RSS IRQ Affinity</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Support-for-UDP-RSS"><span class="nav-number">8.3.1.</span> <span class="nav-text">Support for UDP RSS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Receive-Flow-Streering-RFS"><span class="nav-number">9.</span> <span class="nav-text">Receive Flow Streering (RFS)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Accelerated-RFS"><span class="nav-number">9.0.1.</span> <span class="nav-text">Accelerated RFS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interrupt-Moderation-interrupt-coalescence-or-Interrupt-Blanking"><span class="nav-number">10.</span> <span class="nav-text">Interrupt Moderation (interrupt coalescence or Interrupt Blanking)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#disable-PCIE-power-save"><span class="nav-number">11.</span> <span class="nav-text">disable PCIE power save</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#disable-numa"><span class="nav-number">11.1.</span> <span class="nav-text">disable numa</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RFC2544-stipulates-that-the-latency-test"><span class="nav-number">12.</span> <span class="nav-text">RFC2544 stipulates that the latency test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-network-latency"><span class="nav-number">13.</span> <span class="nav-text">The network latency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tools"><span class="nav-number">14.</span> <span class="nav-text">Tools</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#nc-test"><span class="nav-number">14.1.</span> <span class="nav-text">nc test</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#iperf3"><span class="nav-number">14.2.</span> <span class="nav-text">iperf3</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#qperf"><span class="nav-number">14.3.</span> <span class="nav-text">qperf</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#netperf"><span class="nav-number">14.4.</span> <span class="nav-text">netperf</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sockperf"><span class="nav-number">14.5.</span> <span class="nav-text">sockperf</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#redis"><span class="nav-number">14.6.</span> <span class="nav-text">redis</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RFC6349-TCP-benchmark"><span class="nav-number">15.</span> <span class="nav-text">RFC6349 TCP benchmark</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#irqbalance"><span class="nav-number">15.1.</span> <span class="nav-text">irqbalance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#turbostat"><span class="nav-number">15.2.</span> <span class="nav-text">turbostat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#numastat"><span class="nav-number">15.3.</span> <span class="nav-text">numastat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#numad"><span class="nav-number">15.4.</span> <span class="nav-text">numad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#OProfile"><span class="nav-number">15.5.</span> <span class="nav-text">OProfile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#valgrind"><span class="nav-number">15.6.</span> <span class="nav-text">valgrind</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#intel-cmt-cat"><span class="nav-number">15.7.</span> <span class="nav-text">intel-cmt-cat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mellanox-nic-exapmle"><span class="nav-number">16.</span> <span class="nav-text">mellanox nic exapmle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ixgbe-example"><span class="nav-number">17.</span> <span class="nav-text">ixgbe example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#txqueueleng"><span class="nav-number">18.</span> <span class="nav-text">txqueueleng</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Enable-jumbo-frames"><span class="nav-number">18.1.</span> <span class="nav-text">Enable jumbo frames</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NIC-receive-packages"><span class="nav-number">19.</span> <span class="nav-text">NIC receive packages</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bottle-neck"><span class="nav-number">19.1.</span> <span class="nav-text">Bottle neck</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linux-network-parameters"><span class="nav-number">20.</span> <span class="nav-text">Linux network parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TCP-control-algorithms"><span class="nav-number">20.1.</span> <span class="nav-text">TCP control algorithms</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#The-others-parameters"><span class="nav-number">20.2.</span> <span class="nav-text">The others parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-frto"><span class="nav-number">20.2.1.</span> <span class="nav-text">tcp_frto</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-window-scaling"><span class="nav-number">20.2.2.</span> <span class="nav-text">tcp_window_scaling</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-win-scale"><span class="nav-number">20.2.3.</span> <span class="nav-text">tcp_win_scale</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-timestamps"><span class="nav-number">20.2.4.</span> <span class="nav-text">tcp_timestamps</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#keepalive"><span class="nav-number">20.2.5.</span> <span class="nav-text">keepalive</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-tw-reuse"><span class="nav-number">20.2.6.</span> <span class="nav-text">tcp_tw_reuse</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tcp-sack"><span class="nav-number">20.2.7.</span> <span class="nav-text">tcp_sack</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#netfilter"><span class="nav-number">20.2.8.</span> <span class="nav-text">netfilter</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neighbour-Table-Overflow"><span class="nav-number">20.3.</span> <span class="nav-text">Neighbour Table Overflow</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sysctl-conf"><span class="nav-number">21.</span> <span class="nav-text">sysctl.conf</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NFS-hangs-because-zero-window-problem"><span class="nav-number">22.</span> <span class="nav-text">NFS hangs because zero window problem</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#All-NFS-connection-are-hang-all-TCP-state-are-in-TIMEWAIT-status"><span class="nav-number">22.1.</span> <span class="nav-text">All NFS connection are hang, all TCP state are in TIMEWAIT status</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Monitor"><span class="nav-number">23.</span> <span class="nav-text">Monitor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#kernel"><span class="nav-number">23.1.</span> <span class="nav-text">kernel</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#kernel-improved"><span class="nav-number">23.1.1.</span> <span class="nav-text">kernel improved</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Driver"><span class="nav-number">23.2.</span> <span class="nav-text">Driver</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#About-TCP-fast-retrans"><span class="nav-number">24.</span> <span class="nav-text">About TCP fast retrans</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ABout-TCP-segment-out-of-order"><span class="nav-number">24.0.1.</span> <span class="nav-text">ABout TCP segment out of order</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ginger"
      src="/img/logo-4-blog.png">
  <p class="site-author-name" itemprop="name">Ginger</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ginger</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">799k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">12:06</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://b946c5a0bf547c89.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "http://yoursite.com/2018/06/01/nic_ethernet_tuning/",
            identifier: "2018/06/01/nic_ethernet_tuning/",
            title: "ethernet nic tuning"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://b946c5a0bf547c89.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
